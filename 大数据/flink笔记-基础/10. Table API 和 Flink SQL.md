## 1. 初始

> - Table API 是流处理和批处理通用的关系型 API，Table API 可以基于流输入或者批输入来运行而不需要进行任何修改。
> - Table API 是 SQL 语言的超集并专门为 Apache Flink 设计的， Table API 是 Scala 和 Java 语言集成式的 API。 
> - 与常规 SQL 语言中将 查询指定为字符串不同，Table API 查询是以 Java 或 Scala 中的语言嵌入样式来定义的， 具有 IDE 支持如:自动完成和语法检测。



- 依赖引入

  ```xml
  <dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-planner_2.12</artifactId>
  <version>1.10.1</version>
  </dependency>
  <dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner-blink_2.12</artifactId>
    <version>1.10.1</version>
  </dependency>
  <dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-table-api-scala-bridge_2.12</artifactId>
  <version>1.10.1</version>
  </dependency>
  ```
  
  
  
- 代码示例

  ```scala
  package com.atguigu.apitest
  
  import org.apache.flink.api.common.serialization.SimpleStringSchema
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011
  import org.apache.flink.table.api.{EnvironmentSettings, Table}
  import org.apache.flink.table.api.scala.{StreamTableEnvironment, tableConversions}
  
  import java.util.Properties
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  object TableTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream: DataStream[SensorReading] = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
      val settings: EnvironmentSettings = EnvironmentSettings
        .newInstance()
        .useOldPlanner()
        .inStreamingMode()
        .build()
      // 创建表环境
      val tableEnv: StreamTableEnvironment = StreamTableEnvironment.create(env, settings)
  
      // 基于流创建一张表
      val dataTable: Table = tableEnv.fromDataStream(dataStream)
  
      // 调用table api 进行转换
      val  resultTable = dataTable
        .select("id,temperature")
        .filter("id== 'sensor_1'")
  
      resultTable.toAppendStream[(String,Double)].print("Table API")
  
      // 直接使用SQL实现
      tableEnv.createTemporaryView("dataTable",dataTable)
      val sqlStr = "select id, temperature from dataTable where id = 'sensor_1'"
      val resultSQLTable = tableEnv.sqlQuery(sqlStr)
      resultSQLTable.toAppendStream[(String,Double)].print("sql api")
      env.execute()
    }
  
  }
  
  ```

  

## 2. 基本结构

- 创建TableEnvironment

  ```scala
  val tableEnv = ... // 创建表的执行环境
  ```

- 创建一张表，用于读取数据

  ```scala
  tableEnv.connect(...).createTemporaryTable("inputTable")
  ```

- 注册一张表，用于把计算结果输出

  ```scala
  tableEnv.connect(...).createTemporaryTable("outputTable")
  ```

- 通过 Table API 查询算子，得到一张结果表

  ```scala
  val result = tableEnv.from("inputTable").select(...)
  ```

- 通过 SQL查询语句，得到一张结果表

  ```scala
  val sqlResult = tableEnv.sqlQuery("SELECT ... FROM inputTable ...")
  ```

- 将结果表写入输出表中

  ```scala
  result.insertInto("outputTable")
  ```

  

### 2.1 创建TableEnvironment

- 创建基础table 环境, 需要将 flink 流处理的执行环境传入

  ```scala
  val tableEnv = StreamTableEnvironment.create(env)
  ```

- TableEnvironment 是 flink 中集成 Table API 和 SQL 的核心概念，所有对表 的操作都基于 TableEnvironment

  - 注册 Catalog
  - 在 Catalog 中注册表
  - 执行 SQL 查询
  - 注册用户自定义函数（UDF）

### 2.2 配置 TableEnvironment

##### a. 老版本

- 配置老版本 planner 的流式查询

  ```scala
  val settings = EnvironmentSettings
  .newInstance() 
  .useOldPlanner() 
  .inStreamingMode() 
  .build() 
  val tableEnv = StreamTableEnvironment.create(env, settings)
  ```

- 配置老版本 planner 的批式查询

  ```scala
  val batchEnv = ExecutionEnvironment.getExecutionEnvironment 
  val batchTableEnv = BatchTableEnvironment.create(batchEnv)
  ```

  

##### b. 新版本

- 配置 blink planner 的流式查询

  ```scala
  val bsSettings = EnvironmentSettings.newInstance() 
  .useBlinkPlanner() 
  .inStreamingMode() 
  .build() 
  val bsTableEnv = StreamTableEnvironment.create(env, bsSettings)
  ```

- 配置 blink planner 的批式查询

  ```scala
  val bbSettings = EnvironmentSettings.newInstance() 
  .useBlinkPlanner() 
  .inBatchMode() 
  .build() 
  val bbTableEnv = TableEnvironment.create(bbSettings)
  ```

## 3. 表（Table）

- TableEnvironment 可以注册目录 Catalog，并可以基于 Catalog 注册表
- 表（Table）是由一个“标识符”（identifier）来指定的，由3部分组成： Catalog名、数据库（database）名和对象名
- 表可以是常规的，也可以是虚拟的（视图，View）
-  常规表（Table）一般可以用来描述外部数据，比如文件、数据库表或消息队 列的数据，也可以直接从 DataStream转换而来
- 视图（View）可以从现有的表中创建，通常是 table API 或者 SQL 查询的一 个结果集

### 3.1 创建表

- TableEnvironment 可以调用 `.connect() `方法，连接外部系统，并调 用 `.createTemporaryTable() `方法，在 Catalog 中注册表

  ```scala
  tableEnv
  .connect(...) // 定义表的数据来源，和外部系统建立连接
  .withFormat(...) // 定义数据格式化方法 
  .withSchema(...) // 定义表结构 
  .createTemporaryTable("MyTable") // 创建临时表
  ```

- demo: 从文件中读取数据, 并创建表

  > 可以创建 Table 来描述文件数据，它可以从文件中读取，或者将数据写入文件

  ```scala
  tableEnv
  .connect( new FileSystem().path(“YOUR_Path/sensor.txt”) ) // 定义到文件系统的连接 
  .withFormat(new OldCsv()) // 定义以csv格式进行数据格式化 , 这是旧版
  //.withFormat(new Csv()) // 定义以csv格式进行数据格式化 , 这是新版, 需要引入依赖, 依赖在下方
  .withSchema( new Schema()
  	.field("id", DataTypes.STRING())
  	.field("timestamp", DataTypes.BIGINT())
  	.field("temperature", DataTypes.DOUBLE()) 
  ) // 定义表结构 
  .createTemporaryTable("sensorTable") // 创建临时表
  ```

  - 新版csv依赖: flink-csv

    ```xml
    <dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-csv</artifactId>
    <version>1.10.1</version>
    </dependency>
    ```

  - 完整示例

    ```scala
    package com.atguigu.apitest
    
    import org.apache.flink.streaming.api.scala._
    import org.apache.flink.table.api.DataTypes
    import org.apache.flink.table.api.scala._
    import org.apache.flink.table.descriptors.{Csv, FileSystem, Schema}
    import org.apache.flink.types.Row
    
    import java.util.Properties
    /**
     * 创建样例类, 温度传感器 sensor
     */
    //case class SensorReading(id: String, timestamp: Long, temperature: Double)
    
    object TableTest {
      def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
        val tableEnv = StreamTableEnvironment.create(env)
        tableEnv.connect(new FileSystem().path(filePath))
          .withFormat(new Csv()) // 定义以csv格式进行数据格式化
          .withSchema(
            new Schema()
              .field("id", DataTypes.STRING())
              .field("timestamp", DataTypes.BIGINT())
              .field("temperature", DataTypes.DOUBLE())
          )
          .createTemporaryTable("sensorTable") // 创建临时表
        val inputTable = tableEnv.from("sensorTable")
        inputTable.toAppendStream[(String,Long,Double)].print("input file")
        env.execute()
      }
    
    }
    
    ```

    

### 3.2 连接kafaka

- kafka 的连接器 flink-kafka-connector 中， 1.10 版本的已经提供了 Table API 的支持。我们 可以在 connect 方法中直接传入一个叫做 Kafka 的类， 这就是 kafka 连接器的描述器 ConnectorDescriptor。

  ```scala
  tableEnv.connect(
  	new Kafka()
      .version("0.11") // 定义 kafka 的版本 
      .topic("sensor") // 定义主题 
      .property("zookeeper.connect", "localhost:2181")
      .property("bootstrap.servers", "localhost:9092")
  )
  .withFormat(new Csv()) 
  .withSchema(new Schema() 
              .field("id", DataTypes.STRING()) 
              .field("timestamp", DataTypes.BIGINT()) 
              .field("temperature", DataTypes.DOUBLE())
  )
  .createTemporaryTable("kafkaInputTable")
  ```

- 也可以连接到 ElasticSearch、MySql、HBase、Hive 等外部系统，实现方式基本上是 类似的。

### 3.3 表查询

- Table API 是集成在 Scala 和 Java 语言内的查询 API

- Table API 基于代表“表”的 Table 类，并提供一整套操作处理的方法 API；这 些方法会返回一个新的 Table 对象，表示对输入表应用转换操作的结果

- 有些关系型转换操作，可以由多个方法调用组成，构成链式调用结构

  ```scala
  val sensorTable: Table = tableEnv.from("inputTable") 
  val resultTable: Table = sensorTable 
  .select("id, temperature")
  .filter("id = 'sensor_1'")
  ```

- Flink 的 SQL 集成，基于实现 了SQL 标准的 Apache Calcite

- 在 Flink 中，用常规字符串来定义 SQL 查询语句

- SQL 查询的结果，也是一个新的 Table

  ```scala
  val resultSqlTable: Table = tableEnv
  .sqlQuery("select id, temperature from sensorTable where id ='sensor_1'")
  ```

### 3.4 将 DataStream 转换成表

> Flink 允许我们把 Table 和 DataStream 做转换：我们可以基于一个 DataStream，先流式 地读取数据源，然后 map 成样例类，再把它转成 Table。Table 的列字段（column fields），就 是样例类里的字段，这样就不用再麻烦地定义 schema 了。

- 代码表达

  - 代码中实现非常简单，直接用 tableEnv.fromDataStream()就可以了。默认转换后的 Table schema 和 DataStream 中的字段定义一一对应，也可以单独指定出来。
  - 这就允许我们更换字段的顺序、重命名，或者只选取某些字段出来，相当于做了一次 map 操作（或者 Table API 的 select 操作）。

  - code

    ```scala
    val inputStream: DataStream[String] = env.readTextFile("sensor.txt") 
    val dataStream: DataStream[SensorReading] = inputStream
    .map(data => { val dataArray = data.split(",") SensorReading(dataArray(0), dataArray(1).toLong, dataArray(2).toDouble) })
    val sensorTable: Table = tableEnv.fromDataStream(dataStream)
    val sensorTable2 = tableEnv.fromDataStream(dataStream, 'id, 'timestamp as 'ts)
    ```

- 数据类型与 Schema 的对应

  - DataStream 中的数据类型，与表的 Schema 之间的对应关系，可以有两种： 基于字段名称，或者基于字段位置

    1. 基于字段名称

       ```scala
       val sensorTable = tableEnv.fromDataStream(dataStream, 'timestamp as 'ts, 'id as 'myId, 'temperature)
       ```

    2. 基于字段位置

       ```scala
       val sensorTable = tableEnv.fromDataStream(dataStream, 'myId, 'ts)
       ```

### 3.5 表转换为DateStream

> - 表可以转换为 DataStream 或 DataSet。 这样，自定义流处理或批处理程序就可以继续在 Table API 或 SQL 查询的结果上运行了。
>
> - 将表转换为 DataStream 或 DataSet 时，需要指定生成的数据类型，即要将表的每一行转 换成的数据类型。通常，最方便的转换类型就是 Row。 当然，因为结果的所有字段类型都是 明确的，我们也经常会用元组类型来表示。
>
> - 表作为流式查询的结果，是动态更新的。所以，将这种动态查询转换成的数据流，同样 需要对表的更新操作进行编码，进而有不同的转换模式。

- Table API 中表到 DataStream 有两种模式：

  1. 追加模式（Append Mode） 用于表只会被插入（Insert）操作更改的场景。

     ```scala
     val resultStream: DataStream[Row] = tableEnv.toAppendStream[Row](resultTable)
     ```

     

  2. 撤回模式（Retract Mode） 

     - 用于任何场景。有些类似于更新模式中 Retract 模式，它只有 Insert 和 Delete 两类操作。

     - 得到的数据会增加一个 Boolean 类型的标识位（返回的第一个字段），用它来表示到底 是新增的数据（Insert），还是被删除的数据（老数据， Delete）

       ```scala
       val aggResultStream: DataStream[(Boolean, (String, Long))] = tableEnv .toRetractStream[(String, Long)](aggResultTable)
       ```

       

  ```scala
  val resultStream: DataStream[Row] = tableEnv.toAppendStream[Row](resultTable)
  val aggResultStream: DataStream[(Boolean, (String, Long))] = tableEnv.toRetractStream[(String, Long)](aggResultTable)
  resultStream.print("result") aggResultStream.print("aggResult")
  ```

- 没有经过 groupby 之类聚合操作，可以直接用 **toAppendStream** 来转换；而如果 经过了聚合，有更新操作，一般就必须用 **toRetractDstream**。

### 3.6 创建临时视图（Temporary View）

> 和table 基本一样

- 基于 DataStream 创建临时视图

  ```scala
  tableEnv.createTemporaryView("sensorView", dataStream)
  
  tableEnv.createTemporaryView("sensorView", dataStream, 'id, 'temperature, 'timestamp as 'ts)
  ```

- 基于 Table 创建临时视图

  ```scala
  tableEnv.createTemporaryView("sensorView", sensorTable)
  ```

## 4. 数据输出

> - 表的输出，是通过将数据写入 TableSink 来实现的
>
> - TableSink 是一个通用接口，可以支持不同的文件格式、存储数据库和消息队列
>
> - 输出表最直接的方法，就是通过 Table.insertInto() 方法将一个 Table 写入注册 过的 TableSink 中

### 4.1 输出到文件

```scala
package com.atguigu.apitest

import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.DataTypes
import org.apache.flink.table.api.scala._
import org.apache.flink.table.descriptors.{Csv, FileSystem, Schema}
import org.apache.flink.types.Row

import java.util.Properties
/**
 * 创建样例类, 温度传感器 sensor
 */
//case class SensorReading(id: String, timestamp: Long, temperature: Double)

import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.DataTypes
import org.apache.flink.table.api.scala._
import org.apache.flink.table.descriptors.{Csv, FileSystem, Schema}
import org.apache.flink.types.Row

object TableTest {
  def main(args: Array[String]): Unit = {
    // 1. 创建环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val tableEnv = StreamTableEnvironment.create(env)

    // 2. 连接外部系统，读取数据，注册表
    val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    tableEnv.connect(new FileSystem().path(filePath))
      .withFormat(new Csv())
      .withSchema(new Schema()
        .field("id", DataTypes.STRING())
        .field("timestamp", DataTypes.BIGINT())
        .field("temp", DataTypes.DOUBLE())
        //        .field("pt", DataTypes.TIMESTAMP(3)).proctime()
      )
      .createTemporaryTable("inputTable")

    // 3. 转换操作
    val sensorTable = tableEnv.from("inputTable")
    // 3.1 简单转换
    val resultTable = sensorTable
      .select('id, 'temp)
//      .filter('id === "sensor_1")

    // 3.2 聚合转换
//    val aggTable = sensorTable
//      .groupBy('id)    // 基于id分组
//      .select('id, 'id.count as 'cnt)

    // 4. 输出到文件
    // 注册输出表
    val outputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/output.txt"
    tableEnv.connect(new FileSystem().path(outputPath))
      .withFormat(new Csv())
      .withSchema(new Schema()
        .field("id", DataTypes.STRING())
        .field("temp", DataTypes.DOUBLE())
      )
      .createTemporaryTable("outputTable")
    resultTable.insertInto("outputTable")
    resultTable.toAppendStream[(String, Double)].print("result")
//    aggTable.toRetractStream[Row].print("agg")

    env.execute()
  }
}


```



### 4.2 更新模式

> 在流处理过程中，表的处理并不像传统定义的那样简单。 对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行 转换。与外部系统交换的消息类型，由更新模式（update mode）指定。 

- Flink Table API 中的更新模式有以下三种：

  1. 追加模式（Append Mode）: 在追加模式下，表（动态表）和外部连接器只交换插入（Insert）消息。

  2. 撤回模式（Retract Mode）: 在撤回模式下，表和外部连接器交换的是：

     - 添加（Add）和撤回（Retract）消息。 
     - 插入（Insert）会被编码为添加消息； 
     - 删除（Delete）则编码为撤回消息； 
     - 更新（Update）则会编码为，已更新行（上一行）的撤回消息，和更新行（新行） 的添加消息。 

     `在此模式下，不能定义 key，这一点跟 upsert 模式完全不同。`

  3. Upsert（更新插入）模式: 在 Upsert 模式下，动态表和外部连接器交换 Upsert 和 Delete 消息。 这个模式需要一个唯一的 key， 通过这个 key 可以传递更新消息。为了正确应用消息， 外部连接器需要知道这个唯一 key 的属性。

     - 插入（Insert）和更新（Update）都被编码为 Upsert 消息； 
     - 删除（Delete）编码为 Delete 信息。

     `这种模式和 Retract 模式的主要区别在于， Update 操作是用单个消息编码的，所以效率 会更高。`

​	

### 4.3  输出到 Kafka

```scala
// 输出到 kafka

tableEnv.connect(

new Kafka()
  .version("0.11") 
  .topic("sinkTest") 
  .property("zookeeper.connect", "localhost:2181") 
  .property("bootstrap.servers", "localhost:9092")
)
.withFormat( new Csv() ) 
.withSchema( new Schema()
.field("id", DataTypes.STRING())
.field("temp", DataTypes.DOUBLE()) 
           ) 
.createTemporaryTable("kafkaOutputTable")
resultTable.insertInto("kafkaOutputTable")
```



### 4.4 输出到es

> ElasticSearch 的 connector 可以在 upsert（update+insert， 更新插入）模式下操作，这样 就可以使用 Query 定义的键（key）与外部系统交换 UPSERT/DELETE 消息。
>
> 另外，对于“仅追加”（append-only）的查询， connector 还可以在 append 模式下操作， 这样就可以与外部系统只交换 insert 消息。

- 需要引入 依赖：

  ```xml
  <dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-json</artifactId>
  <version>1.10.1</version>
  </dependency>
  ```

  

- demo

  ```scala
  // 输出到 es
  
  tableEnv.connect(
    new Elasticsearch() 
      .version("6") 
      .host("localhost", 9200, "http") 
      .index("sensor") 
      .documentType("temp")
  )
  .inUpsertMode()  // 指定是 Upsert 模式
  .withFormat(new Json()) 
  .withSchema( new Schema()
  .field("id", DataTypes.STRING())
  .field("count", DataTypes.BIGINT()) ) 
  .createTemporaryTable("esOutputTable")
  
  aggResultTable.insertInto("esOutputTable")
  ```

  

### 4.5 输出到Mysql

> jdbc 连接的代码实现比较特殊，因为没有对应的 java/scala 类实现 ConnectorDescriptor， 所以不能直接 tableEnv.connect()。不过 Flink SQL 留下了执行 DDL 的接口：tableEnv.sqlUpdate()。

- 引入依赖

  ```xml
  <dependency>
  <groupId>org.apache.flink</groupId> 
  <artifactId>flink-jdbc_2.12</artifactId>
  <version>1.10.1</version>
  </dependency>
  ```

- 输出到mysql

  ```scala
  // 输出到 Mysql: 1. create table jdbcOutputTable 是在flink 的表环境中创建表, 2. 'connector.table' = 'sensor_count', 插入mysql的实体表
  
  val sinkDDL: String =""" 
  |create table jdbcOutputTable ( 
  | id varchar(20) not null, 
  | cnt bigint not null ) 
  | with (
  | 'connector.type' = 'jdbc',
  | 'connector.url' = 'jdbc:mysql://localhost:3306/test',
  | 'connector.table' = 'sensor_count',
  | 'connector.driver' = 'com.mysql.jdbc.Driver',
  | 'connector.username' = 'root',
  | 'connector.password' = '123456'
  |) 
  """.stripMargin
  
  tableEnv.sqlUpdate(sinkDDL) 
  aggResultSqlTable.insertInto("jdbcOutputTable")
  ```

- 这是另外一种输出方式(推荐), 也可以手动在table env建表输出

### 4.6 查看执行计划

- Table API 提供了一种机制来解释计算表的逻辑和优化查询计划

- 查看执行计划，可以通过 `TableEnvironment.explain(table)` 方法或 `TableEnvironment.explain() `方法完成，返回一个字符串，描述三个计划

  1. 优化的逻辑查询计划
  2. 优化后的逻辑查询计划
  3. 实际执行计划。

  ```scala
  val explaination: String = tableEnv.explain(resultTable) println(explaination)
  ```

  

## 5. 流处理和关系代数（表，及 SQL）的区别

- 其实关系代数（主要就是指关系型数据库中的表）和 SQL，主要就是针对批 处理的

![image-20220111160003058](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220111160003058.png)
