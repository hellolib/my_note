# 一. 动态表

## 1. 动态表和持续查询

- 动态表是 Flink 对流数据的 Table API 和 SQL 支持的核心概念

- 与表示批处理数据的静态表不同，动态表是随时间变化的

> - 持续查询（Continuous Query）: 动态表可以像静态的批处理表一样进行查询，查询一个动态表会产生持续查 询（Continuous Query）
> - 连续查询永远不会终止，并会生成另一个动态表
> - 查询会不断更新其动态结果表，以反映其动态输入表上的更改

![image-20220111235826572](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220111235826572.png)

- 流式表查询的处理过程：
  1. 流被转换为动态表
  2. 对动态表计算连续查询，生成新的动态表
  3. 生成的动态表被转换回流

## 2. 将流转换成动态表

- 为了处理带有关系查询的流，必须先将其转换为表

- 从概念上讲，流的每个数据记录，都被解释为对结果表的插入

- insert(修改操作)

  

  ![image-20220112000013457](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220112000013457.png)

## 3. 持续查询

- 持续查询会在动态表上做计算处理，并作为结果生成新的动态表

  ![image-20220112000206994](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220112000206994.png)

## 4. 将动态表转换成 DataStream

> - 与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删 除（Delete）更改，进行持续的修改
> - 将动态表转换为流或将其写入外部系统时，需要对这些更改进行编码

- 仅追加（Append-only）流

  - 仅通过插入（Insert）更改来修改的动态表，可以直接转换为仅追加流

- 撤回（Retract）流

  - 撤回流是包含两类消息的流：添加（Add）消息和撤回（Retract）消息

  ![image-20220112000628408](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220112000628408.png)

- Upsert（更新插入）流

  - Upsert 流也包含两种类型的消息：Upsert 消息和删除（Delete）消息。

  ![](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220112000646447.png)



# 二. 时间特性

> - 基于时间的操作（比如 Table API 和 SQL 中窗口操作），需要定义相关的时 间语义和时间数据来源的信息
>
> - Table 可以提供一个逻辑上的时间字段，用于在表处理程序中，指示时间和访 问相应的时间戳
>
> -  时间属性，可以是每个表schema的一部分。一旦定义了时间属性，它就可以 作为一个字段引用，并且可以在基于时间的操作中使用
>
> - 时间属性的行为类似于常规时间戳，可以访问，并且进行计算

## 1. 定义处理时间（Processing Time）

> 处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间 的最简单概念。它既不需要提取时间戳，也不需要生成 watermark

### 1.1 由 DataStream 转换成表时指定

- 在定义Schema期间，可以使用.proctime，指定字段名定义处理时间字段

- 这个proctime属性只能通过附加逻辑字段，来扩展物理schema。因此，只能 在schema定义的末尾定义它

  ```scala
  val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp, 'pt.proctime)
  ```

  

### 1.2 定义 Table Schema 时指定

```scala
.withSchema(
  new Schema()
	.field("id", DataTypes.STRING()) 
  .field("timestamp", DataTypes.BIGINT()) 
  .field("temperature", DataTypes.DOUBLE()) 
  .field("pt", DataTypes.TIMESTAMP(3)) 
  .proctime() // 定义 Table Schema 时指定
)
```

### 1.3 在创建表的 DDL 中定义(推荐)

```scala
val sinkDDL: String = """ 
|create table dataTable ( 
| id varchar(20) not null, 
| ts bigint, 
| temperature double, 
| pt AS PROCTIME() 
|) with ( 
| 'connector.type'='filesystem',
| 'connector.path'='/sensor.txt',
|'format.type'='csv'
|) """.stripMargin 
tableEnv.sqlUpdate(sinkDDL)
```

## 2. 定义事件时间（Event Time）

> - 事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样 即使在有乱序事件或者延迟事件时，也可以获得正确的结果。
>
> - 为了处理无序事件，并区分流中的准时和迟到事件；Flink 需要从事件数据中， 提取时间戳，并用来推进事件时间的进展

### 2.1 由 DataStream 转换成表时指定

- 在 DataStream 转换成 Table，使用 .rowtime 可以定义事件时间属性

  ```scala
  // 将 DataStream转换为 Table，并指定时间字段
  val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'timestamp.rowtime, 'temperature)
  
  // 或者，直接追加时间字段
  val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp, 'rt.rowtime)
  ```

  

### 2.2 定义 Table Schema 时指定

```scala
.withSchema(
  new Schema()
  .field("id", DataTypes.STRING()) 
  .field("timestamp", DataTypes.BIGINT()) 
  .rowtime(
    new Rowtime()
    .timestampsFromField("timestamp")  // 从字段中提取时间戳 
    .watermarksPeriodicBounded(1000)   // watermark延迟1秒
  ) 
  .field("temperature", DataTypes.DOUBLE())
)
```



### 2.3 在创建表的 DDL 中定义

```scala
val sinkDDL: String = """ 
|create table dataTable ( 
| id varchar(20) not null, 
| ts bigint, 
| temperature double, 
| rt AS TO_TIMESTAMP( FROM_UNIXTIME(ts) ), 
| watermark for rt as rt - interval '1' second
|) with ( 
| 'connector.type'='filesystem',
| 'connector.path'='/sensor.txt',
| 'format.type'='csv'
|) """.stripMargin 
tableEnv.sqlUpdate(sinkDDL)
```



# 三. 窗口

> 时间语义，要配合窗口操作才能发挥作用

- 在 Table API 和 SQL 中，主要有两种窗口
  1. Group Windows（分组窗口）
     - 根据时间或行计数间隔，将行聚合到有限的组（Group）中，并对每个组的数据执 行一次聚合函数
  2. Over Windows
     - 针对每个输入行，计算相邻行范围内的聚合

## 1. TableAPI Group Windows

- Group Windows 是使用 window（w:GroupWindow）子句定义的，并且必 须由as子句指定一个别名。

- 为了按窗口对表进行分组，窗口的别名必须在 group by 子句中，像常规的 分组字段一样引用

  ```scala
  val table = input
  .window([w: GroupWindow] as 'w) // 定义窗口，别名为 w 
  .groupBy('w, 'a) // 按照字段 a和窗口 w分组 
  .select('a, 'b.sum) // 聚合
  ```

- Table API 提供了一组具有特定语义的预定义 Window 类，这些类会被转换 为底层 DataStream 或 DataSet 的窗口操作

### 1.1 滚动窗口（Tumbling windows）

```scala
// Tumbling Event-time Window  事件时间滚动窗口
.window(Tumble over 10.minutes on 'rowtime as 'w)

// Tumbling Processing-time Window  处理时间滚动窗口
.window(Tumble over 10.minutes on 'proctime as 'w)

// Tumbling Row-count Window  计数滚动窗口
.window(Tumble over 10.rows on 'proctime as 'w)
```

### 1.2 滑动窗口（Sliding windows）

```scala
// Sliding Event-time Window 事件滑动窗口
.window(Slide over 10.minutes every 5.minutes on 'rowtime as 'w)

// Sliding Processing-time window  处理时间滑动窗口
.window(Slide over 10.minutes every 5.minutes on 'proctime as 'w)

// Sliding Row-count window 
.window(Slide over 10.rows every 5.rows on 'proctime as 'w)
```

### 1.3 会话窗口（Session windows）

```scala
// Session Event-time Window  
.window(Session withGap 10.minutes on 'rowtime as 'w)

// Session Processing-time Window  
.window(Session withGap 10.minutes on 'proctime as 'w)
```

## 2. SQL Group Windows

> Group Windows 定义在 SQL 查询的 Group By 子句中

1. TUMBLE(time_attr, interval)
   - 定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度 

2. HOP(time_attr, interval, interval)
   - 定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是 窗口长度

3. SESSION(time_attr, interval)
   - 定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔

## 3. Over Window

- Over window 聚合是标准 SQL 中已有的（over 子句），可以在查询的 SELECT 子句中定义

- Over window 聚合，会针对每个输入行，计算相邻行范围内的聚合

- Over windows 使用 window（w:overwindows*）子句定义，并在 select（） 方法中通过别名来引用

  ```scala
  val table = input
  .window([w: OverWindow] as 'w)
  .select('a, 'b.sum over 'w, 'c.min over 'w)
  ```

- Table API 提供了 Over 类，来配置 Over 窗口的属性

### 3.1 无界OverWindow

- 可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定 义 Over windows

- 界的 over window 是使用常量指定的

  ```scala
  // 无界的事件时间 over window
  .window(Over partitionBy 'a orderBy 'rowtime preceding UNBOUNDED_RANGE as 'w)
  
  //无界的处理时间 over window
  .window(Over partitionBy 'a orderBy 'proctime preceding UNBOUNDED_RANGE as 'w)
  
  // 无界的事件时间 Row-count over window
  .window(Over partitionBy 'a orderBy 'rowtime preceding UNBOUNDED_ROW as 'w)
  
  //无界的处理时间 Row-count over window
  .window(Over partitionBy 'a orderBy 'proctime preceding UNBOUNDED_ROW as 'w)
  ```

  

### 3.2 有界OverWindow

- 有界的 over window 是用间隔的大小指定的

  ```scala
  // 有界的事件时间 over window
  .window(Over partitionBy 'a orderBy 'rowtime preceding 1.minutes as 'w)
  
  // 有界的处理时间 over window
  .window(Over partitionBy 'a orderBy 'proctime preceding 1.minutes as 'w)
  
  // 有界的事件时间 Row-count over window
  .window(Over partitionBy 'a orderBy 'rowtime preceding 10.rows as 'w)
  
  // 有界的处理时间 Row-count over window
  .window(Over partitionBy 'a orderBy 'proctime preceding 10.rows as 'w)
  ```

### 3.3 SQL 中的 Group Windows

- Group Windows 定义在 SQL 查询的 Group By 子句中

1. TUMBLE(time_attr, interval) 
   - 定义一个滚动窗口，第一个参数是时间字段，第二个参数是窗口长度
2. HOP(time_attr, interval, interval)
   - 定义一个滑动窗口，第一个参数是时间字段，第二个参数是窗口滑动步长，第三个是 窗口长度
3. SESSION(time_attr, interval)
   - 定义一个会话窗口，第一个参数是时间字段，第二个参数是窗口间隔

### 3.4 SQL 中的 Over Windows

- 用 Over 做窗口聚合时，所有聚合必须在同一窗口上定义，也就是说必须是 相同的分区、排序和范围

- 目前仅支持在当前行范围之前的窗口

-  ORDER BY 必须在单一的时间属性上指定

  ```scala
  SELECT COUNT(amount) OVER ( PARTITION BY user ORDER BY proctime ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) FROM Orders
  ```

  

## 4. window demo

```scala
package com.atguigu.tableTest

import com.atguigu.apitest.SensorReading
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.table.api.{EnvironmentSettings, Over, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.types.Row

/**
 * 创建样例类, 温度传感器 sensor
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

object GroupTest {
  def main(args: Array[String]): Unit = {
    // 1. 创建环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

//    val settings = EnvironmentSettings
//      .newInstance()
//      .useBlinkPlanner()
//      .inStreamingMode()
//      .build()
    val tableEnv = StreamTableEnvironment.create(env)

    // 读取数据
    val inputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream = env.readTextFile(inputPath)
    //    val inputStream = env.socketTextStream("localhost", 7777)

    // 先转换成样例类类型（简单转换操作）
    val dataStream = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
      })
      .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
        override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
      })

    val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp.rowtime as 'ts)

    // 1. Group Window
    // 1.1 table api
    val resultTable = sensorTable
      .window(Tumble over 10.seconds on 'ts as 'tw)     // 每10秒统计一次，滚动时间窗口
      .groupBy('id, 'tw)
      .select('id, 'id.count, 'temperature.avg, 'tw.end)

    // 1.2 sql
    tableEnv.createTemporaryView("sensor", sensorTable)
    val resultSqlTable = tableEnv.sqlQuery(
      """
        |select
        |  id,
        |  count(id),
        |  avg(temperature),
        |  tumble_end(ts, interval '10' second)
        |from sensor
        |group by
        |  id,
        |  tumble(ts, interval '10' second)
      """.stripMargin)
    // 转换为流输出结果
    //    resultTable.toAppendStream[Row].print("resultTable")
        resultSqlTable.toRetractStream[Row].print("resultSQLTable")

    // 2. Over window：统计每个sensor每条数据，与之前两行数据的平均温度
    // 2.1 table api
    val overResultTable = sensorTable
      .window( Over partitionBy 'id orderBy 'ts preceding 2.rows as 'ow )
      .select('id, 'ts, 'id.count over 'ow, 'temperature.avg over 'ow)

    // 2.2 sql
    val overResultSqlTable = tableEnv.sqlQuery(
      """
        |select
        |  id,
        |  ts,
        |  count(id) over ow,
        |  avg(temperature) over ow
        |from sensor
        |window ow as (
        |  partition by id
        |  order by ts
        |  rows between 2 preceding and current row
        |)
      """.stripMargin)

    // 转换成流打印输出
    overResultTable.toAppendStream[Row].print("result")
    overResultSqlTable.toRetractStream[Row].print("sql")

    env.execute()
  }
}

```

