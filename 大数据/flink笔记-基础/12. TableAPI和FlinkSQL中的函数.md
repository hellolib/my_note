# 一. 内置函数

> Flink Table 和 SQL 内置了很多 SQL 中支持的函数；如果有无法满足的需要，则可以实 现用户自定义的函数（UDF）来解决。
>
> 以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。

1. **比较函数**

   - SQL

     ```scala
     - value1 = value2
     - value1 > value2
     ```

   - TableAPI

     ```scala
     - ANY1 === ANY2
     - ANY1 > ANY2
     ```
     
   
2. **逻辑函数**

   - SQL

     ```scala
     - boolean1 OR boolean2
     - boolean IS FALSE
     - NOT boolean
     ```
   
   - TableAPI
   
     ```scala
     - BOOLEAN1 || BOOLEAN2
     - BOOLEAN.isFalse
     - !BOOLEAN
     ```

3. **算术函数**

   - SQL

     ```scala
     - numeric1 + numeric2
     - POWER(numeric1, numeric2)
     ```

   - TableAPI
   
     ```scala
     - NUMERIC1 + NUMERIC2
     
     - NUMERIC1.power(NUMERIC2)
     ```

4. **字符串函数**

   - SQL

     ```scala
     - string1 || string2
     - UPPER(string)
     - CHAR_LENGTH(string)
     ```
   
   - TableAPI
   
     ```scala
     - STRING1 + STRING2
     - STRING.upperCase()
     - STRING.charLength()
     ```

5. **时间函数**

   - SQL

     ```scala
     - DATE string
     - TIMESTAMP string
     - CURRENT_TIME
     - INTERVAL string range
     ```
   
   - TableAPI
   
     ```scala
     - STRING.toDate
     - STRING.toTimestamp
     - currentTime()
     - NUMERIC.days
     - NUMERIC.minutes
     ```

6. **聚合函数**
   - SQL

     ```scala
     - COUNT(*)
     - SUM(expression)
     - RANK()
     - ROW_NUMBER()
     ```
   
   - TableAPI
   
     ```scala
     - FIELD.count
     - FIELD.sum0   // 如果没有值的话为0
     ```



# 二. UDF 自定义函数

> - 用户定义函数（User-defined Functions，UDF）是一个重要的特性，它们显 著地扩展了查询的表达能力
>
> -  在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用
>
> - 函数通过调用` registerFunction（）`方法在 TableEnvironment 中注册。当用 户定义的函数被注册时，它被插入到 TableEnvironment 的函数目录中，这样 Table API 或 SQL 解析器就可以识别并正确地解释它

## 1. 标量函数（Scalar Functions）

> - 一对一
>
> - 用户定义的标量函数，可以将0、1或多个标量值，映射到新的标量值
>
> - 为了定义标量函数，必须在 org.apache.flink.table.functions 中扩展基类 Scalar Function，并实现（一个或多个）求值（eval）方法
>
> - 标量函数的行为由求值方法决定，求值方法必须公开声明并命名为 eval
>
>   ```scala
>   class HashCode( factor: Int ) extends ScalarFunction { 
>     def eval( s: String ): Int = { 
>       s.hashCode * factor 
>     }
>   }
>   ```
>

```scala
package com.atguigu.tableTest

import com.atguigu.apitest.SensorReading
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.table.api.{EnvironmentSettings, Over, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.table.functions.ScalarFunction
import org.apache.flink.types.Row

/**
 * 创建样例类, 温度传感器 sensor
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

/**
 * 自定义标量函数
 */
class HashCode(factor: Int) extends ScalarFunction {
  def eval(s: String): Int = {
    s.hashCode * factor
  }
}

object FuncTest {
  def main(args: Array[String]): Unit = {
    // 1. 创建环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val settings = EnvironmentSettings
      .newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tableEnv = StreamTableEnvironment.create(env, settings)

    // 读取数据
    val inputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream = env.readTextFile(inputPath)
    //    val inputStream = env.socketTextStream("localhost", 7777)

    // 先转换成样例类类型（简单转换操作）
    val dataStream = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
      })
      .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
        override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
      })

    val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp.rowtime as 'ts)
    //1. tableapi 调用自定义函数
    val hashCode = new HashCode(10)
    val resultTable = sensorTable
      .select('id,'ts,hashCode('id))
    // 2.sql 调用自定义函数
    // 需要先注册自定义函数
    tableEnv.createTemporaryView("sensor",sensorTable)
    tableEnv.registerFunction("hashCode", hashCode)
    val resultSQLTable  = tableEnv.sqlQuery("select id , ts ,hashCode(id) from sensor")
    resultSQLTable.toAppendStream[Row].print()
    resultSQLTable.toAppendStream[Row].print("result")
    env.execute()
  }
}

```



## 2. 表函数（Table Functions）

> - 一对多
> - 与用户定义的标量函数类似，用户定义的表函数，可以将 0、1 或多个标量值作为输入 参数；与标量函数不同的是，它可以返回任意数量的行作为输出，而不是单个值。
>
> - 为了定义一个表函数，必须扩展 `org.apache.flink.table.functions` 中的基类` TableFunction` 并实现（一个或多个）求值方法。表函数的行为由其求值方法决定，求值方法必须是 public 的，并命名为 eval。 求值方法的参数类型，决定表函数的所有有效参数。
>
> - 表函数的行为由其求值方法决定，求值方法必须是 public 的，并命名为 eval
>
>   ```scala
>   class Split(separator: String) extends TableFunction[(String, Int)]{ 
>     def eval(str: String): Unit = { 
>       str.split(separator).foreach( 
>         word => collect((word, word.length)) 
>       ) 
>     }
>   }
>   ```

```scala
package com.atguigu.tableTest

import com.atguigu.apitest.SensorReading
import org.apache.commons.math3.geometry.spherical.oned.ArcsSet.Split
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.table.api.{EnvironmentSettings, Over, Tumble}
import org.apache.flink.table.api.scala._
import org.apache.flink.table.functions.{ScalarFunction, TableFunction}
import org.apache.flink.types.Row

/**
 * 创建样例类, 温度传感器 sensor
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

/**
 * 自定义表函数
 */
// 自定义 TableFunction

class Split(separator: String) extends TableFunction[(String, Int)] {
  def eval(str: String): Unit = {
    str.split(separator).foreach(
      word => collect((word, word.length))
    )
  }
}

object FuncTest {
  def main(args: Array[String]): Unit = {
    // 1. 创建环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val settings = EnvironmentSettings
      .newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tableEnv = StreamTableEnvironment.create(env, settings)

    // 读取数据
    val inputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream = env.readTextFile(inputPath)
    //    val inputStream = env.socketTextStream("localhost", 7777)

    // 先转换成样例类类型（简单转换操作）
    val dataStream = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
      })
      .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
        override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
      })

    val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp.rowtime as 'ts)
    val split = new Split("_")
    //1. tableAPI 调用自定义函数
    val resultTable = sensorTable
      .joinLateral(split('id) as ('word,'length))
      .select('id,'ts,'word,'length)
    // 2.sql 调用自定义函数
    // 需要先注册自定义函数
    tableEnv.createTemporaryView("sensor", sensorTable)
    tableEnv.registerFunction("split", split)
    val resultSQLTable = tableEnv.sqlQuery(
      """
        |select
        |id,ts,word,length
        |from
        |sensor ,lateral table( split(id) ) as splitid(word,length)
        |""".stripMargin
    )
    resultTable.toAppendStream[Row].print("Table")
    resultSQLTable.toAppendStream[Row].print("SQL")
    env.execute("table UDF")
  }
}

/*
Table> sensor_1,2019-01-17T09:43:19,sensor,6
Table> sensor_1,2019-01-17T09:43:19,1,1
SQL> sensor_1,2019-01-17T09:43:19,sensor,6
SQL> sensor_1,2019-01-17T09:43:19,1,1
*/
```



## 3. 聚合函数（Aggregate Functions）

> - 多对一
> - 用户自定义聚合函数（User-Defined Aggregate Functions，UDAGGs）可以 把一个表中的数据，聚合成一个标量值
> - 用户定义的聚合函数，是通过继承 AggregateFunction 抽象类实现的
> - 工作原理
>   - 首先，它需要一个累加器，用来保存聚合中间结果的数据结构（状态）。可以通过 调用 AggregateFunction 的 createAccumulator（）方法创建空累加器。
>   - 随后，对每个输入行调用函数的 accumulate（）方法来更新累加器。 
>   -  处理完所有行后，将调用函数的 getValue（）方法来计算并返回最终结果。
> - AggregationFunction 要求必须实现的方法：
>   1. `createAccumulator()`
>   2. `accumulate()`
>   3. `getValue()`

- 示例
  - 假设现在有一张表，包含了各种饮料的数据。该表由三列（id、name 和 price） 、五行 组成数据。现在我们需要找到表中所有饮料的最高价格，即执行 max（）聚合， 结果将是一 个数值。
  - 

<img src="/Users/liusaisai/Library/Application Support/typora-user-images/image-20220113215922893.png" alt="image-20220113215922893" style="zoom:50%;" />

- code

  ```scala
  package com.atguigu.tableTest
  
  import com.atguigu.apitest.SensorReading
  import org.apache.commons.math3.geometry.spherical.oned.ArcsSet.Split
  import org.apache.flink.streaming.api.TimeCharacteristic
  import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.api.windowing.time.Time
  import org.apache.flink.table.api.{EnvironmentSettings, Over, Tumble}
  import org.apache.flink.table.api.scala._
  import org.apache.flink.table.functions.{AggregateFunction, ScalarFunction, TableFunction}
  import org.apache.flink.types.Row
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  /**
   * 定义 AggregateFunction 的 Accumulator
   */
  class AvgTempAcc {
    var sum: Double = 0.0
    var count: Int = 0
  }
  /**
   * 自定义聚合函数, 求每个温度传感器的平均温度值
   */
  class AvgTemp extends AggregateFunction[Double, AvgTempAcc] { // [Double,AvgTempAcc] [温度值,(当前温度值求和,温度个数)]
    override def getValue(acc: AvgTempAcc): Double = acc.sum / acc.count // 当前温度值求和/温度个数
  
    override def createAccumulator(): AvgTempAcc = new AvgTempAcc // 累加器默认都是0
  
    // 还要再实现一个具体的处理计数函数 accumulate
    def accumulate(acc: AvgTempAcc, temp: Double): Unit = {
      acc.sum += temp
      acc.count += 1
    }
  
  }
  
  object FuncTest {
    def main(args: Array[String]): Unit = {
      // 1. 创建环境
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
  
      val settings = EnvironmentSettings
        .newInstance()
        .useBlinkPlanner()
        .inStreamingMode()
        .build()
      val tableEnv = StreamTableEnvironment.create(env, settings)
  
      // 读取数据
      val inputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream = env.readTextFile(inputPath)
      //    val inputStream = env.socketTextStream("localhost", 7777)
  
      // 先转换成样例类类型（简单转换操作）
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
        })
        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
          override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
        })
  
      val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp.rowtime as 'ts)
      val avgTemp = new AvgTemp()
      //1. tableAPI 调用自定义函数
      val resultTable = sensorTable
        .groupBy('id)
        .aggregate(avgTemp('temperature) as 'avgTemperature)
        .select('id,'avgTemperature)
      // 2.sql 调用自定义函数
      // 需要先注册自定义函数
      tableEnv.createTemporaryView("sensor", sensorTable)
      tableEnv.registerFunction("avgTemp", avgTemp)
      val resultSQLTable = tableEnv.sqlQuery(
        """
          |select id, avgTemp(temperature) as avgTemperature
          |from
          |sensor
          |group by id
          |""".stripMargin)
      resultTable.toRetractStream[Row].print("Table")
      resultSQLTable.toRetractStream[Row].print("SQL")
      env.execute("table UDF")
    }
  }
  
  ```

  

## 4. 表聚合函数（Table Aggregate Functions）

> - 多对多: 应用Top N
>
> - 用户定义的表聚合函数（User-Defined Table Aggregate Functions， UDTAGGs），可以把一个表中数据，聚合为具有多行和多列的结果表
>
> - 用户定义表聚合函数，是通过继承 TableAggregateFunction 抽象类来实现的
>
>   ![image-20220113222617155](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220113222617155.png)
>
> - AggregationFunction 要求必须实现的方法：
>
>   1. createAccumulator()
>
>   2. accumulate()
>
>   3. emitValue()
>
> - TableAggregateFunction 的工作原理如下:
>
>   - 首先，它同样需要一个累加器（Accumulator），它是保存聚合中间结果的数据结 构。通过调用 createAccumulator() 方法可以创建空累加器。
>   - 随后，对每个输入行调用函数的 accumulate() 方法来更新累加器。
>   - 处理完所有行后，将调用函数的 emitValue() 方法来计算并返回最终结果。

- code

  ```scala
  package com.atguigu.tableTest
  
  import com.atguigu.apitest.SensorReading
  import org.apache.commons.math3.geometry.spherical.oned.ArcsSet.Split
  import org.apache.flink.streaming.api.TimeCharacteristic
  import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.api.windowing.time.Time
  import org.apache.flink.table.api.{EnvironmentSettings, Over, Tumble}
  import org.apache.flink.table.api.scala._
  import org.apache.flink.table.functions.{AggregateFunction, ScalarFunction, TableAggregateFunction, TableFunction}
  import org.apache.flink.types.Row
  import org.apache.flink.util.Collector
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  /**
   * 定义 Top2TempAcc 的 Accumulator
   */
  // 先定义一个 Accumulator
  
  class Top2TempAcc {
    var highestTemp: Double = Int.MinValue // 最小的int值
    var secondHighestTemp: Double = Int.MinValue
  }
  
  /**
   * 自定义表聚合函数, 提取最高的两个温度值
   */
  class Top2Temp extends TableAggregateFunction[(Double, Int), Top2TempAcc] {
    override def createAccumulator(): Top2TempAcc = new Top2TempAcc
  
    def accumulate(acc: Top2TempAcc, temp: Double): Unit = {
      if (temp > acc.highestTemp) {
        acc.secondHighestTemp = acc.highestTemp
        acc.highestTemp = temp
      } else if (temp > acc.secondHighestTemp) {
        acc.secondHighestTemp = temp
      }
    }
  
    def emitValue(acc: Top2TempAcc, out: Collector[(Double, Int)]): Unit = {
      out.collect(acc.highestTemp, 1)
      out.collect(acc.secondHighestTemp, 2)
    }
  }
  
  object FuncTest {
    def main(args: Array[String]): Unit = {
      // 1. 创建环境
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
  
      val settings = EnvironmentSettings
        .newInstance()
        .useBlinkPlanner()
        .inStreamingMode()
        .build()
      val tableEnv = StreamTableEnvironment.create(env, settings)
  
      // 读取数据
      val inputPath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream = env.readTextFile(inputPath)
      //    val inputStream = env.socketTextStream("localhost", 7777)
  
      // 先转换成样例类类型（简单转换操作）
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0), arr(1).toLong, arr(2).toDouble)
        })
        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[SensorReading](Time.seconds(1)) {
          override def extractTimestamp(element: SensorReading): Long = element.timestamp * 1000L
        })
  
      val sensorTable = tableEnv.fromDataStream(dataStream, 'id, 'temperature, 'timestamp.rowtime as 'ts)
      val top2Temp = new Top2Temp()
      //1. tableAPI 调用自定义函数
      val resultTable = sensorTable
        .groupBy('id)
        .flatAggregate(top2Temp('temperature) as('temp, 'rank))
        .select('id, 'temp, 'rank)
      // 2.sql 无法调用
      resultTable.toRetractStream[Row].print("Table")
      env.execute("table UDF")
    }
  }
  
  ```

  
