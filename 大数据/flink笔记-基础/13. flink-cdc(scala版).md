# 一. CDC简介

> - CDC 是 Change Data Capture（变更数据获取）的简称。核心思想是，监测并捕获数据库 的变动（包括数据或数据表的插入、更新以及删除等），将这些变更按发生的顺序完整记录 下来，写入到消息中间件中以供其他服务进行订阅及消费。

- CDC 主要分为基于查询和基于 Binlog 两种方式，我们主要了解一下这两种之间的区别

  ![image-20220120145141131](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220120145141131.png)

- flink-cdc : https://github.com/ververica/flink-cdc-connectors

- 支持连接的数据源

  |  Database  |                     Version                     |
  | :--------: | :---------------------------------------------: |
  |   MySQL    |    Database: 5.7, 8.0.x  JDBC Driver: 8.0.16    |
  | PostgreSQL | Database: 9.6, 10, 11, 12  JDBC Driver: 42.2.12 |
  |  MongoDB   | Database: 4.0, 4.2, 5.0  MongoDB Driver: 4.3.1  |
  |   Oracle   |  Database: 11, 12, 19  Oracle Driver: 19.3.0.0  |

- 版本关系

  | Flink CDC Connector Version | Flink Version |                       说明                       |
  | --------------------------- | ------------- | :----------------------------------------------: |
  | 1.0.0                       | 1.11.*        |                                                  |
  | 1.1.0                       | 1.11.*        |                                                  |
  | 1.2.0                       | 1.12.*        |                                                  |
  | 1.3.0                       | 1.12.*        |                                                  |
  | 1.4.0                       | 1.13.*        |                                                  |
  | **2.0.***                   | **1.13.***    | **1. 不支持oracle<br />2. 不需要源端表提供主键** |
  | **2.1.***                   | **1.13***     |    **1. 支持oracle<br />2. 需要源表提供主键**    |



# 二. Flink CDC 实战

## 1. DataStream 实现CDC

> 环境要求: 
>
> - flink 1.12.0 (Flink cluster with version 1.12+ and Java 8+)
> - flink-cdc-2.0.0 (flink-cdc 2.1.1 需要源端表必须有主键)

### 1.1 maven 依赖导入

```xml
    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <flink.version>1.12.0</flink.version>
        <scala.binary.version>2.12</scala.binary.version>
        <kafka.version>2.4.0</kafka.version>
    </properties>
<dependencies>
  
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka_${scala.binary.version}</artifactId>
            <version>${kafka.version}</version>
        </dependency>
        <!-- 通用kafka 连接器 , 不需要指定版本-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-statebackend-rocksdb_2.12</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.76</version>
        </dependency>

        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.5.2</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.25</version>
        </dependency>

        <dependency>
            <groupId>com.ververica</groupId>
            <artifactId>flink-connector-mysql-cdc</artifactId>
            <version>2.0.0</version>
        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-table-planner-blink_2.12</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-java</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-streaming-java_2.12</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

        <groupId>org.apache.flink</groupId>

        <artifactId>flink-clients_2.12</artifactId>

        <version>1.12.0</version>

    </dependency>
    </dependencies>
```

### 1.2 代码编写

> mysql 查看是否开启binlog :` show VARIABLES like 'log_bin';`
>
> mysql 开启binlog: 
>
> ```sh
> #1og_bin
> 1og-bin = mysql-bin
> binlog-format = ROW 
> server_id =1#配置mysql replication需要定义，不能和canal的slaveId重复
> ```

```scala
package com.dcits.cdc

import com.ververica.cdc.connectors.mysql.MySqlSource
import com.ververica.cdc.connectors.mysql.table.StartupOptions
import com.ververica.cdc.debezium.{DebeziumSourceFunction, StringDebeziumDeserializationSchema}
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._

object WatchMysqlChange {
  def main(args: Array[String]): Unit = {
    // 1. 初始化flink 环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)

    val mysqlSource: DebeziumSourceFunction[String] = MySqlSource
      .builder()
      .hostname("127.0.0.1")
      .port(3306)
      .username("root")
      .password("")
      .databaseList("sensor_db")
      .tableList("sensor_db.city", "sensor_db.goods") //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据，注意：指定的时候需要使用"db.table"的方式
      .startupOptions(StartupOptions.initial()) 
      .deserializer(new StringDebeziumDeserializationSchema()) // JsonDebeziumDeserializationSchema
      .build()
    // 使用 CDC Source 从 MySQL 读取数据
    val mysqlDS = env.addSource(mysqlSource)
    // todo 数据加工, 实时计算
    mysqlDS.print()
    env.execute()
  }
}

```

- 输出打印

  ```
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=1,city_name=北京},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013499,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013504}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=2,city_name=上海},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013512,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013512}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=3,city_name=河南},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013514,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013515}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=1,goods_name=黄金},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013526,snapshot=true,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013527}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=2,goods_name=钻石},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013528,snapshot=true,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013528}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=3,goods_name=稀有金属2},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013528,snapshot=last,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013528}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  
  ```

- 修改一条数据 , 输出打印

  ```
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={transaction_id=null, ts_sec=1642680729, file=binlog.000150, pos=571, row=1, server_id=1, event=2}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{before=Struct{id=3,goods_name=稀有金属2},after=Struct{id=3,goods_name=稀有金属},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680729000,db=sensor_db,table=goods,server_id=1,file=binlog.000150,pos=724,row=0},op=u,ts_ms=1642680729634}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  
  ```

  

### 1.3 startupOptions 参数说明

> mysql 为例

- **initial**: 先查询读取mysql 全量表数据, 然后切换至binlog, 读取最新数据
  - initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.
- **earliest**: 从binlog最开始的地方读取, 从表创建开始的全部记录,
  - latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.
- **latest**: 直接读取binlog最新的地方开始读取
- **specificOffset**: 指定offset读取binlog
  - specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset.
- **timestamp**: 指定时间戳读取binlog
  - /timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp. The consumer will traverse the binlog from the beginning and ignore change events whose timestamp is smaller than the specified timestamp.

### 1.4 测试

1. 打包上传jar包
2. 开启 MySQL Binlog 并重启 MySQL
3. 启动 Flink 集群`bin/start-cluster.sh`
4. 启动hdfs集群`start-dfs.sh`
5. 启动程序`bin/flink run -c com.dcits.FlinkCDC flink-1.0-SNAPSHOT-jar-with-dependencies.jar`
6. 在 MySQL 的 gmall-flink.z_user_info 表中添加、修改或者删除数据
7. 给当前的 Flink 程序创建 Savepoint`bin/flink savepoint JobId hdfs://hadoop102:8020/flink/save`
8. 关闭程序以后从 Savepoint 重启程序`bin/flink run -c com.dcits.FlinkCDC flink-1.0-SNAPSHOT-jar-with-dependencies.jar`

## 2. FlinkSQL 实现CDC

> 环境依赖
>
> - 源端表需要主键
>
> | Flink CDC Connector Version | Flink Version |
> | --------------------------- | ------------- |
> | 1.0.0                       | 1.11.*        |
> | 1.1.0                       | 1.11.*        |
> | 1.2.0                       | 1.12.*        |
> | 1.3.0                       | 1.12.*        |
> | **1.4.0**                   | **1.13.***    |
> | **2.0.***                   | **1.13.***    |

- 依赖引入

  ```xml
  <dependencies>
          <dependency>
              <groupId>com.ververica</groupId>
              <artifactId>flink-connector-mysql-cdc</artifactId>
              <version>2.0.1</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-table-planner-blink_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-java</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-java_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-clients_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-jdbc_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-csv</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-json</artifactId>
              <version>${flink.version}</version>
          </dependency>
      </dependencies>
  ```

```scala
package com.dcits.cdc

import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment
import org.apache.flink.types.Row

object FlinkSQLWatchMysqlChange {
  def main(args: Array[String]): Unit = {
    // 1. 初始化flink 环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
    // 创建table环境
    val bsSettings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tableEnv = StreamTableEnvironment.create(env,bsSettings)
    // **创建flink表字段大小写和原表保持一致!!!!!!**
    tableEnv.executeSql(
      """
        |CREATE TABLE goods (
        |id INT primary key,
        |goods_name STRING )
        |WITH
        |('connector' = 'mysql-cdc',
        |'scan.startup.mode' = 'initial',
        |'hostname' = '127.0.0.1',
        |'port' = '3306',
        |'username' = 'root',
        |'password' = '',
        |'database-name' = 'sensor_db',
        |'table-name' = 'goods'
        |)""".stripMargin)
    val table = tableEnv.sqlQuery("select * from goods")
    tableEnv.toRetractStream[Row](table).print()
    env.execute()
  }
}
```

- 注意事项:
  - **创建flink表字段大小写和原表保持一致!!!!!!**

# 三. 自定义反序列化器!!!

> - 自定义反序列化器可以 自定义数据源返回数据格式
> - 基于DataStream API 构建自定义反序列化器
> - **flink-cdc 2.1+ 提供json序列化器, 但是flink-cdc2.1+ 需要源端表必须有主键**

## 1. 构建反序列化器

```scala
package com.dcits.cdc

import com.ververica.cdc.connectors.mysql.MySqlSource
import com.ververica.cdc.connectors.mysql.table.StartupOptions
import com.ververica.cdc.debezium.{DebeziumDeserializationSchema, DebeziumSourceFunction, StringDebeziumDeserializationSchema}
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector
import org.apache.kafka.connect.source.SourceRecord
import io.debezium.data.Envelope
import com.alibaba.fastjson.{JSON, JSONObject}
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.kafka.connect.data.Struct

import javax.ws.rs.client.Entity.json

/**
 * 自定义反序列化器, 解析源端变动数据
 */
object CustomDeserializationWatchMysql {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
    val mysqlSource: DebeziumSourceFunction[String] = MySqlSource
      .builder()
      .hostname("127.0.0.1")
      .port(3306)
      .username("root")
      .password("")
      .databaseList("sensor_db")
      .tableList("sensor_db.city", "sensor_db.goods") //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据，注意：指定的时候需要使用"db.table"的方式
      .startupOptions(StartupOptions.initial())
      .deserializer(new MyDebeziumDeserialization())
      .build()

    env.addSource(mysqlSource).print()
    env.execute("custom deserialization")
  }

}

/**
 * 自定义反序列化解析器
 */

class MyDebeziumDeserialization extends DebeziumDeserializationSchema[String] {
  override def deserialize(sourceRecord: SourceRecord, collector: Collector[String]): Unit = {
    // 获取主题信息,包含着数据库和表名 mysql_binlog_source.sensor_db.goods
    val topic: String = sourceRecord.topic
    val arr: Array[String] = topic.split("\\.")
    val db: String = arr(1)
    val tableName: String = arr(2)
    //获取操作类型 READ DELETE UPDATE CREATE
    val operation = Envelope.operationFor(sourceRecord)
    //获取值信息并转换为 Struct 类型
    val value = sourceRecord.value().asInstanceOf[Struct]
    //获取变化后的数据
    val before = value.getStruct("before")
    //创建 JSON 对象用于存储数据信息
    val beforeData = new JSONObject()
    if (before != null){
      before.schema.fields.forEach(
        field =>{
          beforeData.put(field.name(),before.get(field))
        }
      )
    }
    //获取变化后的数据
    val after = value.getStruct("after")
    //创建 JSON 对象用于存储数据信息
    val afterData = new JSONObject()
    if (afterData != null){
      after.schema.fields.forEach(
        field =>{
          afterData.put(field.name(),after.get(field))
        }
      )
    }
    //发送数据至下游
    val result = new JSONObject()
    result.put("operation", operation.toString.toLowerCase)
    result.put("before_data", beforeData)
    result.put("after_data", afterData)
    result.put("database", db)
    result.put("table", tableName)
    collector.collect(result.toJSONString)


  }

  override def getProducedType: TypeInformation[String] = {
    TypeInformation.of(classOf[String])
  }
}
```

## 2. 格式化输出 源端 数据变更记录

- 全表扫描

  ```json
  {"database":"sensor_db","after_data":{"city_name":"北京","id":1},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"city_name":"上海","id":2},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"city_name":"河南","id":3},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"goods_name":"黄金","id":1},"before_data":{},"operation":"read","table":"goods"}
  {"database":"sensor_db","after_data":{"goods_name":"钻石","id":2},"before_data":{},"operation":"read","table":"goods"}
  {"database":"sensor_db","after_data":{"goods_name":"稀有金属","id":3},"before_data":{},"operation":"read","table":"goods"}
  ```

- 数据变更

  ```json
  {"database":"sensor_db","after_data":{"goods_name":"稀有金属2","id":3},"before_data":{"goods_name":"稀有金属","id":3},"operation":"update","table":"goods"}
  ```

  

## 3. 注意事项

- **注意, 将sourceRecord 转为Struct 后必须使用foreach方法获取深层次数据!!!**

  ```scala
      //获取值信息并转换为 Struct 类型
      val value = sourceRecord.value().asInstanceOf[Struct]
      //获取变化后的数据
      val before = value.getStruct("before")
      //创建 JSON 对象用于存储数据信息
      val beforeData = new JSONObject()
      if (before != null){
        before.schema.fields.forEach(
          field =>{
            beforeData.put(field.name(),before.get(field))
          }
        )
      }
  ```

  

# 四. Flink-CDC 2.0+ 解决问题

## 1. Flink-CDC 1.* 版本问题

1. 一致性通过加锁保证: Debesium在保证数据一致性时，需要对读取的库或者表加锁。

2. 不支持水平拓展: Flink-CDC只支持单并发，全量读取数据阶段，如果表数据据量级大，读取效率在小时级别。

3. 全量读取阶段不支持ckeckpoint

## 2. Flink-CDC 2.0+

> - 无锁
> - 支持水平扩展
> - 支持checkPoint

- 在对于有主键的表做初始化模式，整体的流程主要分为 5 个阶段：

  1. Chunk 切分；

     - 根据 Netflix DBlog 的论文中的无锁算法原理，对于目标表按照主键进行数据分片，设置每个切片的区间为左闭右开或者左开右闭来保证数据的连续性。

       ![image-20230512192813612](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-01/20230512192813.png)

  2. Chunk 分配；（实现并行读取数据&CheckPoint）

     - 将划分好的 Chunk 分发给多个 SourceReader，每个 SourceReader 读取表中的一部分数据，实现了并行读取的目标。
       同时在每个 Chunk 读取的时候可以单独做 CheckPoint，某个 Chunk 读取失败只需要单独执行该 Chunk 的任务，而不需要像 1.x 中失败了只能从头读取。
       若每个 SourceReader 保证了数据一致性，则全表就保证了数据一致性。

       <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220121015344790.png" alt="image-20220121015344790" style="zoom:50%;" />

  3. Chunk 读取；（实现无锁读取）

     读取可以分为 5 个阶段

     1）SourceReader 读取表数据之前先记录当前的 Binlog 位置信息记为低位点；
     2）SourceReader 将自身区间内的数据查询出来并放置在 buffer 中；
     3）查询完成之后记录当前的 Binlog 位置信息记为高位点；
     4）在增量部分消费从低位点到高位点的 Binlog； 

     5）根据主键，对 buffer 中的数据进行修正并输出。
     可以保证每个Chunk最终的输出就是在高位点时该Chunk中最新的数据，但是目前只是做到了保证单个 Chunk 中的数据一致性。

     <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220121015407817.png" alt="image-20220121015407817" style="zoom:50%;" />

  4. Chunk 汇报；

     在 Snapshot Chunk 读取完成之后，有一个汇报的流程，如上图所示，即 SourceReader 需要将 Snapshot Chunk 完成信息汇报给 SourceEnumerator。

     ![image-20220121015427276](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220121015427276.png)

  5. Chunk 分配

     FlinkCDC 是支持全量+增量数据同步的，在 SourceEnumerator 接收到所有的 Snapshot。Chunk 完成信息之后，还有一个消费增量数据（Binlog）的任务，此时是通过下发 Binlog Chunk给任意一个 SourceReader 进行单并发读取来实现的。