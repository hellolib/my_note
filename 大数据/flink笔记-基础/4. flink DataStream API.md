![image-20220104102449857](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104102449857.png)

## 1. environment

- **getExecutionEnvironment** 

  - 创建一个执行环境，表示当前执行程序的上下文。 
  - 如果程序是独立调用的，则 此方法返回本地执行环境；
  - 如果从命令行客户端调用程序以提交到集群， 则此方法 返回此集群的执行环境， 
  - 会根据查询运行的方 式决定返回什么样的运行环境， 是最常用的一种创建执行环境的方式。
  - 如果没有设置并行度， 会以 flink-conf.yaml 中的配置为准， 默认是 1。

  ```scala
  val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
  ```

- createLocalEnvironment 返回本地执行环境， 需要在调用时指定默认的并行度。

  ```scala
  val env = StreamExecutionEnvironment.createLocalEnvironment(1)
  ```

- createRemoteEnvironment  返回集群执行环境， 将 Jar 提交到远程服务器。 需要在调用时指定 JobManager 的 IP 和端口号， 并指定要在集群中运行的 Jar 包。

  ```scala
  val env = ExecutionEnvironment.createRemoteEnvironment("jobmanage-hostname", 6123,"YOURPATH//wordcount.jar")
  ```

  

## 2. source

### 2.1 读取集合中数据

```scala
package com.atguigu.apitest

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._

/**
 * 创建样例类, 温度传感器
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)


object SourceTest {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val dataList = List(
      SensorReading("sensor_1", 1547718199, 35.8),
      SensorReading("sensor_6", 1547718201, 15.4),
      SensorReading("sensor_7", 1547718202, 6.7),
      SensorReading("sensor_10", 1547718205, 38.1)
    )
    val sourceStream = env.fromCollection(dataList)
    sourceStream.print("sourceStream:").setParallelism(1)

    env.execute()

  }
}
```



### 2.2 读取文件数据

```scala
val stream2 = env.readTextFile("YOUR_FILE_PATH")
// 读取socket
val socketStream = env.socketTextStream("127.0.0.1",7777)
```



### 2.3 读取kafaka数据

- 环境配置

  ```xml
          <!--https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 -->
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-kafka-0.11_2.12</artifactId>
              <version>1.10.1</version>
          </dependency>
  ```


- 设置

  ```scala
  setStartFromEarliest():从消息最开始消费；
  setStartFromLatest():从消息的最后开始消费；
  setStartFromTimestamp(long startupOffsetsTimestamp):从设定的时间戳消费；
  setStartFromGroupOffsets():从kafka保存的消费者组的offsets消费，如果没有，会根据auto.offset.reset设定进行消费（auto.offset.reset默认为latest），也就是说我第一次启动程序的时候从哪里消费也需要考虑；
  setStartFromSpecificOffsets(Map<KafkaTopicPartition, Long> specificStartupOffsets):从特定offset消费；
  ```

  

- demo

  ```scala
  package com.atguigu.apitest
  
  import org.apache.flink.api.common.serialization.SimpleStringSchema
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011
  
  import java.util.Properties
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  
  object SourceTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      // 定义kafka 属性
      val properties = new Properties()
      properties.setProperty("bootstrap.servers", "hadoop01:9092")
  
      properties.setProperty("group.id", "consumer-group")
      // properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
      // properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
      properties.setProperty("auto.offset.reset", "latest")
  
      // 读取kafka数据
      val kafkaStream: DataStream[String] = env.addSource(new FlinkKafkaConsumer011[String](
        "sensor", // topic
        new SimpleStringSchema(),
        properties
      ))
      kafkaStream.print()
      env.execute()
  
    }
  }
  
  
  
  ```

### 2.4 自定义数据源

> - 可以自定义连接mysql postgres或者hive等数据源, 这些数据源都是有界数据, 其实不适合作为kafkaStream数据源
> - 自定义数据源就是自定义一个SourceFunction
> - 如果需要连接数据库读取数据库数据的话需要重写下面demo 中run方法, 重建run方法逻辑

```scala
package com.atguigu.apitest

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.functions.source.SourceFunction
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011

import java.util.Properties
import scala.util.Random

/**
 * 创建样例类, 温度传感器 sensor
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

class MySensorSource extends SourceFunction[SensorReading] {

  // flag: 表示数据源是否还在正常运行
  var running: Boolean = true
  override def cancel(): Unit = {
    running = false
  }
  override def run(ctx: SourceFunction.SourceContext[SensorReading]): Unit = { // 初始化一个随机数发生器
    val rand = new Random()
    var curTemp = 1.to(10).map(i => ("sensor_" + i, 65 + rand.nextGaussian() * 20))
    while (running) {
      // 更新温度值
      curTemp = curTemp.map(t => (t._1, t._2 + rand.nextGaussian())) // 获取当前时间戳 val curTime = System.currentTimeMillis()
      var curTime = System.currentTimeMillis()
      curTemp.foreach(t => ctx.collect(SensorReading(t._1, curTime, t._2)))
      Thread.sleep(100)
    }

  }

}

object SourceTest {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 自定义数据源
    val stream4 = env.addSource(new MySensorSource())
    stream4.print()
    env.execute()

  }
}


```



### 2.5 问题:

1. flink读取kafka如何设置偏移量?
2. flink 读取kafka数据如何入库?
   - 使用sink

 

## 3. transform 转换算子

### 3.1 map

- 对每一个元素进行操作

  ```scala
  val streamMap = stream.map { x => x * 2 }
  ```

### 3.2 flatMap

- 与 map 相似, 但是每个输入项可用被映射为 **0 个或者多个**输出项

  ```scala
  val streamFlatMap = stream.flatMap{ x => x.split(" ") }
  ```

- demo

  ```scala
  //例如: 
  flatMap(List(1,2,3))(i ⇒ List(i,i)) 结果是 List(1,1,2,2,3,3), 
  //而
  List(" a b" , "c d" ).flatMap(line ⇒ line.split(" ")) 结果是 List(a, b, c, d)。
  ```

  

### 3.3 filter

- 过滤, 输出符合条件的值

  ```scala
  val streamFilter = stream.filter{
  	x => x == 1
  }
  ```

  

### 3.4 KeyBy

> - 分组
> - DataStream → KeyedStream：逻辑地将一个流拆分成不相交的分区， 每个分区包含具有相同 key 的元素， 在内部以 hash 的形式实现的。

<img src="/Users/liusaisai/Library/Application Support/typora-user-images/image-20220104161151221.png" alt="image-20220104161151221" style="zoom:50%;" />

### 3.5 Rolling Aggregation

> 滚动聚合算子: 比如min(), 第一条数据是1,最小数据就是1, 第二条数据是2, 那么最小数据也是1, 第三条0.5 最小数据是0.5

- 这些算子可以针对 KeyedStream 的每一个支流做聚合。 
  - **sum()** 
  - **min()**  // 根据最小字段得到一条不完整数据, 只会关心当前比较字段, 其他字段会错乱,max一样
  - **max()**
  - **minBy()**   // 根据最小字段得到一条完整数据,maxBy()一样
  - **maxBy()**  

### 3.6 Reduce

> - KeyedStream → DataStream  : 一个分组数据流的聚合操作， 合并当前的元素 和上次聚合的结果， 产生一个新的值， 返回的流中包含每一次聚合的结果， 而不是只返回最后一次聚合的最终结果。
>
> - 举例, 获取最小值会输出最小值的数据,  reduce 可以再输出最小值的时候更改该条数据的其他值
>
>   ```scala
>   .keyBy("id")  
>   .reduce((oldData,newData)=>{  //  oldData 上一次的数据状态, newData新数据
>       // 获取最小的温度, 但是输出当前的时间戳
>       SensorReading(newData.id,newData.timestamp,oldData.temperature.min(newData.temperature))
>     })
>   ```

- demo

  - 数据

    ```
    sensor_1, 1547718199, 35.8
    sensor_1, 1547718192, 1000.9
    sensor_1, 1547718195, 1002
    sensor_2, 1547718201, 15.4
    sensor_2, 1547718203, 15.0
    sensor_3, 1547718202, 6.7
    sensor_3, 1547718205, 38.1
    ```

  ```scala
  package com.atguigu.apitest
  
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.streaming.api.scala._
  
  
  //case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  object TransForm {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream :DataStream[SensorReading] = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
      // 2. 分组
      val reduceStream = dataStream
        .keyBy("id")
        .reduce((oldData,newData)=>{  //  oldData 上一次的数据状态, newData新数据
          // 获取最小的温度, 但是输出当前的时间戳
          SensorReading(newData.id,newData.timestamp,oldData.temperature.min(newData.temperature))
        })
      reduceStream.print()
      env.execute("transform test")
    }
  }
  
  ```

  - 输出

    ```
    SensorReading(sensor_1,1547718199,35.8)
    SensorReading(sensor_1,1547718192,35.8)
    SensorReading(sensor_1,1547718195,35.8)
    SensorReading(sensor_2,1547718201,15.4)
    SensorReading(sensor_2,1547718203,15.0)
    SensorReading(sensor_3,1547718202,6.7)
    SensorReading(sensor_3,1547718205,6.7)
    ```

    



### 3.7 Split 和 Select

> Split 分流   DataStream → SplitStream
>
> Select 合流  SplitStream → DataStream

- Split

  > DataStream → SplitStream：根据某些特征把一个 DataStream 拆分成两个或者 多个 DataStream。

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104180450699.png" alt="image-20220104180450699" style="zoom:50%;" />

- Select

  > SplitStream→DataStream：从一个 SplitStream 中获取一个或者多个DataStream。

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104180545421.png" alt="image-20220104180545421" style="zoom:50%;" />

```scala
package com.atguigu.apitest

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._


//case class SensorReading(id: String, timestamp: Long, temperature: Double)

object TransForm {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream: DataStream[String] = env.readTextFile(filePath)
    // 1. 转换为样例类类型
    val dataStream: DataStream[SensorReading] = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
      })

    //    3.分流操作
    val ssStream = dataStream
      .split(data=>{
        if (data.temperature > 35.0 ) Seq("high") else Seq("low")
      })

    val highTempStream = ssStream.select("high")
    val lowTempStream = ssStream.select("low")
    val allTempStream = ssStream.select("high","low")
    highTempStream.print("high")
    lowTempStream.print("low")
    allTempStream.print("all")
    env.execute("transform test")


  }
}
// -------
high> SensorReading(sensor_1,1547718199,35.8)
all> SensorReading(sensor_1,1547718199,35.8)
high> SensorReading(sensor_1,1547718192,1000.9)
all> SensorReading(sensor_1,1547718192,1000.9)
high> SensorReading(sensor_1,1547718195,1002.0)
all> SensorReading(sensor_1,1547718195,1002.0)
low> SensorReading(sensor_2,1547718201,15.4)
all> SensorReading(sensor_2,1547718201,15.4)
low> SensorReading(sensor_2,1547718203,15.0)
all> SensorReading(sensor_2,1547718203,15.0)
low> SensorReading(sensor_3,1547718202,6.7)
all> SensorReading(sensor_3,1547718202,6.7)
high> SensorReading(sensor_3,1547718205,38.1)
all> SensorReading(sensor_3,1547718205,38.1)
```



### 3.8 Connect 和 CoMap

- Connect

  > DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数据流， 两个数据流被 Connect 之后， 只是被放在了一个同一个流中， 内部依然保持 各自的数据和形式不发生任何变化， 两个流相互独立。

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104223804448.png" alt="image-20220104223804448" style="zoom:50%;" />

  

- CoMap

  > CoMap   ,    CoFlatMap
  >
  > ConnectedStreams → DataStream：作用于 ConnectedStreams 上， 功能与 map和 flatMap 一样， 对 ConnectedStreams 中的每一个 Stream 分别进行 map 和 flatMap 处理。

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104223824225.png" alt="image-20220104223824225" style="zoom:50%;" />

```scala
package com.atguigu.apitest

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._


//case class SensorReading(id: String, timestamp: Long, temperature: Double)

object TransForm {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream: DataStream[String] = env.readTextFile(filePath)
    // 1. 转换为样例类类型
    val dataStream: DataStream[SensorReading] = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
      })

    //    3.分流操作
    val ssStream = dataStream
      .split(data => {
        if (data.temperature > 35.0) Seq("high") else Seq("low")
      })

    val highTempStream = ssStream.select("high")
    val lowTempStream = ssStream.select("low")
    val allTempStream = ssStream.select("high", "low")
    // 4. 合流操作 connect
    val warningStream = highTempStream.map(data => (data.id, data.temperature))
    val connectedStreams = warningStream.connect(lowTempStream)
    // 使用CoMap 对 connectedStream 分别处理
    val coMapStream = connectedStreams
      .map(
        warningData => (warningData._1, warningData._2, "warning"),
        lowData => (lowData.id, "healthy")
      )
    coMapStream.print("coMap")

    env.execute("transform test")

  }
}
// -----输出
coMap> (sensor_1,35.8,warning)
coMap> (sensor_2,healthy)
coMap> (sensor_1,1000.9,warning)
coMap> (sensor_2,healthy)
coMap> (sensor_1,1002.0,warning)
coMap> (sensor_3,healthy)
coMap> (sensor_3,38.1,warning)

```

### 3.9 Union

> - DataStream → DataStream：对两个或者两个以上的 DataStream 进行 union 操 作， 产生一个包含所有 DataStream 元素的新 DataStream。
>
> - Union 和 Connect 区别
>
>   1. Union 之前两个流的类型必须是一样，Connect 可以不一样，在之后的 coMap中再去调整成为一样的。
>
>   2. Connect 只能操作两个流， Union 可以操作多个类型相同的流。

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104223944348.png" alt="image-20220104223944348" style="zoom:50%;" />

```scala
package com.atguigu.apitest

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._


//case class SensorReading(id: String, timestamp: Long, temperature: Double)

object TransForm {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
    val inputStream: DataStream[String] = env.readTextFile(filePath)
    // 1. 转换为样例类类型
    val dataStream: DataStream[SensorReading] = inputStream
      .map(data => {
        val arr = data.split(",")
        SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
      })

    //    3.分流操作
    val ssStream = dataStream
      .split(data => {
        if (data.temperature > 35.0) Seq("high") else Seq("low")
      })

    val highTempStream = ssStream.select("high")
    val lowTempStream: DataStream[SensorReading] = ssStream.select("low")
    val allTempStream = ssStream.select("high", "low")
    // 4. 合流操作 connect
    val warningStream: DataStream[(String,Double)] = highTempStream.map(data => (data.id, data.temperature))
    val connectedStreams = warningStream.connect(lowTempStream)
    // 使用CoMap 对 connectedStream 分别处理
    val coMapStream = connectedStreams
      .map(
        warningData => (warningData._1, warningData._2, "warning"),
        lowData => (lowData.id, "healthy")
      )
    // 5. 合流union操作
    val unionStream = highTempStream.union(lowTempStream,allTempStream)
    unionStream.print("unionStream")

    env.execute("transform test")


  }
}

```



## 4. sink

> - Flink 没有类似于 spark 中 foreach 方法， 让用户进行迭代的操作。 所有对外的 输出操作都要利用 Sink 完成。 最后通过类似如下方式完成整个任务最终输出操作。
>
>   `stream.addSink(new MySink(xxxx))`
>
> - 对外输出使用 sink 方法, 比如写入数据库, 相比直接穿件数据库连接, sink 可以进行状态管理

- 官方提供了一部分的框架的 sink。 除此以外， 需要用户自定义实现 sink。

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104232829828.png" alt="image-20220104232829828" style="zoom:50%;" />

- sink 写入文件

  ```scala
  package com.atguigu.sinktest
  
  import org.apache.flink.api.common.serialization.SimpleStringEncoder
  import org.apache.flink.core.fs.Path
  import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.streaming.api.scala._
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  object FileTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream: DataStream[SensorReading] = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
      // sink 写入文件
      dataStream.print() // 本身也是一个sink
      //    dataStream.writeAsCsv("/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/outSink.csv")
      //    dataStream.writeAsText("/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/outSink.txt")
      dataStream.addSink(
        StreamingFileSink.forRowFormat(
          new Path("/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/outSink2.csv"),
          new SimpleStringEncoder[SensorReading]()
        ).build()
      )
      env.execute()
    }
  }
  
  
  ```

  

### 4.1 Kafka (source/sink)

>  Kafka (source/sink) 既可以作为数据源也可以作为输出端

- 加入依赖项

  ```xml
  <!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 -->
  
  <dependency>
  
  <groupId>org.apache.flink</groupId>
  
  <artifactId>flink-connector-kafka-0.11_2.12</artifactId>
    <version>1.10.1</version>
  
  </dependency>
  ```

- 创建sink 写入kafka

  ```scala
  package com.atguigu.sinktest
  
  import org.apache.flink.api.common.serialization.{SimpleStringEncoder, SimpleStringSchema}
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, _}
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011
  
  object KafkaSinkTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble).toString
        })
      // todo 从kafka 读取数据
  
      // sink 写入Kafka
      dataStream.addSink(
        new FlinkKafkaProducer011[String](
          "hadoop01:9092", "sensor", new SimpleStringSchema()
        )
      )
  
      env.execute()
    }
  }
  // 监控kafka 消费记录
  ```

  

### 4.2 Redis (sink)

> Redis (sink) 作为输出端

- 加入pom 依赖

  ```xml
  <!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis -->
  
  <dependency>
  
  <groupId>org.apache.bahir</groupId>
  
  <artifactId>flink-connector-redis_2.11</artifactId>
  
  <version>1.0</version>
  
  </dependency>
  ```

- demo

  ```scala
  package com.atguigu.sinktest
  
  import org.apache.flink.api.common.serialization.{SimpleStringEncoder, SimpleStringSchema}
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, _}
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011
  import org.apache.flink.streaming.connectors.redis.RedisSink
  import org.apache.flink.streaming.connectors.redis.common.config.FlinkJedisPoolConfig
  import org.apache.flink.streaming.connectors.redis.common.mapper.{RedisCommand, RedisCommandDescription, RedisMapper}
  
  class MyRedisMapper extends RedisMapper[SensorReading] {
    // 定会写入redis的命令,HSET :  表名 + k:v
    override def getCommandDescription: RedisCommandDescription = {
      new RedisCommandDescription(RedisCommand.HSET, "sensor_temperature")
    }
  
    override def getValueFromData(t: SensorReading): String = t.temperature.toString  // value
  
    override def getKeyFromData(t: SensorReading): String = t.id  // key
  
  }
  
  object RedisSinkTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
      // redis sink
      val conf = new FlinkJedisPoolConfig.Builder()
        .setHost("localhost")
        .setPort(6379)
        .build()
      dataStream.addSink(
        new RedisSink[SensorReading](conf, new MyRedisMapper)
      )
  
      env.execute()
    }
  }
  
  ```

  

### 4.3 ElasticSearch (sink)

- 添加依赖

  ```xml
  <dependency>
  
  <groupId>org.apache.flink</groupId>
  
  <artifactId>flink-connector-elasticsearch6_2.12</artifactId>
  
  <version>1.10.1</version>
  
  </dependency>
  ```

- demo

  ```scala
  package com.atguigu.sinktest
  
  import org.apache.flink.api.common.functions.RuntimeContext
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, _}
  import org.apache.flink.streaming.connectors.elasticsearch.{ElasticsearchSinkFunction, RequestIndexer}
  import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink
  import org.apache.http.HttpHost
  import org.elasticsearch.client.Requests
  
  import java.util
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  object EsSinkTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
  
      // es sink
  
      val httpHosts = new util.ArrayList[HttpHost]()
      httpHosts.add(new HttpHost("localhost", 9200))
  
      val esSinkBuilder = new ElasticsearchSink.Builder[SensorReading](
        httpHosts,
        new ElasticsearchSinkFunction[SensorReading] {
          override def process(t: SensorReading, runtimeContext: RuntimeContext, requestIndexer: RequestIndexer): Unit = {
            println("saving data: " + t)
            // 包装一个map作为datasource
            val datasource = new util.HashMap[String, String]()
            datasource.put("id", t.id)
            datasource.put("temperature", t.temperature.toString)
            datasource.put("timestamp", t.timestamp.toString)
            // 创建一个request 发送http请求
            val indexRequest = Requests.indexRequest()
              .index("sensor")
              .`type`("readingData")
              .source(datasource)
            requestIndexer.add(indexRequest)
            println("saved successfully")
          }
        }
      )
      dataStream.addSink(esSinkBuilder.build())
  
      env.execute()
  
    }
  }
  
  ```

  

### 4.4 JDBC 自定义sink

> 以mysql 作为示例

- 添加pom 依赖

  ```xml
          <!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->
  
          <dependency>
              <groupId>mysql</groupId>
              <artifactId>mysql-connector-java</artifactId>
              <version>8.0.25</version>
          </dependency>
  ```

- 完整示例

  ```scala
  package com.atguigu.sinktest
  
  import org.apache.flink.configuration.Configuration
  import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, _}
  
  import java.sql.{Connection, DriverManager, PreparedStatement}
  
  /**
   * 创建样例类, 温度传感器 sensor
   */
  //case class SensorReading(id: String, timestamp: Long, temperature: Double)
  
  object MysqlJDBCSinkTest {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      env.setParallelism(1)
      val filePath = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/sensor.txt"
      val inputStream: DataStream[String] = env.readTextFile(filePath)
      // 1. 转换为样例类类型
      val dataStream = inputStream
        .map(data => {
          val arr = data.split(",")
          SensorReading(arr(0).trim, arr(1).trim.toLong, arr(2).trim.toDouble)
        })
      // 自定义mysql jdbc sink
      dataStream.addSink(new MyJdbcSink())
      env.execute()
    }
  }
  
  
  class MyJdbcSink() extends RichSinkFunction[SensorReading] {
    // 定义连接, 预编译语句
    var conn: Connection = _
    var insertStmt: PreparedStatement = _
    var updateStmt: PreparedStatement = _
  
    // open 主要是创建连接 上下文管理
    override def open(parameters: Configuration): Unit = {
      super.open(parameters)
      conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/sensor_db", "root", "")
      insertStmt = conn.prepareStatement("INSERT INTO temperatures (sensor, temp) VALUES (?, ?)")
      updateStmt = conn.prepareStatement("UPDATE temperatures SET temp = ? WHERE sensor = ?")
    }
  
    // 调用连接，执行 sql
    override def invoke(value: SensorReading, context: SinkFunction.Context[_]): Unit = {
      updateStmt.setDouble(1, value.temperature)
      updateStmt.setString(2, value.id)
      updateStmt.execute()
  
      if (updateStmt.getUpdateCount == 0) {
        insertStmt.setString(1, value.id)
        insertStmt.setDouble(2, value.temperature)
        insertStmt.execute()
      }
    }
  
    override def close(): Unit = {
      insertStmt.close()
      updateStmt.close()
      conn.close()
    }
  }
  ```

  

## 5. Flink 支持的数据类型

> - Flink 流应用程序处理的是以数据对象表示的事件流。 所以在 Flink 内部， 我们 需要能够处理这些对象。 它们需要被序列化和反序列化， 以便通过网络传送它们； 或者从状态后端、检查点和保存点读取它们。为了有效地做到这一点，Flink 需要明 确知道应用程序所处理的数据类型。Flink 使用类型信息的概念来表示数据类型，并 为每个数据类型生成特定的序列化器、反序列化器和比较器。
>
> - Flink 还具有一个类型提取系统，该系统分析函数的输入和返回类型，以自动获 取类型信息， 从而获得序列化器和反序列化器。 但是， 在某些情况下， 例如 lambda 函数或泛型类型， 需要显式地提供类型信息， 才能使应用程序正常工作或提高其性 能。
>
> - Flink 支持 Java 和 Scala 中**所有常见数据类型**。 使用最广泛的类型有以下几种。
>   1. 基础数据类型
>   2. Java 和 Scala 元组（Tuples）
>   3. Scala 样例类（case classes）
>   4. Java 简单对象（POJOs）
>   5. 其它（Arrays, Lists, Maps, Enums, 等等）

1. 基础数据类型

   ```scala
   val numbers: DataStream[Long] = env.fromElements(1L, 2L, 3L, 4L) numbers.map( n => n + 1 )
   ```

   

2. Java 和 Scala 元组（Tuples）

   ```scala
   val persons: DataStream[(String, Integer)] = env.fromElements( ("Adam" , 17), ("Sarah" , 23) ) persons.filter(p => p._2 > 18)
   ```

   

3. Scala 样例类（case classes）

   ```scala
   case class Person(name: String, age: Int)
   val persons: DataStream[Person] = env.fromElements( Person("Adam", 17), Person("Sarah", 23) ) persons.filter(p => p.age > 18)
   ```

   

4. Java 简单对象（POJOs）

   ```scala
   public class Person {
   
   public String name; public int age; public Person() {} public Person(String name, int age) {
   
   this.name = name;
   
   this.age = age; }
   
   }
   
   DataStream<Person> persons = env.fromElements(
   
   new Person("Alex", 42),
   
   new Person("Wendy", 23));
   ```

   

5. 其它（Arrays, Lists, Maps, Enums, 等等）

   - Flink 对 Java 和 Scala 中的一些特殊目的的类型也都是支持的， 比如 Java 的 ArrayList， HashMap， Enum 等等。

   

## 6. 实现 UDF 函数—更细粒度的控制流

### 6.1 函数类（Function Classes）

> Flink 暴露了所有 udf 函数的接口(实现方式为接口或者抽象类)。 例如 MapFunction, FilterFunction, ProcessFunction 等等。

- 自定义fliter

  ```scala
  class FilterFilter extends FilterFunction[String] {
    override def filter(value: String): Boolean = { value.contains("flink") }
  } 
  val flinkTweets = tweets.filter(new FlinkFilter)
  ```

- 还可以将函数实现成匿名类

  ```scala
  val flinkTweets = tweets.filter(
  	new RichFilterFunction[String] { 
      override def filter(value: String): Boolean = { value.contains("flink") }
    }
  )
  ```

### 6.2 匿名函数（Lambda Functions）

```scala
val tweets: DataStream[String] = ...

val flinkTweets = tweets.filter(_.contains("flink"))
```

### 6.3 富函数（Rich Functions）

> “富函数”是 DataStream API 提供的一个函数类的接口， 所有 Flink 函数类都 有其 Rich 版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一 些生命周期方法， 所以可以实现更复杂的功能。

- RichMapFunction 
- RichFlatMapFunction 
- RichFilterFunction 
-  …
- Rich Function 有一个生命周期的概念。 典型的生命周期方法有： 
- open()方法是 rich function 的初始化方法， 当一个算子例如 map 或者 filter 被调用之前 open()会被调用。 
- close()方法是生命周期中的最后一个调用的方法， 做一些清理工作。
-  getRuntimeContext()方法提供了函数的 RuntimeContext 的一些信息，例如函 数执行的并行度， 任务的名字， 以及 state 状态

```scala
class MyFlatMap extends RichFlatMapFunction[Int, (Int, Int)] { 
  var subTaskIndex = 0
  override def open(configuration: Configuration): Unit = { 
    subTaskIndex = getRuntimeContext.getIndexOfThisSubtask 
    // 以下可以做一些初始化工作，例如建立一个数据库连接 或者 HDFS 的连接 
  }
  override def flatMap(in: Int, out: Collector[(Int, Int)]): Unit = { 
    if (in % 2 == subTaskIndex) { 
      out.collect((subTaskIndex, in)) 
    }
  }

  override def close(): Unit = {
    // 以下做一些清理工作，例如断开 数据库连接 或者 HDFS 的连接。
  }

}
```

