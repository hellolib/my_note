## 1. Standalone 模式

### 1.1 安装

- 解压缩 flink-1.10.1-bin-scala_2.12.tgz， 进入 conf 目录中。

  1. 修改 flink/conf/flink-conf.yaml 文件：

     ![image-20220103150149589](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150149589.png)

  2. 修改 /conf/slaves 文件：

     ![image-20220103150209959](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150209959.png)

  3. 分发给另外两台机子：

     ![image-20220103150231946](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150231946.png)

  4. 启动

     ![image-20220103150256788](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150256788.png)

  5. 访问flink集群控制台 http://localhost:8081 

### 1.2 提交任务

1. 准备数据文件（如果需要）

2. 把含数据文件的文件夹， 分发到 taskmanage 机器中, 如果从文件中读取数据，由于是从本地磁盘读取，实际任务会被分发到 taskmanage 的机器中， 所以要把目标文件分发。

   ![image-20220103150538179](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150538179.png)

3. 执行程序

   ```sh
   ./flink run -c com.atguigu.wc.StreamWordCount –p 2 FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar --host lcoalhost –port 7777
   ```

4. 到目标文件夹中查看计算结果

   - 注 意 ： 如 果 计 算 结 果 输 出 到 文 件 ， 会 保 存 到 taskmanage 的 机 器 下 ， 不 会 在 jobmanage 下。

5. 在 webui 控制台查看计算过程

   ![image-20220103150708821](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103150708821.png)

## 2. Yarn 模式

> 以 Yarn 模式部署 Flink 任务时， 要求 Flink 是有 Hadoop 支持的版本， Hadoop 环境需要保证版本在 2.2 以上， 并且集群中安装有 HDFS 服务。

- 略

## 3. Kubernetes 部署 Flink

> Flink standalone集群

### 3.1  构建 flink 镜像

1. 下载`https://archive.apache.org/dist/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.12.tgz`

2. 解压并修改配置文件

   ```sh
   mv flink-1.10.1-bin-scala_2.12.tgz /home/docker
   tar xzvf flink-1.10.1-bin-scala_2.12.tgz -C ./
   
   vim flink-1.10.1/conf/flink-conf.yaml
   # --
   jobmanager.rpc.address: flink-jm
   taskmanager.numberOfTaskSlots: 10
   # --
   ```

3. 配置dockerfile

   ```dockerfile
   FROM centos:centos7
   RUN yum install java-1.8.0-openjdk -y
   #ADD jre-8u231-linux-x64.tar.gz /usr/java/jdk
   #ENV JAVA_HOME /usr/java/jdk/jre1.8.0_231
   #ENV PATH ${PATH}:${JAVA_HOME}/bin
   ARG FLINK_VERSION=1.10.1
   ARG SCALA_VERSION=2.12
   ARG FLINK_TAR_NAME=flink-${FLINK_VERSION}-bin-scala_${SCALA_VERSION}.tgz
   ENV FLINK_HOME=/flink-${FLINK_VERSION}
   ADD ${FLINK_TAR_NAME} /
   #RUN mkdir /home
   COPY flink_run.sh /home
   EXPOSE 8088
   VOLUME $FLINK_HOME/conf
   RUN chmod u+x /home/flink_run.sh
   #ENTRYPOINT cd /home && sh /home/flink_run.sh
   
   ENTRYPOINT ["/home/flink_run.sh"]
   ```

4. 配置启动tm,jm `flink_run.sh`

   ```sh
   #!/bin/bash
   FLINK_HOME=/flink-1.10.1
   FLINK_JOB_MANAGER_SH=$FLINK_HOME/bin/jobmanager.sh
   FLINK_TASK_MANAGER_SH=$FLINK_HOME/bin/taskmanager.sh
   
   case "$1" in
   "jobmanager")
   $FLINK_JOB_MANAGER_SH start-foreground
   ;;
   
   "taskmanager")
   $FLINK_TASK_MANAGER_SH start-foreground
   ;;
   
   *)
   echo "COMMAND ERROR"
   ;;
   esac
   ```

5. 构建镜像 `docker build -t bigox/flink:1.10.1 ./`

6. 查看镜像`docker images |grep flink`,  

> 以下为可选步骤
>
> 7. 创建Flink的network `docker network create flink`
>
> 8. 启动1个job manager和2个task manager
>
>    ```sh
>    docker run -itd  --name flink-jm  -p 8081:8081 -v /home/docker/flink-1.10.1/conf:/flink-1.10.1/conf --network flink bigox/flink:1.10.1 jobmanager
>    docker run -itd  --name flink-tm  -v /home/docker/flink-1.10.1/conf:/flink-1.10.1/conf --network flink bigox/flink:1.10.1 taskmanager
>    docker run -itd  --name flink-tm2  -v /home/docker/flink-1.10.1/conf:/flink-1.10.1/conf --network flink bigox/flink:1.10.1 taskmanager
>    ```
>
> 9. 查看是否存在flink-jm和flink-tm容器`docker ps`
>
> 10. 外部访问:
>     http://xxx.xxx.xxx.xxx:8081/#/overview

### 3.2 k8s 部署

> 首相将打包镜像分发到每一个k8s 节点
>
> 打包:`docker save bigox/flink:1.10.1  > ./bigox-flink.tar.gz`
>
> 分发:`docker load < bigox-flink.tar.gz`

1. 创建命名空间 `kubectl create namespace bigdata`

2. 创建configmap 

   ```sh
   cd /apps/k8s/flink-file/flink-1.10.1 
   kubectl create configmap flink-config --from-file=./conf -n bigdata
   ```

3. 创建**Job manager**和**Task manager**的deployment

   - Job manager的Deployment如下所示。需要注意的是这里replica设置为1，`flink-conf`的内容作为数据卷，挂载到`/flink-1.10.1/conf`.

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: flink-jobmanager
       labels:
         app: flink
     spec:
       replicas: 1
       selector:
         matchLabels:
           app: flink
           role: jobmanager
       template:
         metadata:
           labels:
             app: flink
             role: jobmanager
         spec:
           containers:
           - name: flink
             image: bigox/flink:1.10.1
             args: ["jobmanager"]
             ports:
             - name: web-port
               containerPort: 8081
             volumeMounts:
             - name: flink-config-volume
               mountPath: /flink-1.10.1/conf
           volumes:
             - name: flink-config-volume
               configMap:
                 name: flink-config
     ```

   - Task manager的Deployment。这里我们设定replica为3，即启动3个task manager。

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: flink-taskmanager
       labels:
         app: flink
         role: taskmanager
     spec:
       replicas: 3
       selector:
         matchLabels:
           app: flink
       template:
         metadata:
           labels:
             app: flink
         spec:
           containers:
           - name: flink
             image: bigox/flink:1.10.1
             args: ["taskmanager"]
             ports:
             - name: web-port
               containerPort: 8081
             volumeMounts:
             - name: flink-config-volume
               mountPath: /flink-1.10.1/conf
           volumes:
             - name: flink-config-volume
               configMap:
                 name: flink-config
     ```

   - 创建

     ```sh
     kubectl apply -f job-manager.yaml -n bigdata
     kubectl apply -f task-manager.yaml -n bigdata
     ```

4. 创建Job Manager RPC service

   > 目前需要解决的是Task manager如何找到job manager。Job manager的pod会被k8s调度到任意的节点，这个结果是不可控的。因此，我们需要创建一个service，指向job manager这个pod。
   >
   > Kubenetes会自动创建一个DNS入口。这样一来，无论job manager调度到哪个节点上，它的IP如何变化，k8s自动帮我们更新，并通过DNS关联起来。我们的程序不用做任何修改。
   >
   > 创建这个service之后，无论这个pod的IP怎么变化，我们可以通过`flink-jm`这个DNS访问到job manager。

   - 创建job-service.yaml ,并执行`kubectl apply -f job-service.yaml -n bigdata`

     ```yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: flink-jm
     spec:
       clusterIP: None
       selector:
         role: jobmanager
       ports:
         - protocol: TCP
           port: 6123
           targetPort: 6123
     ```

5. 暴露Flink web端口到集群外

   > 创建一个service 映射端口

   - 创建 port-service.yaml ,并执行`kubectl apply -f port-service.yaml -n bigdata `

     ```yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: flink-web-service
     spec:
       selector:
         app: flink
       type: NodePort
       ports:
         - protocol: TCP
           port: 8081
           targetPort: 8081
           nodePort: 30123
     ```

6. 验证配置 `kubectl get all`

7. 然后我们在浏览器访问k8s节点的30123端口，然后查看所有的Task manager.

   ![image-20220104100952790](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220104100952790.png)