

## 1. flink 初识

- Apache Flink 是一个框架和分布式处理引擎，用于对无界和有界数据 流进行状态计算。

- 特点:
  - 低延迟 
  - 高吞吐 
  - 结果的准确性和良好的容错性

- 应用场景:

  - 电商和市场营销 

    Ø 数据报表、广告投放、业务流程需要

  - 物联网（IOT） 

    Ø 传感器实时数据采集和显示、实时报警，交通运输业 

  - 电信业 

    Ø 基站流量调配 

  - 银行和金融业 

    Ø 实时结算和通知推送，实时检测异常行为

## 2. 传统数据处理架构

- 事务处理

  ![image-20220525101651331](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220525101651331.png)

- 分析处理 : 将数据从业务数据库复制到数仓，再进行分析和查询

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103133230557.png" alt="image-20220103133230557" style="zoom: 33%;" />

## 3. flink 主要特点

- 事件驱动（Event-driven）

- 基于流的世界观 

  - 在 Flink 的世界观中，一切都是由流组成的，**离线数据是有界的流； 实时数据是一个没有界限的流：这就是所谓的有界流和无界流**

- 分层API 

  - 越顶层越抽象，表达含义越简明，使用越方便 

  - 越底层越具体，表达能力越丰富，使用越灵活

    <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103133512052.png" alt="image-20220103133512052" style="zoom:50%;" />

- 支持事件时间（event-time）和处理时间（processing-time） 

- 精确一次（exactly-once）的状态一致性保证 

- 低延迟，每秒处理数百万个事件，毫秒级延迟 

- 与众多常用存储系统的连接 

- 高可用，动态扩展，实现7*24小时全天候运行

## 4. Flink 和 Spark Streaming

- 流（stream）和微批（micro-batching）

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220103133647037.png" alt="image-20220103133647037" style="zoom:33%;" />

- 数据模型 

  - spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组 组小批数 据 RDD 的集合
  - flink 基本数据模型是数据流，以及事件（Event）序列 

- 运行时架构  
  - spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个 
  - flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理

## 5. 第一个word count程序

- idea创建maven项目

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <project xmlns="http://maven.apache.org/POM/4.0.0"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
      <modelVersion>4.0.0</modelVersion>
  
      <groupId>com.atguigu</groupId>
      <artifactId>firstFlink</artifactId>
      <version>1.0-SNAPSHOT</version>
  
      <properties>
          <maven.compiler.source>8</maven.compiler.source>
          <maven.compiler.target>8</maven.compiler.target>
      </properties>
  
  
      <dependencies>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-scala_2.12</artifactId>
              <version>1.10.1</version>
          </dependency>
          <!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala -->
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-scala_2.12</artifactId>
              <version>1.10.1</version>
          </dependency>
      </dependencies>
  
      <build>
          <plugins> <!-- 该插件用于将 Scala 代码编译成 class 文件 -->
              <plugin>
                  <groupId>net.alchim31.maven</groupId>
                  <artifactId>scala-maven-plugin</artifactId>
                  <version>3.4.6</version>
                  <executions>
                      <execution> <!-- 声明绑定到 maven 的 compile 阶段 -->
                          <goals>
                              <goal>compile</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
              <plugin>
                  <!-- 打包为jar -->
                  <groupId>org.apache.maven.plugins</groupId>
                  <artifactId>maven-assembly-plugin</artifactId>
                  <version>3.0.0</version>
                  <configuration>
                      <!--添加后缀-->
                      <descriptorRefs>
                          <descriptorRef>jar-with-dependencies</descriptorRef>
                      </descriptorRefs>
                  </configuration>
                  <executions>
                      <execution>
                          <id>make-assembly</id>
                          <phase>package</phase>
                          <goals>
                              <goal>single</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
          </plugins>
      </build>
  
  </project>
  ```

### 5.1 批处理 word count

```scala
package com.atguigu.wc

import org.apache.flink.api.scala.ExecutionEnvironment
import org.apache.flink.api.scala._

/**
 * 批处理 word count
 */
object WordCount {
  def main(args: Array[String]): Unit = {
    // 1. 创建批处理执行环境
    val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
    // 2. 读取数据(文件)
    val inputPath: String = "/Users/liusaisai/workspace/scalaProject/firstFlink/src/main/resources/hello.txt"
    val inputDataSet: DataSet[String] = env.readTextFile(inputPath)
    // 3. 对数据进行转换统计, 先分词, 然后安装word分组, 分组统计
    val resultDataSet: DataSet[(String,Int)] = inputDataSet
      .flatMap(_.split(" "))
      .map((_,1))
      .groupBy(0) // 以第一个元素进行分组
      .sum(1) // 以所有数据第二个元素求和
    // 打印输出
    resultDataSet.print()
  }
}

```



### 5.2 流处理 word count

```scala
package com.atguigu.wc

import org.apache.flink.api.java.utils.ParameterTool
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._

/**
 * 流处理 word count
 */
object StreamWordCount {
  def main(args: Array[String]): Unit = {
    //1 创建流处理的执行环境
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(2) // 设置并行度
    // 解析命名行参数
    val paramTool: ParameterTool = ParameterTool.fromArgs(args)
    val host = paramTool.get("host")   // 命令行参数 --host 127.0.0.1
    val port = paramTool.getInt("port")  // 命令行参数 --port 7777

    // 读取数据, 接收一个socket文本流, 也可以使用file
    val inputStream: DataStream[String] = env.socketTextStream(host, port)
    // 进行转换处理统计
    val resultDataStream: DataStream[(String, Int)] = inputStream
      .flatMap(_.split(" "))
      .filter(_.nonEmpty)
      .map((_, 1))
      .keyBy(0)
      .sum(1)
    // resultDataStream.print().setParallelism(1) // 使用一个线程输出(写入文件可以使用)
    resultDataStream.print()
    // 启动任务执行
    env.execute("stream word count")
  }
}


// 在命令行中 启动 nc -lk 7777  并输入内容
// 控制台打印. 数字> 数字是默认的流处理程序编号, 并行线程数, 默认是核心数一致, 编号分配是根据字符串的hash值进行的分配
2> (q,1)
4> (dfa,1)
8> (qwe,1)
2> (q,2)
2> (q,3)
2> (q,4)
1> (weq,1)
```



