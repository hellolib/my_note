# 一. Flink CDC 实战

- 版本关系

| Flink CDC Connector Version | Flink Version |                       说明                       |
| --------------------------- | ------------- | :----------------------------------------------: |
| 1.0.0                       | 1.11.*        |                                                  |
| 1.1.0                       | 1.11.*        |                                                  |
| 1.2.0                       | 1.12.*        |                                                  |
| 1.3.0                       | 1.12.*        |                                                  |
| 1.4.0                       | 1.13.*        |                                                  |
| **2.0.***                   | **1.13.***    | **1. 不支持oracle<br />2. 不需要源端表提供主键** |
| **2.1.***                   | **1.13***     |    **1. 支持oracle<br />2. 需要源表提供主键**    |

## 1. DataStream 实现CDC

> 环境要求: 
>
> - flink 1.12.0 (Flink cluster with version 1.12+ and Java 8+)
> - flink-cdc-2.0.0 (flink-cdc 2.1.1 需要源端表必须有主键)

### 1.1 maven 依赖导入

```xml
    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <flink.version>1.12.0</flink.version>
        <scala.binary.version>2.12</scala.binary.version>
        <kafka.version>2.4.0</kafka.version>
    </properties>
<dependencies>
  
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka_${scala.binary.version}</artifactId>
            <version>${kafka.version}</version>
        </dependency>
        <!-- 通用kafka 连接器 , 不需要指定版本-->
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-statebackend-rocksdb_2.12</artifactId>
            <version>${flink.version}</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.76</version>
        </dependency>

        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.5.2</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.25</version>
        </dependency>

        <dependency>
            <groupId>com.ververica</groupId>
            <artifactId>flink-connector-mysql-cdc</artifactId>
            <version>2.0.0</version>
        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-table-planner-blink_2.12</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-java</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

            <groupId>org.apache.flink</groupId>

            <artifactId>flink-streaming-java_2.12</artifactId>

            <version>1.12.0</version>

        </dependency>
        <dependency>

        <groupId>org.apache.flink</groupId>

        <artifactId>flink-clients_2.12</artifactId>

        <version>1.12.0</version>

    </dependency>
    </dependencies>
```

### 1.2 代码编写

> mysql 查看是否开启binlog :` show VARIABLES like 'log_bin';`
>
> mysql 开启binlog: 
>
> ```sh
> #1og_bin
> 1og-bin = mysql-bin
> binlog-format = ROW 
> server_id =1#配置mysql replication需要定义，不能和canal的slaveId重复
> ```

```scala
package com.dcits.cdc

import com.ververica.cdc.connectors.mysql.MySqlSource
import com.ververica.cdc.connectors.mysql.table.StartupOptions
import com.ververica.cdc.debezium.{DebeziumSourceFunction, StringDebeziumDeserializationSchema}
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._

object WatchMysqlChange {
  def main(args: Array[String]): Unit = {
    // 1. 初始化flink 环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)

    val mysqlSource: DebeziumSourceFunction[String] = MySqlSource
      .builder()
      .hostname("127.0.0.1")
      .port(3306)
      .username("root")
      .password("")
      .databaseList("sensor_db")
      .tableList("sensor_db.city", "sensor_db.goods") //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据，注意：指定的时候需要使用"db.table"的方式
      .startupOptions(StartupOptions.initial()) 
      .deserializer(new StringDebeziumDeserializationSchema()) // JsonDebeziumDeserializationSchema
      .build()
    // 使用 CDC Source 从 MySQL 读取数据
    val mysqlDS = env.addSource(mysqlSource)
    // todo 数据加工, 实时计算
    mysqlDS.print()
    env.execute()
  }
}

```

- 输出打印

  ```
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=1,city_name=北京},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013499,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013504}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=2,city_name=上海},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013512,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013512}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.city', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=3,city_name=河南},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013514,snapshot=true,db=sensor_db,table=city,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013515}, valueSchema=Schema{mysql_binlog_source.sensor_db.city.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=1,goods_name=黄金},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013526,snapshot=true,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013527}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492, snapshot=true}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=2,goods_name=钻石},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013528,snapshot=true,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013528}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={ts_sec=1642680013, file=binlog.000150, pos=492}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{after=Struct{id=3,goods_name=稀有金属2},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680013528,snapshot=last,db=sensor_db,table=goods,server_id=0,file=binlog.000150,pos=492,row=0},op=r,ts_ms=1642680013528}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  
  ```

- 修改一条数据 , 输出打印

  ```
  SourceRecord{sourcePartition={server=mysql_binlog_source}, sourceOffset={transaction_id=null, ts_sec=1642680729, file=binlog.000150, pos=571, row=1, server_id=1, event=2}} ConnectRecord{topic='mysql_binlog_source.sensor_db.goods', kafkaPartition=null, key=null, keySchema=null, value=Struct{before=Struct{id=3,goods_name=稀有金属2},after=Struct{id=3,goods_name=稀有金属},source=Struct{version=1.5.2.Final,connector=mysql,name=mysql_binlog_source,ts_ms=1642680729000,db=sensor_db,table=goods,server_id=1,file=binlog.000150,pos=724,row=0},op=u,ts_ms=1642680729634}, valueSchema=Schema{mysql_binlog_source.sensor_db.goods.Envelope:STRUCT}, timestamp=null, headers=ConnectHeaders(headers=)}
  
  ```

  

### 1.3 startupOptions 参数说明

> mysql 为例

- **initial**: 先查询读取mysql 全量表数据, 然后切换至binlog, 读取最新数据
  - initial (default): Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.
- **earliest**: 从binlog最开始的地方读取, 从表创建开始的全部记录,
  - latest-offset: Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.
- **latest**: 直接读取binlog最新的地方开始读取
- **specificOffset**: 指定offset读取binlog
  - specific-offset: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified offset.
- **timestamp**: 指定时间戳读取binlog
  - /timestamp: Never to perform snapshot on the monitored database tables upon first startup, and directly read binlog from the specified timestamp. The consumer will traverse the binlog from the beginning and ignore change events whose timestamp is smaller than the specified timestamp.

### 1.4 测试

1. 打包上传jar包
2. 开启 MySQL Binlog 并重启 MySQL
3. 启动 Flink 集群`bin/start-cluster.sh`
4. 启动hdfs集群`start-dfs.sh`
5. 启动程序`bin/flink run -c com.dcits.FlinkCDC flink-1.0-SNAPSHOT-jar-with-dependencies.jar`
6. 在 MySQL 的 gmall-flink.z_user_info 表中添加、修改或者删除数据
7. 给当前的 Flink 程序创建 Savepoint`bin/flink savepoint JobId hdfs://hadoop102:8020/flink/save`
8. 关闭程序以后从 Savepoint 重启程序`bin/flink run -c com.dcits.FlinkCDC flink-1.0-SNAPSHOT-jar-with-dependencies.jar`

## 2. FlinkSQL 实现CDC

> 环境依赖
>
> - 源端表需要主键
>
> | Flink CDC Connector Version | Flink Version |
> | --------------------------- | ------------- |
> | 1.0.0                       | 1.11.*        |
> | 1.1.0                       | 1.11.*        |
> | 1.2.0                       | 1.12.*        |
> | 1.3.0                       | 1.12.*        |
> | **1.4.0**                   | **1.13.***    |
> | **2.0.***                   | **1.13.***    |

- 依赖引入

  ```xml
  <dependencies>
          <dependency>
              <groupId>com.ververica</groupId>
              <artifactId>flink-connector-mysql-cdc</artifactId>
              <version>2.0.1</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-table-planner-blink_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-java</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-streaming-java_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-clients_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-jdbc_2.12</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-csv</artifactId>
              <version>${flink.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-json</artifactId>
              <version>${flink.version}</version>
          </dependency>
      </dependencies>
  ```

```scala
package com.dcits.cdc

import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.EnvironmentSettings
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment
import org.apache.flink.types.Row

object FlinkSQLWatchMysqlChange {
  def main(args: Array[String]): Unit = {
    // 1. 初始化flink 环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
    // 创建table环境
    val bsSettings = EnvironmentSettings.newInstance()
      .useBlinkPlanner()
      .inStreamingMode()
      .build()
    val tableEnv = StreamTableEnvironment.create(env,bsSettings)
    // **创建flink表字段大小写和原表保持一致!!!!!!**
    tableEnv.executeSql(
      """
        |CREATE TABLE goods (
        |id INT primary key,
        |goods_name STRING )
        |WITH
        |('connector' = 'mysql-cdc',
        |'scan.startup.mode' = 'initial',
        |'hostname' = '127.0.0.1',
        |'port' = '3306',
        |'username' = 'root',
        |'password' = '',
        |'database-name' = 'sensor_db',
        |'table-name' = 'goods'
        |)""".stripMargin)
    val table = tableEnv.sqlQuery("select * from goods")
    tableEnv.toRetractStream[Row](table).print()
    env.execute()
  }
}
```

- 注意事项:
  - **创建flink表字段大小写和原表保持一致!!!!!!**

# 二. 自定义反序列化器!!!

> - 自定义反序列化器可以 自定义数据源返回数据格式
> - 基于DataStream API 构建自定义反序列化器
> - **flink-cdc 2.1+ 提供json序列化器, 但是flink-cdc2.1+ 需要源端表必须有主键**

## 1. 构建反序列化器

```scala
package com.dcits.cdc

import com.ververica.cdc.connectors.mysql.MySqlSource
import com.ververica.cdc.connectors.mysql.table.StartupOptions
import com.ververica.cdc.debezium.{DebeziumDeserializationSchema, DebeziumSourceFunction, StringDebeziumDeserializationSchema}
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector
import org.apache.kafka.connect.source.SourceRecord
import io.debezium.data.Envelope
import com.alibaba.fastjson.{JSON, JSONObject}
import org.apache.flink.api.common.typeinfo.TypeInformation
import org.apache.kafka.connect.data.Struct

import javax.ws.rs.client.Entity.json

/**
 * 自定义反序列化器, 解析源端变动数据
 */
object CustomDeserializationWatchMysql {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = "file:///tmp/flink-cdc-checkpoints"
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
    val mysqlSource: DebeziumSourceFunction[String] = MySqlSource
      .builder()
      .hostname("127.0.0.1")
      .port(3306)
      .username("root")
      .password("")
      .databaseList("sensor_db")
      .tableList("sensor_db.city", "sensor_db.goods") //可选配置项,如果不指定该参数,则会读取上一个配置下的所有表的数据，注意：指定的时候需要使用"db.table"的方式
      .startupOptions(StartupOptions.initial())
      .deserializer(new MyDebeziumDeserialization())
      .build()

    env.addSource(mysqlSource).print()
    env.execute("custom deserialization")
  }

}

/**
 * 自定义反序列化解析器
 */

class MyDebeziumDeserialization extends DebeziumDeserializationSchema[String] {
  override def deserialize(sourceRecord: SourceRecord, collector: Collector[String]): Unit = {
    // 获取主题信息,包含着数据库和表名 mysql_binlog_source.sensor_db.goods
    val topic: String = sourceRecord.topic
    val arr: Array[String] = topic.split("\\.")
    val db: String = arr(1)
    val tableName: String = arr(2)
    //获取操作类型 READ DELETE UPDATE CREATE
    val operation = Envelope.operationFor(sourceRecord)
    //获取值信息并转换为 Struct 类型
    val value = sourceRecord.value().asInstanceOf[Struct]
    //获取变化后的数据
    val before = value.getStruct("before")
    //创建 JSON 对象用于存储数据信息
    val beforeData = new JSONObject()
    if (before != null){
      before.schema.fields.forEach(
        field =>{
          beforeData.put(field.name(),before.get(field))
        }
      )
    }
    //获取变化后的数据
    val after = value.getStruct("after")
    //创建 JSON 对象用于存储数据信息
    val afterData = new JSONObject()
    if (afterData != null){
      after.schema.fields.forEach(
        field =>{
          afterData.put(field.name(),after.get(field))
        }
      )
    }
    //发送数据至下游
    val result = new JSONObject()
    result.put("operation", operation.toString.toLowerCase)
    result.put("before_data", beforeData)
    result.put("after_data", afterData)
    result.put("database", db)
    result.put("table", tableName)
    collector.collect(result.toJSONString)


  }

  override def getProducedType: TypeInformation[String] = {
    TypeInformation.of(classOf[String])
  }
}
```

## 2. 格式化输出 源端 数据变更记录

- 全表扫描

  ```json
  {"database":"sensor_db","after_data":{"city_name":"北京","id":1},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"city_name":"上海","id":2},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"city_name":"河南","id":3},"before_data":{},"operation":"read","table":"city"}
  {"database":"sensor_db","after_data":{"goods_name":"黄金","id":1},"before_data":{},"operation":"read","table":"goods"}
  {"database":"sensor_db","after_data":{"goods_name":"钻石","id":2},"before_data":{},"operation":"read","table":"goods"}
  {"database":"sensor_db","after_data":{"goods_name":"稀有金属","id":3},"before_data":{},"operation":"read","table":"goods"}
  ```

- 数据变更

  ```json
  {"database":"sensor_db","after_data":{"goods_name":"稀有金属2","id":3},"before_data":{"goods_name":"稀有金属","id":3},"operation":"update","table":"goods"}
  ```

  

## 3. 注意事项

- **注意, 将sourceRecord 转为Struct 后必须使用foreach方法获取深层次数据!!!**

  ```scala
      //获取值信息并转换为 Struct 类型
      val value = sourceRecord.value().asInstanceOf[Struct]
      //获取变化后的数据
      val before = value.getStruct("before")
      //创建 JSON 对象用于存储数据信息
      val beforeData = new JSONObject()
      if (before != null){
        before.schema.fields.forEach(
          field =>{
            beforeData.put(field.name(),before.get(field))
          }
        )
      }
  ```

  