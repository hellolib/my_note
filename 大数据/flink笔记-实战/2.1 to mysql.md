## 1. 依赖引入

```xml
    <dependencies>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.25</version>
        </dependency>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-statebackend-rocksdb_${scala.binary.version}</artifactId>
            <version>${flink.version}</version>
        </dependency>
    </dependencies>
```

## 2. 简单插入

```scala
package com.dcits.simple

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}

import java.sql.{Connection, DriverManager, PreparedStatement}
import java.util.Properties


/**
 * 读取kafka数据写入mysql
 */
object SimpleToMysqlTest {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    // 定义kafka 属性
    val properties = new Properties()
    properties.put("bootstrap.servers", "hadoop01:9092")
    properties.put("zookeeper.connect", "hadoop01:2181")
    properties.put("group.id", "flink-msyql")
    properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("auto.offset.reset", "latest")
    val flinkKafkaConsumer = new FlinkKafkaConsumer[String](
      "sensor", // topic
      new SimpleStringSchema(),
      properties
    )
    // 读取kafka数据
    val kafkaStream: DataStream[String] = env.addSource(flinkKafkaConsumer)
    val resultValue = kafkaStream
      .map(data => {
        val strArr: Array[String] = data.split(",")
        SensorReading(strArr(0).trim, strArr(1).trim.toLong, strArr(2).trim.toDouble)
      })
    // 写入到 mysql
    resultValue.addSink(new MysqlJDBCSink)
    resultValue.print()
    env.execute("kafka to mysql")
  }
}

/**
 * 创建样例类, 温度传感器 sensor
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

/**
 * 自定义 mysql jdbc sink
 */
class MysqlJDBCSink() extends RichSinkFunction[SensorReading] {
  // 自定义连接, 预编译语句
  var conn: Connection = _
  var insertStmt: PreparedStatement = _
  var updateStmt: PreparedStatement = _

  // open 上下文管理创建数据库连接
  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/sensor_db", "root", "")
    insertStmt = conn.prepareStatement("INSERT INTO temperatures (sensor, temp) VALUES (?, ?)")
    updateStmt = conn.prepareStatement("UPDATE temperatures SET temp = ? WHERE sensor = ?")
  }

  // 调用连接，执行 sql
  override def invoke(value: SensorReading, context: SinkFunction.Context[_]): Unit = {
    updateStmt.setDouble(1, value.temperature)
    updateStmt.setString(2, value.id)
    updateStmt.execute()
    if (updateStmt.getUpdateCount == 0) {
      insertStmt.setString(1, value.id)
      insertStmt.setDouble(2, value.temperature)
      insertStmt.execute()
    }

  }

  override def close(): Unit = {
    insertStmt.close()
    updateStmt.close()
    conn.close()
  }

}
```

## 3. 批量插入

- main

  ```scala
  import java.util.Properties
  
  import function.FileToMysqlWindowFunction
  import org.apache.flink.api.common.serialization.SimpleStringSchema
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import sink.SinkToMysql
  import source.SourceFromFile
  import vo.ClassInfo
  import org.apache.flink.api.scala._
  import org.apache.flink.streaming.api.windowing.time.Time
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
  
  
  object FileToMysql {
  
    def main(args: Array[String]): Unit = {
      val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
  
      var props: Properties = new Properties();
      props.setProperty("bootstrap.servers", "localhost:9092");
      props.setProperty("group.id", "demoGroup");
  
      val kafkaConsumer: FlinkKafkaConsumer[String] = new FlinkKafkaConsumer[String]("demoTopic", new SimpleStringSchema(), props);
      kafkaConsumer.setStartFromLatest();
  
      env
        .addSource(kafkaConsumer)
        .setParallelism(2)
        .map(line => parse(line))
        .timeWindowAll(Time.seconds(60))
        .process(new FileToMysqlWindowFunction)
        .addSink(new SinkToMysql)
      env.execute()
    }
  
  
    private def parse(line: String): ClassInfo = {
      var classInfo: ClassInfo = new ClassInfo()
      val arr = line.split(",")
      classInfo.id = arr(0)
      classInfo.classname = arr(1)
      classInfo.teacherId = arr(2)
      classInfo
    }
  
  }
  ```

- windowFunction

  ```scala
  import org.apache.flink.streaming.api.scala.function.ProcessAllWindowFunction
  import org.apache.flink.streaming.api.windowing.windows.TimeWindow
  import org.apache.flink.util.Collector
  import vo.ClassInfo
  
  import scala.collection.mutable.ListBuffer
  
  class FileToMysqlWindowFunction extends ProcessAllWindowFunction[ClassInfo, List[ClassInfo], TimeWindow] {
  
    override def process(context: Context, elements: Iterable[ClassInfo], out: Collector[List[ClassInfo]]): Unit = {
      var buffer: ListBuffer[ClassInfo] = ListBuffer()
      val iterator = elements.iterator
      while (iterator.hasNext) {
        buffer.append(iterator.next())
      }
  
      out.collect(buffer.toList)
    }
  
  }
  ```

- Sink

  ```scala
  import java.sql.{Connection, DriverManager, PreparedStatement}
  
  import org.apache.flink.configuration.Configuration
  import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
  import vo.ClassInfo
  
  class SinkToMysql extends RichSinkFunction[List[ClassInfo]] {
  
    private val jdbcUrl = "jdbc:mysql://localhost:3306/szw?useSSL=false&serverTimezone=GMT%2B8"
    private val username = "root"
    private val password = ""
    private val driverName = "com.mysql.cj.jdbc.Driver"
    private var connection: Connection = null
    private var ps: PreparedStatement = null
  
    override def open(parameters: Configuration): Unit = {
      super.open(parameters)
  
      Class.forName(driverName)
      connection = DriverManager.getConnection(jdbcUrl, username, password)
  
      val sql: String = "replace into class_info(id, classname, teacher_id) values(?,?,?)"
      ps = connection.prepareStatement(sql)
    }
  
    override def invoke(list: List[ClassInfo], context: SinkFunction.Context[_]): Unit = {
      for (classInfo <- list) {
        ps.setString(1, classInfo.id)
        ps.setString(2, classInfo.classname)
        ps.setString(3, classInfo.teacherId)
        ps.addBatch()
      }
      ps.executeBatch()
    }
  
  }
  ```

  
