## 1. 设置多topic

```scala
    val properties = new Properties()
    properties.put("bootstrap.servers", "hadoop01:9092")
    properties.put("zookeeper.connect", "hadoop01:2181")
    properties.put("group.id", GroupID)
    properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    //    properties.put("auto.offset.reset", "latest")
    val topics = new util.LinkedList[String]
    topics.add("HX_DJ.DZDZ_FPXX_PTFP_FLINK_TEST")
    topics.add("test")
    val flinkKafkaConsumer = new FlinkKafkaConsumer[ConsumerRecord[String, String]](
      topics, // topic
      new SimpleStringSchema(),
      properties
    )
```

## 2. 解析消息元数据信息

```scala
...

class MyKafkaDeserializationSchema extends KafkaDeserializationSchema[ConsumerRecord[String, String]] {
  /*是否流结束，比如读到一个key为end的字符串结束，这里不再判断，直接返回false 不结束*/
  override def isEndOfStream(t: ConsumerRecord[String, String]): Boolean = {
    false
  }
	/*获取msg的key,处理key为null 的情况*/
  private def getMsgKey(record: ConsumerRecord[Array[Byte], Array[Byte]]): String = {
    val msgKey = record.key()
    if (msgKey == null) {
      return ""
    } else {
      return new String(msgKey, "UTF-8")
    }
  }
  // 解析
  override def deserialize(record: ConsumerRecord[Array[Byte], Array[Byte]]): ConsumerRecord[String, String] = {
    val ret = new ConsumerRecord(
      record.topic(),
      record.partition(),
      record.offset(),
      getMsgKey(record),
      new String(record.value(), "UTF-8")
    )
    return ret
  }
  /*用于获取反序列化对象的类型*/
  override def getProducedType: TypeInformation[ConsumerRecord[String, String]] = {
    TypeInformation.of(new TypeHint[ConsumerRecord[String, String]] {})
  }
}
...


val flinkKafkaConsumer = new FlinkKafkaConsumer[ConsumerRecord[String, String]](
      topics, // topic
      //      new SimpleStringSchema(),
      new MyKafkaDeserializationSchema(),
      properties
    )


```

## 3. 指定partition

- 开启多并行, 一个并行自动分配一个partition

- 计算公式

  1. kafkaPartition mod 并行度总数 = 分配到并行度中的partition; partition 个数为 6；并行度为 3

     ![在这里插入图片描述](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/watermark%252Ctype_ZmFuZ3poZW5naGVpdGk%252Cshadow_10%252Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxMzgzNDM1%252Csize_16%252Ccolor_FFFFFF%252Ct_70.png)

     ![image-20220531144326064](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220531144326064.png)

  2. 如上分析，`如果并行度 大于 partition总数，那么多余的并行度分配不到 partition，该并行度也就不会有数据 如下图：3个kafka partition，flink设置4个并行度为例，编号为3的并行度将获取不到数据`

     ![image-20220531144358503](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220531144358503.png)

## 4. windowApi 批量入库

```scala
      .keyBy(_.topic)
      .timeWindow(Time.seconds(5))
//      .allowedLateness(Time.minutes(1))
      .process( new MyProcessWindowFunction )
      .print()

...

class MyProcessWindowFunction extends ProcessWindowFunction[SqlObj,List[SqlObj],String,TimeWindow] {
  override def process(key: String, context: Context, elements: Iterable[SqlObj], out: Collector[List[SqlObj]]): Unit = {
    val buffer: ListBuffer[SqlObj] = ListBuffer()
    val iterator = elements.iterator
    while (iterator.hasNext) {
      buffer.append(iterator.next())
    }
    if (buffer.isEmpty) {
      return
    }
    out.collect(buffer.toList)
  }
}
```

