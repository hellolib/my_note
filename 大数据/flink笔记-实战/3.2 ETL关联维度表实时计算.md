> 目标: 
>
> 1. 订单关联维度表, 生成订单详细信息
> 2. 宽表写入kafka
> 3. 入库vertica 持久化存储

## 1. 样例类定义

```scala
package com.dcits.realtime.etl.bean

import com.alibaba.fastjson.JSON
import scala.beans.BeanProperty

/**
 * 订单样例类
 */
case class OrderEntity(
                        @BeanProperty orderId: Long = 0,
                        @BeanProperty goodsId: Long = 0,
                        @BeanProperty goodsName: String = "", // 扩宽字段, goods 表
                        @BeanProperty cityId: Long = 0,
                        @BeanProperty cityName: String = "", // 扩宽字段, city 表
                      )
```

## 2. process 代码编写

- 在表扩宽时使用异步IO进行 性能优化

```scala
package com.dcits.realtime.etl.process

import com.alibaba.fastjson.JSON
import com.alibaba.fastjson.serializer.SerializerFeature
import com.dcits.realtime.etl.`trait`.MysqlBaseETL
import com.dcits.realtime.etl.async.AsyncOrderDetailRedisRequest
import com.dcits.realtime.etl.bean.OrderEntity
import com.dcits.realtime.etl.utils.{GlobalConfigUtil, VerticaPoolUtil}
import org.apache.flink.api.common.functions.RichFunction
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
import org.apache.flink.streaming.api.scala.{AsyncDataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaProducer, FlinkKafkaProducer011}
import org.apache.kafka.common.serialization.Serializer

import java.sql.{Connection, PreparedStatement}
import java.util.concurrent.TimeUnit

/**
 * 订单数据的ETL
 * 1. 将订单表关联dim表, 写入vertica ; 明细查询
 * 2. 将订单表关联dim表, 写入kafka ;
 *
 * @param env
 */
case class OrderDataETL(env: StreamExecutionEnvironment, topic: String) extends MysqlBaseETL(env) {
  /**
   * etl 数据操作
   */
  override def process(): Unit = {
    /**
     * 实现步骤
     * 1. 过滤订单表数据, 将rowdata 转换为orderEntity 样例类
     * 2. 将订单表关联dim表,实时拉宽 (异步IO)
     * 3. 将拉宽后的数据写入到kafka
     * 4. 将拉宽厚的数据写入到vertica, 持久化存储
     */

    // 1. 过滤订单表数据, 将 rowData 转换为orderEntity 样例类
    val allDataStream = getKafkaDataStream(topic)
    val orderDataStream = allDataStream
      .filter(rowData => {
        val jsonObject = JSON.parseObject(rowData)
        jsonObject.getString("table_name") match {
          case "orders" => true
          case _ => false
        }
      })
    // 2. 将订单表关联dim表,实时拉宽 (异步IO)
    // AsyncDataStream.unorderedWait 无序  AsyncDataStream.orderedWait() 有序
    val orderDetailWideEntityDataStream = AsyncDataStream.unorderedWait(
      orderDataStream, // 数据流
      new AsyncOrderDetailRedisRequest(), // 异步请求对象
      1, // 一分钟超时
      TimeUnit.SECONDS,
      100 // 100 个并发请求
    )
    // 3. 将拉宽后的数据写入到kafka
    var orderJsonDataStream = orderDetailWideEntityDataStream
      .map(dataRow => {
        JSON.toJSONString(dataRow, SerializerFeature.DisableCircularReferenceDetect)
      })
    orderJsonDataStream.addSink(new FlinkKafkaProducer[String](
      GlobalConfigUtil.`bootstrap.servers`,
      "orders_res",
      new SimpleStringSchema()
    ))

    // 4. 将拉宽后的数据写入到vertica, 持久化存储
    orderDetailWideEntityDataStream.addSink(new VerticaRichSink())
    orderJsonDataStream.print("to kafka")
    orderDetailWideEntityDataStream.print("to vertica")
  }
}

```



## 3. 异步请求对象定义

```scala
package com.dcits.realtime.etl.async

import com.alibaba.fastjson.{JSON, JSONObject}
import com.dcits.realtime.etl.bean.OrderEntity
import com.dcits.realtime.etl.utils.RedisUtil
import org.apache.flink.configuration.Configuration
import org.apache.flink.runtime.concurrent.Executors
import org.apache.flink.runtime.executiongraph.Execution
import org.apache.flink.streaming.api.scala.async.{ResultFuture, RichAsyncFunction}
import redis.clients.jedis.Jedis

import scala.concurrent.{ExecutionContext, Future}

/**
 * 异步查询订单明细数据与维度表数据进行关联
 * dim 数据在redis中, 使用richAsyncFunction
 */
class AsyncOrderDetailRedisRequest extends RichAsyncFunction[String, OrderEntity] {
  var jedis: Jedis = _

  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    // 获取redis 连接
    jedis = RedisUtil.getJedis
    // 指定redis 数据库索引
    jedis.select(1)
  }

  override def close(): Unit = {
    if (jedis.isConnected) {
      jedis.close()
    }
  }

  // redis 超时, 重写后执行该方法里的逻辑
  override def timeout(input: String, resultFuture: ResultFuture[OrderEntity]): Unit = {
    println("redis连接超时: 订单操作, redis连接超时")
  }

  // 定义异步回调上下文: implicit 隐式转换
  implicit lazy val executor = ExecutionContext.fromExecutor(Executors.directExecutor())

  // 异步操作, 对数据流中每一条数据操作, 异步处理
  override def asyncInvoke(input: String, resultFuture: ResultFuture[OrderEntity]): Unit = {
    // 发起异步请求, 获取要结束的future
    Future {
      val inputJsonObj = JSON.parseObject(input)
      // 1. 根据商品id获取商品的详细信息
      val goodsJson = jedis.hget("dim_goods_info", inputJsonObj.getLong("goodsId").toString)
      val dimGoods = JSON.parseObject(goodsJson)
      // 2. 根据city id获取城市详细信息
      val cityJson = jedis.hget("dim_city_info", inputJsonObj.getLong("cityId").toString)
      val dimCity = JSON.parseObject(cityJson)
      // 3. 拉宽数据
      val orderEntity = OrderEntity(
        inputJsonObj.getLong("id"),
        inputJsonObj.getLong("goodsId"),
        dimGoods.getString("goodsName"),
        inputJsonObj.getLong("cityId"),
        dimCity.getString("cityName")
      )
      // 异步请求回调
      resultFuture.complete(Array(orderEntity))
    }
  }
}

```

## 4. 自定义verticaSink

```scala

class VerticaRichSink extends RichSinkFunction[OrderEntity] {
  var verticaConn: Connection = _
  var insertStmt: PreparedStatement = _

  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    verticaConn = VerticaPoolUtil.getConn
    insertStmt = verticaConn.prepareStatement("INSERT INTO  HX_SB.orders_res VALUES (?,?,?)")
  }

  override def close(): Unit = {
    verticaConn.close()
    insertStmt.close()
  }

  override def invoke(value: OrderEntity, context: SinkFunction.Context[_]): Unit = {
    insertStmt.setLong(1, value.orderId)
    insertStmt.setString(2, value.goodsName)
    insertStmt.setString(3, value.cityName)
    insertStmt.execute()
  }
}
```

