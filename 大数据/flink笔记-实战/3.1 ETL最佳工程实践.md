# 一.  创建父项目

- 公共依赖引入

# 二.  创建子项目

- 子项目依赖引入

## 1. 项目结构

![image-20220118155209653](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220118155209653.png)

```sh
.
├── pom.xml
├── src
│   ├── main
│   │   ├── resources
│   │   │   ├── application.conf
│   │   │   ├── log4j.properties
│   │   │   └── testData.json
│   │   └── scala
│   │       └── com
│   │           └── dcits
│   │               └── realtime
│   │                   └── etl
│   │                       ├── app
│   │                       │   └── VerticaApp.scala
│   │                       ├── async
│   │                       │   └── SyncDimData.scala
│   │                       ├── bean
│   │                       │   └── DimEntity.scala
│   │                       ├── dataloader
│   │                       │   └── DimDataLoader.scala
│   │                       ├── process
│   │                       ├── trait
│   │                       │   ├── BaseETL.scala
│   │                       │   ├── MQBaseETL.scala
│   │                       │   └── MysqlBaseETL.scala
│   │                       └── utils
│   │                           ├── GlobalConfigUtil.scala
│   │                           ├── KafkaPropsUtil.scala
│   │                           ├── RedisUtil.scala
│   │                           └── VerticaUtil.scala
└── target
    ├── classes
    │   ├── application.conf
    │   ├── com
    │   │   └── dcits
    │   │       └── realtime
    │   │           └── etl
    │   │               ├── app
    │   │               │   ├── VerticaApp$.class
    │   │               │   └── VerticaApp.class
    │   │               ├── async
    │   │               │   ├── RedisSink.class
    │   │               │   ├── SyncDimData$.class
    │   │               │   └── SyncDimData.class
    │   │               ├── bean
    │   │               │   ├── DimGoodsEntity$.class
    │   │               │   └── DimGoodsEntity.class
    │   │               ├── dataloader
    │   │               │   ├── DimDataLoader$.class
    │   │               │   └── DimDataLoader.class
    │   │               ├── trait
    │   │               │   ├── BaseETL.class
    │   │               │   ├── MQBaseETL.class
    │   │               │   └── MysqlBaseETL.class
    │   │               └── utils
    │   │                   ├── GlobalConfigUtil$.class
    │   │                   ├── GlobalConfigUtil.class
    │   │                   ├── KafkaPropsUtil$.class
    │   │                   ├── KafkaPropsUtil.class
    │   │                   ├── RedisUtil$.class
    │   │                   ├── RedisUtil.class
    │   │                   ├── VerticaUtil$.class
    │   │                   └── VerticaUtil.class
    │   ├── log4j.properties
    │   └── testData.json
    └── generated-sources
        └── annotations

```



## 2. 配置文件初始化

```scala
package com.dcits.realtime.etl.utils

import com.typesafe.config.ConfigFactory

/**
 * 编写读取配置文件的工具类
 */
object GlobalConfigUtil {
  // 默认加载resource 下的application.conf
  private val config = ConfigFactory.load();

  /**
   * 读取配置项配置信息
   */

  val `bootstrap.servers`: String = config.getString("bootstrap.servers")
  val `kafka.request.timeout`: String = config.getString("kafka.request.timeout")
  val `zookeeper.connect`: String = config.getString("zookeeper.connect")
  val `group.id`: String = config.getString("group.id")
  val `enable.auto.commit`: String = config.getString("enable.auto.commit")
  val `auto.commit.interval.ms`: String = config.getString("auto.commit.interval.ms")
  val `auto.offset.reset`: String = config.getString("auto.offset.reset")
  val `key.serializer`: String = config.getString("key.serializer")
  val `key.deserializer`: String = config.getString("key.deserializer")
  val `vertica.server.ip`: String = config.getString("vertica.server.ip")
  val `vertica.server.port`: String = config.getString("vertica.server.port")
  val `vertica.server.user`: String = config.getString("vertica.server.user")
  val `vertica.server.password`: String = config.getString("vertica.server.password")
  val `vertica.server.dbname`: String = config.getString("vertica.server.dbname")
  val `checkpointPath`: String = config.getString("checkpointPath")
  val `redis.server.ip`: String = config.getString("redis.server.ip")
  val `redis.server.port`: String = config.getString("redis.server.port")

  def main(args: Array[String]): Unit = {
    println(`bootstrap.servers`)
  }
}

```

## 3. 工具类初始化

### 3.1 kafka 初始化

```scala
package com.dcits.realtime.etl.utils

import org.apache.kafka.clients.consumer.ConsumerConfig

import java.util.Properties

object KafkaPropsUtil {
  /**
   * 返回kafka 封装好的配置对象
   */
  def getKafkaProperties: Properties ={
    // kafka 属性配置
    val properties = new Properties()
    properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,GlobalConfigUtil.`bootstrap.servers`)
    properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG,GlobalConfigUtil.`group.id`)
    properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,GlobalConfigUtil.`enable.auto.commit`)
    properties.setProperty(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,GlobalConfigUtil.`auto.commit.interval.ms`)
    properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,GlobalConfigUtil.`auto.offset.reset`)
    properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,GlobalConfigUtil.`key.deserializer`)
    properties.setProperty(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG,GlobalConfigUtil.`kafka.request.timeout`)
    properties
  }
}

```



### 3.2 Redis 初始化

```scala
package com.dcits.realtime.etl.utils

import redis.clients.jedis.{Jedis, JedisPool, JedisPoolConfig}


object RedisUtil {
  private val config = new JedisPoolConfig()
  //是否启用后进先出,默认true
  //  config.setLifo(true)
  //最大空闲连接数,默认8个
  config.setMaxIdle(8)
  //最大连接数,默认8个
  config.setMaxTotal(1000)
  //获取连接时的最大等待毫秒数(如果设置为阻塞时 BlockWhenExhausted),如果超时就抛异常,小于零:阻塞不确定的时间,默认-1
  config.setMaxWaitMillis(-1)
  //出连接的最小空団时间默认180060秒(30分钟)
  config.setMinEvictableIdleTimeMillis(1800000)
  //最小空闲连接数,默认0
  config.setMaxIdle(0)
  //每次逐出检查时逐出的最大数目如果为负数就是:1/abs(n),默认3
  config.setNumTestsPerEvictionRun(3)
  //对象空闲多久后逐出,当空闲时间>该值且空闲连接>最大空闲数时直接逐出,不再根据 MinEvictableIdleTimeMillis判断(默认逐出策略)
  config.setSoftMinEvictableIdleTimeMillis(1800000)
  // 在获取连接的时候检查有效性,默认 false
  config.setTestOnBorrow(false)
  // /在空闲时检查有效性,默认 false
  config.setTestWhileIdle(false)

  // 初始化redis 连接池
  val jedisPool: JedisPool = new JedisPool(
    config,
    GlobalConfigUtil.`redis.server.ip`,
    GlobalConfigUtil.`redis.server.port`.toInt
  )

  /**
   * 获取redis 连接
   */
  def getJedis: Jedis ={
    jedisPool.getResource
  }
}

```



### 3.3 vertica 初始化

```scala
package com.dcits.realtime.etl.utils

import java.sql.{Connection, DriverManager}
import java.util.Properties

object VerticaUtil {
  private val verticaProp = new Properties()
  verticaProp.put("user", GlobalConfigUtil.`vertica.server.user`)
  verticaProp.put("password", GlobalConfigUtil.`vertica.server.password`)

  /**
   * 获取vertica连接
   * todo 数据库连接池 (官方推荐使用开源 c3p0 或 DBCP 库 来实现连接池)
   */
  def getConn: Connection ={
    val conn: Connection = DriverManager.getConnection(
      f"""
         |jdbc:vertica://
         |${GlobalConfigUtil.`vertica.server.ip`}:
         |${GlobalConfigUtil.`vertica.server.port`}/
         |${GlobalConfigUtil.`vertica.server.dbname`}
         |""".stripMargin,
      verticaProp);
    conn
  }
}

```

### 3.3 vertica数据库连接池

```scala
package com.dcits.realtime.etl.utils

import com.mchange.v2.c3p0.ComboPooledDataSource

import java.sql.Connection

object VerticaPoolUtil {
  private val dataSource = new ComboPooledDataSource //创建连接池实例
  dataSource.setDriverClass("com.vertica.jdbc.Driver") //设置驱动
  dataSource.setJdbcUrl(
    f"jdbc:vertica://${GlobalConfigUtil.`vertica.server.ip`}:" +
      f"${GlobalConfigUtil.`vertica.server.port`}/" +
      f"${GlobalConfigUtil.`vertica.server.dbname`}"
  ) //设置url
  dataSource.setUser(GlobalConfigUtil.`vertica.server.user`) //设置用户
  dataSource.setPassword(GlobalConfigUtil.`vertica.server.password`) //设置密码
  dataSource.setMinPoolSize(3) //设置池中的最小数量
  dataSource.setMaxPoolSize(20) //设置池里面能放的最大数量
  dataSource.setAcquireIncrement(5) //当池子里面不够的时候每一次的增量
  dataSource.setInitialPoolSize(10); //设置连接池的初始连接数
  dataSource.setMaxStatements(100); //设置最大的并发数

  def getConn: Connection = {
    val conn: Connection = dataSource.getConnection
    conn
  }


  def main(args: Array[String]): Unit = {
    val conn = getConn
    val stmt = conn.createStatement
    val res = stmt.executeQuery("select * from HX_SB.v2h_check_tab")
    while (res.next()) {
      println(res.getObject(1),res.getObject(2),res.getObject("bz"))
    }
  }
}
```

## 4. ETL特征类(接口)封装

- 基类

  ```scala
  package com.dcits.realtime.etl.`trait`
  
  import org.apache.flink.streaming.api.scala.DataStream
  import org.apache.kafka.common.internals.Topic
  
  /**
   * 定义特质, 抽取所有ETL公共方法
   */
  trait BaseETL[T] {
    /**
     * 根据业务抽取kafka数据
     *
     * @param topic : 主题名称
     * @return
     */
    def getKafkaDataStream(topic: String): DataStream[T]
  
    /**
     * etl 数据操作
     */
    def process()
  }
  
  ```

  

- etl 类

  ```scala
  package com.dcits.realtime.etl.`trait`
  
  import com.dcits.realtime.etl.utils.{GlobalConfigUtil, KafkaPropsUtil}
  import org.apache.flink.api.common.serialization.SimpleStringSchema
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
  
  abstract class MysqlBaseETL(env: StreamExecutionEnvironment) extends BaseETL[String] {
    /**
     * 根据业务抽取kafka数据
     *
     * @param topic : 主题名称
     * @return
     */
    override def getKafkaDataStream(topic: String): DataStream[String] = {
      // 创建消费者对象
      val kafkaConsumer = new FlinkKafkaConsumer[String](
        topic,
        new SimpleStringSchema(),
        KafkaPropsUtil.getKafkaProperties
      )
      // 将 kafkaConsumer 封装到env数据源中
      val kafkaDataStream: DataStream[String] = env.addSource(kafkaConsumer)
      // 返回消费者消费到的数据
      kafkaDataStream
    }
  
  
  }
  
  ```

  

## 5. 全量加载数据示例

> mysql -> redis

- 维度表定义

  ```scala
  package com.dcits.realtime.etl.bean
  
  import com.alibaba.fastjson.JSON
  
  import scala.beans.BeanProperty
  
  /**
   * 商品类 维度表样例类
   */
  // @BeanProperty 会生成getter和setter方法
  case class DimGoodsEntity(@BeanProperty goodsId: Long = 0,
                            @BeanProperty goodsName: String = "")
  
  /**
   * 商品样例类伴生对象: 为了解析数据
   */
  object DimGoodsEntity{
    def apply(json:String): DimGoodsEntity ={
    if(json != null){
      val jsonObj  = JSON.parseObject(json)
      new DimGoodsEntity(
        jsonObj.getLong("goodsId"),
        jsonObj.getString("goodsName")
      )
    }else{
      new DimGoodsEntity()
    }
    }
  }
  ```

- 全量数据加载

  ```scala
  package com.dcits.realtime.etl.dataloader
  
  import com.alibaba.fastjson.JSON
  import com.alibaba.fastjson.serializer.SerializerFeature
  import com.dcits.realtime.etl.bean.DimGoodsEntity
  import com.dcits.realtime.etl.utils.RedisUtil
  import redis.clients.jedis.Jedis
  import sun.plugin2.message.Serializer
  
  import java.sql.{Connection, DriverManager}
  
  /**
   * 将数据库中的dim表装载到redis
   */
  object DimDataLoader {
    def main(args: Array[String]): Unit = {
      // 注册mysql 驱动
      Class.forName("com.mysql.jdbc.Driver")
      // 创建mysql连接
      val connect = DriverManager.getConnection(
        s"jdbc:mysql://127.0.0.1:3306/etl_py",
        "root",
        ""
      )
      // 创建redis 的连接
      val jedis = RedisUtil.getJedis
      // 指定写入的redis 库
      jedis.select(1)
      // 写入到redis
      loadDimGoods(connect, jedis)
      connect.close()
    }
  
    /**
     * 加载商品维度表
     */
    def loadDimGoods(connection: Connection, jedis: Jedis): Unit = {
      // 定义sql查询语句
      val sqlStr = """select id as goodsId,name as goodsName FROM test1"""
      // 创建statement
      val statement = connection.createStatement()
      val resSet = statement.executeQuery(sqlStr)
      // 遍历所有商品的数据
      while (resSet.next()) {
        val goodsId = resSet.getLong("goodsId")
        val goodsName = resSet.getString("goodsName")
        // 写入到redis
        // 先将每一行都转为json, 然后存入redis
        val goodsEntity = DimGoodsEntity(goodsId, goodsName)
        println(goodsEntity)
        val goodsEntityJson: String = JSON.toJSONString(goodsEntity,SerializerFeature.DisableCircularReferenceDetect) // 关闭循环引用
        jedis.hset("dim_goods_info", goodsId.toString, goodsEntityJson)
      }
    }
  }
  
  
  ```

  



## 6. 增量更新维度表数据

> kafka -> redis

```scala
package com.dcits.realtime.etl.async

import com.alibaba.fastjson.serializer.SerializerFeature
import com.alibaba.fastjson.{JSON, JSONObject}
import com.dcits.realtime.etl.`trait`.MysqlBaseETL
import com.dcits.realtime.etl.bean.DimGoodsEntity
import com.dcits.realtime.etl.utils.RedisUtil
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.kafka.common.internals.Topic
import redis.clients.jedis.Jedis

/**
 * 所有的sync子任务
 *
 * @param env
 */
case class SyncDimData(env: StreamExecutionEnvironment,topic:String) extends MysqlBaseETL(env) {
  /**
   * etl 数据操作
   */
  override def process(): Unit = {
    /**
     * 实现步骤
     * 1. 获取数据源
     * 2. 过滤出维度表
     * 3. 处理同步数据 更新到维度表
     */
    // 1. 获取数据源
    val allDataStream = getKafkaDataStream(topic)
    allDataStream.print()
    // 2. 过滤维度表
    val dimDataStream = allDataStream
      .filter(rowData => {
        val jsonObject = JSON.parseObject(rowData)
        jsonObject.getString("table_name") match {
          case "test1" => true
          case _ => false
        }
      }
      )
    // 3. 处理同步数据 更新到维度表
    dimDataStream.addSink(new RedisSink)
  }
}

/**
 * redisSink
 */

class RedisSink extends RichSinkFunction[String] {
  var jedis: Jedis = _

  override def invoke(value: String, context: SinkFunction.Context[_]): Unit = {
    val jsonObject = JSON.parseObject(value)
    jsonObject.getString("type") match {
      case eventType if (eventType == "insert" || eventType == "update") => updateDimData(value)
      case "delete" => deleteDimData(value)
      case _ =>
    }
  }

  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    // 获取一个连接
    jedis = RedisUtil.getJedis
    // 选择数据库
    jedis.select(1)
  }

  override def close(): Unit = {
    if (jedis.isConnected) {
      jedis.close()
    }
  }

  /**
   * 更新
   *
   * @param value
   */
  def updateDimData(value: String): Unit = {
    val jsonObject = JSON.parseObject(value)
    val goodsId = jsonObject.getLong("goodsId")
    val goodsName = jsonObject.getString("goodsName")
    val goodsEntity = DimGoodsEntity(goodsId, goodsName)
    val goodsEntityJson: String = JSON.toJSONString(goodsEntity, SerializerFeature.DisableCircularReferenceDetect) // 关闭循环引用
    jedis.hset("dim_goods_info", goodsId.toString, goodsEntityJson)
  }

  /**
   * 删除
   *
   * @param value
   */
  def deleteDimData(value: String): Unit = {
    val jsonObject = JSON.parseObject(value)
    val goodsId = jsonObject.getLong("goodsId")
    jedis.hdel("dim_goods_info", goodsId.toString)
  }
}

```

## 7. main 程序入口

```scala
package com.dcits.realtime.etl.app

import com.dcits.realtime.etl.async.SyncDimData
import com.dcits.realtime.etl.utils.GlobalConfigUtil
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._

/**
 * 创建实时 ETL 驱动类
 * 实现ETL业务
 */
object VerticaApp {
  def main(args: Array[String]): Unit = {
    /**
     * // 1. 初始化flink 环境
     * // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
     * // 3. 开启flink 的checkpoint
     * // 4. 消费kafka 数据
     * // 5. 实现etl作业
     * // 6. 执行任务
     */
    // 1. 初始化flink 环境
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 2. 设置并行度, 测试环境为1. 生产环境推荐提交作业时指定并行度
    env.setParallelism(1)
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    val checkpointPath: String = GlobalConfigUtil.`checkpointPath`
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
    // 5. ETL 业务
    // 5.1 dim表的增量抽取
    val syncProcess = new SyncDimData(env, topic = "sensor")
    syncProcess.process()
    // 6. 执行
    env.execute("realtime etl")
  }
}

```



# 三. 容错策略

## 1. 启动flink checkPoint

```scala
    // 3. 开启flink 的checkpoint
    env.enableCheckpointing(interval = 500L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    ) // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 5000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
```

## 2. 启用状态后端 保存checkPoint

```scala
val checkpointPath: String = GlobalConfigUtil.`checkpointPath`
    val backend = new RocksDBStateBackend(checkpointPath)
    env.setStateBackend(backend)
```

# 四. 问题和待优化

## 1. 待优化

### 1.1 vertica 实现数据库连接池

```scala
package com.dcits.realtime.etl.utils

import java.sql.{Connection, DriverManager}
import java.util.Properties

object VerticaUtil {
  private val verticaProp = new Properties()
  verticaProp.put("user", GlobalConfigUtil.`vertica.server.user`)
  verticaProp.put("password", GlobalConfigUtil.`vertica.server.password`)

  /**
   * 获取vertica连接
   * todo 数据库连接池 (官方推荐使用开源 c3p0 或 DBCP 库 来实现连接池)
   */
  def getConn: Connection ={
    val conn: Connection = DriverManager.getConnection(
      f"""
         |jdbc:vertica://
         |${GlobalConfigUtil.`vertica.server.ip`}:
         |${GlobalConfigUtil.`vertica.server.port`}/
         |${GlobalConfigUtil.`vertica.server.dbname`}
         |""".stripMargin,
      verticaProp);
    conn
  }
}


```

### 1.2 fastApi

- fastApi反序列化 待加入try...catch...

### 1.3 表数量过多场景

- 如果数据表过多, 上千上万张, 如果提高开发效率

## 2. 问题

1. 反序列化报错会触发checkoint, 如果try捕获到该异常是否触发checkPoint