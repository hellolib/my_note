## 场景介绍

### 1. 场景

- 源端: kafka
- 目标端: mysql
- flink开启 checkpoint ,由checkpoint 管理kafka offset
- flink版本: 1.10

### 2. 目标

- flink 读取kafka ,写入mysql 时如果失败,checkpoint 不提交kafka offset, 保证数据不丢失

## 解决方案

> 场景模拟: kafka  => flink => mysql

### 1. RichSinkFunction 大坑

- code(sink)

  ```scala
  class MysqlJDBCSink() extends RichSinkFunction[SqlObjReduce] {
  
    // 自定义连接, 预编译语句
    var conn: Connection = _
    var checkConn: Connection = _
    var insertStmt: PreparedStatement = _
    var updateStmt: PreparedStatement = _
    var preInsertCheckSQl: PreparedStatement = _
  
    def processFunc(value: SqlObjReduce): Unit = {
      // 业务逻辑处理
      var gxsj = NowDate()
      val insertSQl = conn.prepareStatement(s"insert into sensor_db.test_hzb(swjg_dm,fpzt,hjje,gxsj,fs) values('${value.swjg_dm}','${value.fpzt}','${value.hjje}','${gxsj}','${value.fs}')")
      var insert = insertSQl.executeUpdate()
      //    }
      conn.commit()
    }
  
    // open 上下文管理创建数据库连接
    override def open(parameters: Configuration): Unit = {
      super.open(parameters)
      conn = DriverManager.getConnection("jdbc:mysql://10.0.23.109:3306/sensor_db?rewriteBatchedStatements=true", "root", "123456")
      conn.setAutoCommit(false)
    }
  
    // 调用连接，执行 sql
    override def invoke(value: SqlObjReduce, context: SinkFunction.Context[_]): Unit = {
      try {
        // 是否调用逻辑处理方法
        processFunc(value)
      } catch {
        case e: Exception => println(s"数据异常:${e.toString}")
          throw new Exception(e.toString)
      }
    }
  
    override def close(): Unit = {
      insertStmt.close()
      updateStmt.close()
      conn.close()
    }
  
    def NowDate(): String = {
      val now: Date = new Date()
      val dateFormat: SimpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
      val date = dateFormat.format(now)
      return date
    }
  }
  
  ```

  - 其中自定义sink继承的是`RichSinkFunction`, 然后手动管理事务

  >**坑:**
  >
  >checkpoint 保存时,不会检查 `RichSinkFunction` 状态, 但是checkpoint保存后kafka offset自动提交, 造成丟数!

### 2. 两阶段提交保证数据一致性

> TwoPhaseCommitSinkFunction: 2pc提交主要实现**beginTransaction(开启事务准备工作)、preCommit(准备提交)、commit(正式提交)、abort(丢弃)四个方法**

- 实现流程:
  1. 继承TwoPhaseCommitSinkFunction
  2. 实现invoke()、beginTransaction()、preCommit()、commit()、abort()等方法
  3. beginTransaction---->preCommit，同时开启下一次beginTransaction---->commit---->preCommit，同时开启下一次beginTransaction---->commit
- code

- ```scala
  class MysqlJDBCSinkCheckpoint() extends TwoPhaseCommitSinkFunction[SqlObjReduce, String, Void](
    createTypeInformation[String].createSerializer(new ExecutionConfig),
    createTypeInformation[Void].createSerializer(new ExecutionConfig)) {
    var conn: Connection = _
  
    private def buildConn(): Unit = {
      if (conn == null) {
        conn = DriverManager.getConnection("jdbc:mysql://10.0.23.109:3306/sensor_db?rewriteBatchedStatements=true", "root", "123456")
        conn.setAutoCommit(false)
        println("buildConn")
      }
    }
  
    override def invoke(txn: String, in: SqlObjReduce, context: SinkFunction.Context[_]): Unit = {
      println(in)
      val ps = conn.prepareStatement(
        s"""
           |insert into sensor_db.kpxxb_hzb(swjg_dm,fpzt,hjje,fs) values('${in.swjg_dm}',2,12.0,2)
           |""".stripMargin
      )
      ps.execute()
    }
  
    override def beginTransaction(): String = {
      println("beginTransaction")
      buildConn()
      // todo 优化字符串的关联关系
      return UUID.randomUUID().toString
    }
  
    override def preCommit(txn: String): Unit = {
      println("preCommit")
    }
  
    override def commit(txn: String): Unit = {
      println("commit")
      conn.commit()
    }
  
    override def abort(txn: String): Unit = {
      println("abort")
      conn.rollback()
    }
  }
  ```

## 原理解析

- 2pc提交主要实现**beginTransaction(开启事务准备工作)、preCommit(准备提交)、commit(正式提交)、abort(丢弃)四个方法**, 其中数据是在invoke方法中获取到的.

### 1. checkpoint 和 kafka offset

- 默认在不开启checkpoint时: 
  - flink 会自动周期性提交kafka offset,可以通过参数`enable.auto.commit / auto.commit.interval.ms`配置
- 开启了checkpoint时:
  - flink会在每次checkpoint完成时自动提交offset, 可以通过`setCommitOffsetsOnCheckpoints(boolean) `来启用关闭,默认为true,如果为false, flink将不再提交kafka offset.
  - 如果sink继承: `RichSinkFunction`, flink在提交offset时不会检查sink状态是否成功, 会出现数据库写入失败,但是offset提交成功,造成丟数现象
  - 如果sink继承: `TwoPhaseCommitSinkFunction`,  flink在提交offset前检查sink状态, 如果失败, offset不会被提交成功.

### 2. 两阶段提交(2pc)

> 第一阶段: 投票阶段
>
> 第二阶段: 提交/执行阶段, 其中 提交/执行阶段又包含: 成功处理流程,异常处理流程

- 两阶段提交又称**2PC**,2PC是一个非常经典的`中心化的原子提交协议`。中心化是指协议中有两类节点：一个是中心化`协调者节点`（coordinator）和`N个参与者节点`（partcipant）。

- **缺点:** 

  - 性能: 复杂的流程带来的负面影响可想而知: 性能下降; 无论是在第一阶段的过程中,还是在第二阶段,**所有的参与者资源和协调者资源都是被锁住的**,只有当所有节点准备完毕，事务 **协调者** 才会通知进行全局提交，

    **参与者** 进行本地事务提交后才会释放资源。这样的**过程会比较漫长，对性能影响比较大**。

  - 单节点故障: 由于**协调者**的重要性，一旦 **协调者** 发生故障。**参与者** 会一直阻塞下去。尤其在第二阶段，**协调者** 发生故障，那么所有的 **参与者** 还都处于锁定事务资源的状态中，而无法继续完成事务操作。（虽然协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

>  场景模拟: **订单服务A**，需要调用 **支付服务B** 去支付，支付成功则处理购物订单为待发货状态，否则就需要将购物订单处理为失败状态。

#### a. 第一阶段: 投票阶段

![image-20220610113628711](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220610113628711.png)

- 第一阶段主要分为3步

  1. `事务询问`: **协调者** 向所有的 **参与者** 发送**事务预处理请求**，称之为**Prepare**，并开始等待各 **参与者** 的响应。

  2. `执行本地事务`: 各个 **参与者** 节点执行本地事务操作,但在执行完成后并**不会真正提交数据库本地事务**，而是先向 **协调者** 报告说：“我这边可以处理了/我这边不能处理”。.

  3. `各参与者向协调者反馈事务询问的响应`: 如果 **参与者** 成功执行了事务操作,那么就反馈给协调者 **Yes** 响应,表示事务可以执行,如果没有 **参与者** 成功执行事务,那么就反馈给协调者 **No** 响应,表示事务不可以执行。

- 第一阶段执行完后，会有两种可能。

  1. 所有都返回Yes. 
  2. 有一个或者多个返回No。



#### b. 第二阶段: 提交/执行阶段 (成功流程)

- 成功条件: 所有返回这都返回yes

  ![image-20220610114828676](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220610114828676.png)

- 第二阶段主要分为两步

  1. `所有的参与者反馈给协调者的信息都是Yes,那么就会执行事务提交`: **协调者** 向 **所有参与者** 节点发出Commit请求.

  2. `事务提交`: **参与者** 收到Commit请求之后,就会正式执行本地事务Commit操作,并在完成提交之后释放整个事务执行期间占用的事务资源。

#### c. 第二阶段: 提交/执行阶段 (失败流程)

- 失败条件: 任何一个 **参与者** 向 **协调者** 反馈了 **No** 响应,或者等待超时之后,协调者尚未收到所有参与者的反馈响应。

  ![image-20220610115003303](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220610115003303.png)

- 异常流程第二阶段也分为两步

  1. `发送回滚请求`: **协调者** 向所有参与者节点发出 **RoollBack** 请求.

  2. `事务回滚`: **参与者** 接收到RoollBack请求后,会回滚本地事务。

  

### 3. flink 两阶段提交

- `FlinkKafkaProducer`继承了`TwoPhaseCommitSinkFunction`
- `FlinkKafkaConsumer `如果在写入sink时想要做到两阶段提交, sink类也需要继承` TwoPhaseCommitSinkFunction`

#### a. TwoPhaseCommitSinkFunction

- TwoPhaseCommitSinkFunction类继承了RichSinkFunction类且实现了CheckPointedFunction接口以及CheckPointListener接口
- 重写 invoke()、beginTransaction()、preCommit()、commit()、abort()等方法

- **sink类继承了`TwoPhaseCommitSinkFunction`时, 每次保存checkpoint之前, 都会执行beginTransaction()、preCommit()方法, 去检查上一次checkpoint期间是否存在异常的事务; 只有flink检测到数据流的时候 invoke()方法才会被执行**.
  - 比如checkpoint每10s进行一次，此时用FlinkKafkaConsumer实时消费kafka中的消息，消费并处理完消息后，进行一次预提交数据库的操作，如果预提交没有问题，10s后进行真正的插入数据库操作，如果插入成功，进行一次checkpoint，flink会自动记录消费的offset，可以将checkpoint保存的数据放到hdfs中，如果预提交出错，比如在5s的时候出错了，此时Flink程序就会进入不断的重启中，重启的策略可以在配置中设置，当然下一次的checkpoint也不会做了，checkpoint记录的还是上一次成功消费的offset，本次消费的数据因为在checkpoint期间，消费成功，但是预提交过程中失败了，注意此时数据并没有真正的执行插入操作，因为预提交（preCommit）失败，提交（commit）过程也不会发生了。等你将异常数据处理完成之后，再重新启动这个Flink程序，它会自动从上一次成功的checkpoint中继续消费数据，以此来达到Kafka到Mysql的Exactly-Once。

#### b. 第一阶段: 投票阶段

![image-20220611102501572](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220611102501572.png)

#### c. 第二阶段: 提交/执行阶段

![image-20220611102515151](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20220611102515151.png)

#### e. 完整代码示例

```scala
package dcits.com.checkPointTest

import com.alibaba.fastjson.JSON
import org.apache.flink.api.common.ExecutionConfig
import org.apache.flink.api.common.restartstrategy.RestartStrategies
import org.apache.flink.api.common.typeutils.base.VoidSerializer
import org.apache.flink.api.common.typeutils.base.StringSerializer
import org.apache.flink.configuration.Configuration
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.runtime.state.filesystem.FsStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction, TwoPhaseCommitSinkFunction}
import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment, _}
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.kafka.clients.consumer.ConsumerRecord

import java.io.BufferedWriter
import java.sql.{Connection, DriverManager, PreparedStatement}
import java.text.SimpleDateFormat
import java.util
import java.util.{Date, Properties, UUID}

object CheckPointTest {

  def main(args: Array[String]): Unit = {
    val topicStr = "SENSOR_DB.KPXXB_HZB_SPLIT"
    val topics = new util.LinkedList[String]
    if (topicStr.indexOf(",") != -1) {
      val topicLis = topicStr.split(",")
      for (topic <- topicLis) {
        topics.add(topic)
      }
    } else {
      topics.add(topicStr)
    }
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    // 设置开启checkpoint// 设置开启checkpoint
    env.setParallelism(1)
    //    env.enableCheckpointing(20000);
    env.enableCheckpointing(interval = 5000L, mode = CheckpointingMode.EXACTLY_ONCE) // 打开一致性检查点设置, 默认500ms,EXACTLY_ONCE
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1) // 默认为1 , 设置最大允许几个check point 同时进行
    env.getCheckpointConfig.setCheckpointTimeout(3600 * 1000) // 超时时间1小时
    env.getCheckpointConfig.enableExternalizedCheckpoints(
      CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    )
    // 当前作业被取消的时候, 保留之前的checkPoint, 避免数据丢失
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 2000L)) // 固定时间间隔重启, 5s重启一次, 重启3次
    // 开启Incremental Checkpoint
    //  val checkpointPath: String = "file:///Users/liusaisai/workspace/scalaProject/FlinkExplore/src/main/tmp/flink-check-point-test"
    val checkpointPath: String = "file:///tmp/flink-check-point-test"
    val backend = new FsStateBackend(checkpointPath)
    env.setStateBackend(backend)

    // 定义kafka 属性
    val properties = new Properties()
    properties.put("bootstrap.servers", "10.0.23.106:9092")
    properties.put("zookeeper.connect", "10.0.23.106:2181")
    properties.put("group.id", "test-consumer-group")
    properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    // 取消kafka管理偏移量，让flink来管理偏移量
    properties.put("enable.auto.commit", "false")
    val flinkKafkaConsumer = new FlinkKafkaConsumer[ConsumerRecord[String, String]](
      topics, // topic
      new MyKafkaDeserializationSchema(),
      properties
    )
    flinkKafkaConsumer.setStartFromGroupOffsets()
    //    flinkKafkaConsumer.setCommitOffsetsOnCheckpoints(false)
    // 读取kafka数据
    val kafkaStream: DataStream[ConsumerRecord[String, String]] = env.addSource(flinkKafkaConsumer)

    val resultValue = kafkaStream
      .filter(dataObj => dataObj.value().contains("magic") && dataObj.value().contains("transactionEventCounter"))
      .map(dataObj => {
        Thread.sleep(1000)
        // 原始data数据
        val curTopic = dataObj.topic
        val curPartition = dataObj.partition
        val curOffset = dataObj.offset()
        val value: String = dataObj.value() // value 字符串
        val valueJsonObj = JSON.parseObject(value)
        val messageObj = valueJsonObj.getJSONObject("message")
        val operation = messageObj.getJSONObject("headers").getString("operation")
        val data = messageObj.getJSONObject("data")
        //        println(data)
        var swjg_dm = data.getString("SWJG_DM")
        var fpzt = data.getIntValue("FPZT")
        var je = data.getBigDecimal("JE")
        SqlObjReduce(swjg_dm = swjg_dm, fpzt = fpzt, hjje = je, fs = 1)
      })
      .keyBy(data => (data.swjg_dm, data.fpzt))
      //      .timeWindow(Time.minutes(10))
      .timeWindow(Time.seconds(2))
      .reduce(
        (oldData, newData) => {
          SqlObjReduce(swjg_dm = newData.swjg_dm, fpzt = newData.fpzt, hjje = oldData.hjje + newData.hjje, fs = oldData.fs + newData.fs)
        }
      )

    resultValue.addSink(new MysqlJDBCSinkCheckpointTest)
    resultValue.print()
    env.execute("check point test")
  }
}


class MysqlJDBCSinkCheckpoint() extends TwoPhaseCommitSinkFunction[SqlObjReduce, String, Void](
  createTypeInformation[String].createSerializer(new ExecutionConfig),
  createTypeInformation[Void].createSerializer(new ExecutionConfig)) {
  var conn: Connection = _

  private def buildConn(): Unit = {
    if (conn == null) {
      conn = DriverManager.getConnection("jdbc:mysql://10.0.23.109:3306/sensor_db?rewriteBatchedStatements=true", "root", "123456")
      conn.setAutoCommit(false)
      println("buildConn")
    }
  }

  override def invoke(txn: String, in: SqlObjReduce, context: SinkFunction.Context[_]): Unit = {
    println(in)
    val ps = conn.prepareStatement(
      s"""
         |insert into sensor_db.kpxxb_hzb(swjg_dm,fpzt,hjje,fs) values('${in.swjg_dm}',2,12.0,2)
         |""".stripMargin
    )
    ps.execute()
  }

  override def beginTransaction(): String = {
    println("beginTransaction")
    buildConn()
    // todo 优化字符串的关联关系
    return UUID.randomUUID().toString
  }

  override def preCommit(txn: String): Unit = {
    println("preCommit")
  }

  override def commit(txn: String): Unit = {
    println("commit")
    conn.commit()
  }

  override def abort(txn: String): Unit = {
    println("abort")
    conn.rollback()
  }
}

```

