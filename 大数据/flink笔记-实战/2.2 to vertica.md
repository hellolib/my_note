## 1. 项目依赖引入

- maven 项目引入 vertica JDBC

## 2. 简单查询

```scala
package com.dcits.verticaSimple

import java.sql.DriverManager
import java.util.Properties

object SimpleQueryVertica {
  def main(args: Array[String]): Unit = {
    val myProp = new Properties()
    myProp.put("user", "dbadmin")
    myProp.put("password", "oracle@123")
    val conn = DriverManager.getConnection(
      "jdbc:vertica://10.0.23.119:5433/scswsbf",
      myProp
    );
    val stmt = conn.createStatement
    val res = stmt.executeQuery("select * from HX_SB.v2h_check_tab")
    while (res.next()) {
      println(res.toString)
      println(res.getObject(1),res.getObject(2),res.getObject("bz"))
    }
    conn.close()
  }
}

```

## 3. JDBC简单插入

```scala
package com.dcits.verticaSimple

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}

import java.sql.{Connection, DriverManager, PreparedStatement}
import java.util.Properties


object SimpleInsertVertica {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    // 定义kafka 属性
    val properties = new Properties()
    properties.put("bootstrap.servers", "hadoop01:9092")
    properties.put("zookeeper.connect", "hadoop01:2181")
    properties.put("group.id", "flink-vertica")
    properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("auto.offset.reset", "latest")
    val flinkKafkaConsumer = new FlinkKafkaConsumer[String](
      "sensor", // topic
      new SimpleStringSchema(),
      properties
    )
    // 读取kafka数据
    val kafkaStream: DataStream[String] = env.addSource(flinkKafkaConsumer)
    val resultValue = kafkaStream
      .map(data => {
        val strArr: Array[String] = data.split(",")
        SensorReading(strArr(0).trim, strArr(1).trim.toLong, strArr(2).trim.toDouble)
      })
    resultValue.addSink(new VerticaSink)
    resultValue.print()

    env.execute("flink to vertica")
  }
}

/**
 * 创建样例类
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

/**
 * vertica sink
 */

class VerticaSink() extends RichSinkFunction[SensorReading] {
  // 自定义连接, 预编译语句
  var conn: Connection = _
  var insertStmt: PreparedStatement = _
  var updateStmt: PreparedStatement = _
  var id: Int = 1

  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    val myProp = new Properties()
    myProp.put("user", "dbadmin")
    myProp.put("password", "oracle@123")
    conn = DriverManager.getConnection(
      "jdbc:vertica://10.0.23.119:5433/scswsbf",
      myProp
    );
    insertStmt = conn.prepareStatement("INSERT INTO  HX_SB.flink_test VALUES (?,?,?)")
    updateStmt = conn.prepareStatement("UPDATE  HX_SB.flink_test set temperature=? where id=? ")
  }

  // 调用连接，执行 sql
  override def invoke(value: SensorReading, context: SinkFunction.Context[_]): Unit = {
    id += 1
    updateStmt.setInt(2, id)
    updateStmt.setDouble(1, value.temperature)
    updateStmt.execute()
    if (updateStmt.getUpdateCount == 0) {
      insertStmt.setInt(1, id)
      insertStmt.setString(2, value.id)
      insertStmt.setDouble(3, value.temperature)
      insertStmt.execute()
    }
  }

  override def close(): Unit = {
    insertStmt.close()
    updateStmt.close()
    conn.close()
  }
}
```

## 4. JDBC批量插入

> 使用时间窗口(timeWindowAll), 每隔一段时间插入一次

```scala
package com.dcits.verticaBatchInsert

import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}
import org.apache.flink.streaming.api.scala.function.ProcessAllWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector
import sun.nio.ch.sctp.SctpNet

import java.lang
import java.sql.{Connection, DriverManager, PreparedStatement, Statement}
import java.util.{Properties, UUID}
import scala.collection.mutable
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

/**
 * 定义样例类
 */
case class SensorReading(id: String, timestamp: Long, temperature: Double)

/**
 * 批量插入vertica
 */
object BatchInsertVertica {
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
//    env.enableCheckpointing(5000) // 启用检查点
    // kafka 属性配置
    val properties = new Properties()
    properties.put("zookeeper.connect", "hadoop01:2181")
    properties.put("bootstrap.servers", "hadoop01:9092")
    properties.put("group.id", "flink-vertica-batch")
    properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.put("auto.offset.reset", "latest")
    val flinkKafkaConsumer = new FlinkKafkaConsumer[String](
      "sensor", // topic
      new SimpleStringSchema(),
      properties
    )
    val kafkaStream: DataStream[String] = env.addSource(flinkKafkaConsumer)
    val resStream = kafkaStream
      .map(data => {
        val strArr: Array[String] = data.split(",")
        SensorReading(strArr(0).trim, strArr(1).trim.toLong, strArr(2).trim.toDouble)
      })
      .timeWindowAll(Time.seconds(10))
      .allowedLateness(Time.seconds(60))
      .process(new processAllWindowFunction)
    resStream.print("window data")
    resStream.addSink(new VerticaBatchInsertSink)
    env.execute("batch insert vertica")
  }
}

/**
 * VerticaBatchInsertSink
 */
class VerticaBatchInsertSink() extends RichSinkFunction[List[SensorReading]] {
  // 自定义连接, 预编译语句
  var conn: Connection = _
  var insertStmt: PreparedStatement = _

  override def open(parameters: Configuration): Unit = {
    super.open(parameters)
    val verticaProp = new Properties()
    verticaProp.put("user", "dbadmin")
    verticaProp.put("password", "oracle@123")
    conn = DriverManager.getConnection(
      "jdbc:vertica://10.0.23.119:5433/scswsbf",
      verticaProp
    );
    conn.setAutoCommit(false)
    insertStmt = conn.prepareStatement("INSERT INTO  HX_SB.flink_batch_insert_test VALUES (?,?,?)")
  }

  override def invoke(values: List[SensorReading], context: SinkFunction.Context[_]): Unit = {
    for (value <- values) {
      insertStmt.setString(1, value.id)
      insertStmt.setLong(2, value.timestamp)
      insertStmt.setDouble(3, value.temperature)
      insertStmt.addBatch()
    }
    insertStmt.executeBatch()
    conn.commit()
  }

  override def close(): Unit = {
    conn.close()
    insertStmt.close()
  }
}

/**
 * processWindowFunction
 */
class processAllWindowFunction() extends ProcessAllWindowFunction[SensorReading, List[SensorReading], TimeWindow] {
  override def process(context: Context, elements: Iterable[SensorReading], out: Collector[List[SensorReading]]): Unit = {
    val buffer: ListBuffer[SensorReading] = ListBuffer()
    val iterator = elements.iterator
    while (iterator.hasNext) {
      buffer.append(iterator.next())
    }
    if (buffer.isEmpty) {
      return
    }
    out.collect(buffer.toList)
  }
}
```

