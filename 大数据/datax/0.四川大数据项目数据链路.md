## 1. V2H

### 1.1 vertica 和 hdfs 的自带命令抽数

1. **exportdata**

   - 从vertica中读取parquet格式数据文件，并暂存到hdfs临时目录；
   - 传参： 源 schema 源 table

   ```sh
   #!/bin/bash
   ###判断hdfs中是否存在目录，如果已经存在，则删除目录###
   `hdfs dfs -test -e  hdfs://scsw-cdh/tmp/loaddata/${1}/${2}`
   if [[ $? == 0 ]];then
           echo "我在这里"
           sudo -u hive /usr/bin/hdfs dfs -rm -r hdfs://scsw-cdh/tmp/loaddata/${1}/${2}
   fi
   ###从配置表中获取表查询语句####
   select_sql=`/opt/vertica_client/bin/vsql -h 155.16.144.86 -p 5433 -U hive -w hive@scsw -t -b -c "select sql_text from sq_xtyw.loaddata_sql_config where table_schema=upper('${1}') and table_name=upper('${2}')"`
   echo $select_sql
   ###去掉查询结果中的特殊字符####
   select_sql=`echo $select_sql | sed 's//\t/g'`
   ###判断配置表中是否配置了该表的查询语句###
   if [[ `echo $select_sql | awk -v bi=100 '{print($1>bi)?"1":"0"}'`  -eq  0 ]];then
      echo "找不到查询语句，请查看配置表select * from sq_xtyw.loaddata_sql_config where table_schema=upper('${1}') and table_name=upper('${2}')"
      exit 1
   else
      ####执行导出命令，从vertica_client中导出数据####
      sql="EXPORT TO PARQUET(directory = 'hdfs://scsw-cdh/tmp/loaddata/${1}/${2}/',fileMode='1777',dirMode='777') as ${select_sql}"
      echo "$sql" >> /app/loaddata/loaddata.log
      start_time=`date +'%Y-%m-%d %H:%M:%S'`
      /opt/vertica_client/bin/vsql -h 155.16.144.86 -p 5433 -U hive -w hive@scsw -b -c "$sql" 
     ####判断导出语句是否正常结束，不正常返回结果1，脚本以结果1退出，taskctl报异常####
      if [[ "$?" == 1 ]];then
        exit 1
      else
        end_time=`date +'%Y-%m-%d %H:%M:%S'`
        start_seconds=$(date --date="$start_time" +%s);
        end_seconds=$(date --date="$end_time" +%s);
        echo "${1} ${2} runtime is "$((end_seconds-start_seconds))"s" >> /app/loaddata/run.log
     ####判断hdfs中是否写入文件，正常导出并有数据，返回结果1，正常退出
        result=$(hdfs dfs -ls  hdfs://scsw-cdh/tmp/loaddata/${1}/${2}/ |head -1|wc -l)
        if [[ $result == 1 ]];then
           exit 0
     ###hdfs中没有文件写入，从vertica_client中查询该表数据量，比对结果是否一致，一致以0正常结束，否则以1异常结束
        else
   	 tab_count=`/opt/vertica_client/bin/vsql -h 155.16.144.86 -p 5433 -U hive -w hive@scsw -t -b -c "select count(*) from ${1}.${2}"`
            tab_count=`echo $tab_count | sed 's/ //g'`
            if [ `echo $tab_count | awk -v bi=100 '{print($1>bi)?"1":"0"}'`  -eq  0 ]; then
               exit 0
            else 
               exit 1
            fi
        fi
      fi
   fi
   
   
   ```

2. **importdata**

   - 从hdfs 临时目录中获取parquer格式文件，导入到hive中！
   - 传参： 源schema 源table 目标表schema 目标表table

   ```sh
   #!/bin/bash
   sql="load data inpath 'hdfs://scsw-cdh/tmp/loaddata//${1}/${2}/' overwrite into table ${1}.${2}"
   echo "$sql" >> /app/loaddata/loaddata.log
   #export HADOOP_HOME=/hadoop
   #export HIVE_HOME=/hive
   #export PATH=$PATH:$HADOOP_HOME/bin:$HIVE_HOME/bin
   #export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib:$HIVE_HOME/lib
   #/usr/bin/hdfs  dfs -test -s  hdfs://scsw-cdh/tmp/loaddata/${1}/${2}/
   result=$(hdfs dfs -ls  hdfs://scsw-cdh/tmp/loaddata/${1}/${2}/ |head -1|wc -l)
   echo "${1} ${2} $result" >> /app/loaddata/aaa.log
   echo "${result}"
   if [[ $result == 1 ]];then
      /usr/bin/beeline -u jdbc:hive2://155.16.144.3:10000 -n hive -p C!oudera123 -e "load data inpath 'hdfs://scsw-cdh/tmp/loaddata/${1}/${2}/' overwrite into table ${3}.${4}"
      if [[ $? == 1 ]];then
          exit 1
      fi
      /opt/vertica_client/bin/vsql -h 155.16.144.86 -p 5433 -U hive -w hive@scsw -b -c "insert into sq_xtyw.v2h_check_tab values('${3}','${4}','N');commit;"
   else
           echo "进入异常"
           sudo -u hive /usr/bin/hdfs dfs -rm -r hdfs://scsw-cdh/tmp/loaddata/${1}/${2}
   	echo "${1}.${2}数据为空">>/app/loaddata/exception.log
   fi
   
   ```

   

## 2. H2V

### 2.1 vertica copy

- vertcia 使用copy命令从hdfs文件系统中复制并插入到表中

  - 传参 目标端schema 目标端table 源端schema 源端table

  ```sh
  /opt/vertica/bin/vsql -h 155.15.144.86 -p 5433 -U dbadmin -w Scswadmin123! -b -c "copy ${1}.${2} from 'hdfs://scsw-cdh/user/hive/warehouse/${3}.db${4}/*' parquet ABORT ON ERROR;"
  ```

  

## 3. O2H

> sqoop 方式抽取

### 3.1 源端增量抽取

- qlik 增量抽取

### 3.2 临时需求

- 使用sqoop 抽数

  - 传参： 

  ```sh
  
  #!/bin/bash
  #sql="sudo -u hive /opt/sqoop-1.4.6/bin/sqoop import --connect jdbc:oracle:thin:@${1}:${2}/${3} --username ${4} --password ${5}  --table ${6}.${7} -m 1 --hive-import --hive-overxrite --hive-database ${8} --hive-table "${9}" --null-non-string 'null'  --null-string 'null' --delete-target-dir --direct"
  #echo "$sql" >> /app/importdata/importdata.log
  start_time=`date +'%Y-%m-%d %H:%M:%S'`
  #/opt/vertica_client/bin/vsql -h 155.16.144.86 -p 5433 -U hive -w hive@scsw -b -c "$sql" >> /app/loaddata/loaddata.log
  sudo -u hive /opt/sqoop-1.4.6/bin/sqoop import --connect jdbc:oracle:thin:@${1}:${2}/${3} --username ${4} --password ${5}  --table ${6}.${7} -m 1 --hive-import --hive-overwrite --hive-database ${8} --hive-table "${9}" --null-non-string 'NULL' --null-string 'NULL' --delete-target-dir --direct
  impala-shell -i 155.16.144.3:21000 -u hive   -l --auth_creds_ok_in_clear  --ldap_password_cmd='echo -n C!oudera123' -q "invalidate metadata ${8}.${9}"
  end_time=`date +'%Y-%m-%d %H:%M:%S'`
  start_seconds=$(date --date="$start_time" +%s);
  end_seconds=$(date --date="$end_time" +%s);
  echo "${6} ${7} runtime is "$((end_seconds-start_seconds))"s" >> /app/importdata/run.log
  
  ```

  