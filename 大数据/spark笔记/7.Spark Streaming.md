## 1. 流计算概述

- 静态数据

  <img src="/Users/liusaisai/Library/Application Support/typora-user-images/image-20210909220221619.png" alt="image-20210909220221619" style="zoom:33%;" />

- 流数据

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909220349308.png" alt="image-20210909220349308" style="zoom:50%;" />

- 批量计算和实时计算

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909220447408.png" alt="image-20210909220447408" style="zoom: 50%;" />

- 特点

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909220927106.png" alt="image-20210909220927106" style="zoom:50%;" />



## 2.Spark Streaming 概述

> RDD 是 Spark Core 的数据抽象
>
> Dataframe 是 Spark SQL 的数据抽象
>
> DStrame 是 Spark Streaming 的数据抽象

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909221445398.png" alt="image-20210909221445398" style="zoom:50%;" />

> Spark 是以线程级别并行, 实时响应级别高
>
> 可以实现秒级响应, 变相实现高效的流计算

- **spark streaming 并不是真正的流计算, 而是通过短时间段的微小批处理去模拟流计算;** 
- Spark Streaming 的基本原理是将实时输入数据流以时间片 (秒级) 为单位进行拆分, 然后经 Spark 引擎以类似批处理的方式处理每个时间片数据

- Spark Streaming 最主要的抽象是 DStream (`Discretized Stream`, 离散化数据流) , 表示连续不断的数据流. 在内部实现上, Spark Streaming 的输入数据按照时间片 (如 1 秒) 分成一段一段, 每一段数据转换为 Spark 中的 RDD, 这些分段就是 Dstream, 并且**对 DStream 的操作都最终转变为对相应的 RDD 的操作.**

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909222155132.png" alt="image-20210909222155132" style="zoom:50%;" />

  

- spark streaming 和 facebook 的Storm 对比

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909222430191.png" alt="image-20210909222430191" style="zoom:50%;" />

- 未使用spark框的大数据架构

  ![image-20210909222707269](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909222707269.png)

- 使用spark 框架的大数据框架

  ![image-20210909222818393](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909222818393.png)

## 3. Spark Streaming 工作流程

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909223338179.png" alt="image-20210909223338179" style="zoom: 33%;" />

- 在 Spark Streaming 中, 会有一个组件 Receiver, 作为一个长期运行的 task 跑在一个 Executor 上
- 每个 Receiver 都会负责一个 input DStream (比如从文件中读取数据的文件流, 比如套接字流, 或者从 Kafka 中读取的一个输入流等等)
- Spark Streaming 通过 input DStream 与外部数据源进行连接, 读取相关数据

### 3.1 编写Streaming程序步骤

1. 创建输入Dstream, 定义输入源
2. 对Dstream进行转换和输出操作, 进行流计算
3. 使用**streamingContext.start()**启动, 开始接收数据和处理流程
4. 结束流计算程序, 使用streamingContext.awaitTermination() 结束流计算. 也可以手动结束: streamingContext.stop()

### 3.2 创建 StreamingContext 对象

- 如果要运行一个 Spark Streaming 程序, 就需要首先生成一个 StreamingContext 对象, 它是 Spark Streaming 程序的主入口
- 可以从一个 SparkConf 对象创建一个 StreamingContext 对象.

- 创建 StreamingContext对象

  - 在pyspark shell中穿件

    ```python
    # pyspark interactive terminal 
    >>> from pyspark.streaming import StreamingContext
    
    # sc 是自动生成时 SparkContext 对象实例
    >>> ssc = StreamingContext(sc, 1) # 1 时间间隔,每隔1秒启动一起流计算
    ```

  - 在独立的py程序中创建

    ```python
    from pyspark import SparkContext, SparkConf
    from pyspark.streaming import StreamingContext
    
    conf = SparkConf()
    conf.setAppName('TestDStream')
    conf.setMaster('local[2]')
    
    sc = SparkContext(conf=conf)
    ssc = StreamingContext(sc, 1)  # 1 时间间隔,每隔1秒启动一起流计算
    ```

## 4. 基本数据源

### 4.1 文件流

- 在 pyspark shell中创建文件流

  ```sh
  $ cd /usr/local/spark/mycode
  $ mkdir streaming
  $ cd streaming
  $ mkdir logfile
  $ cd logfile
  ```

  ```python
  from pyspark import SparkContext
  from pyspark.streaming import StreamingContext
  
  # 1创建StreamingContext对象, 定义数据源
  ssc = StreamingContext(sc, 10)  # 10 时间间隔,每隔十秒启动一起流计算
  lines = ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
  # 2设计流计算步骤
  words = lines.flatMap(lambda line: line.split(' '))
  wordCounts = words.map(lambda x :(x,1)).reduceByKey(lambda a,b:a+b)
  wordCounts.pprint()
  # 3启动流计算
  ssc.start()
  # 4.停止流计算
  ssc.awaitTermination() 
  ```

  

- 采用独立应用程序方式创建文件流

  ```python
  #!/usr/bin/env python3
  from pyspark import SparkContext, SparkConf
  from pyspark.streaming import StreamingContext
  
  conf = SparkConf()
  conf.setAppName('TestDStream')
  conf.setMaster('local[2]')
  
  sc = SparkContext(conf = conf)
  ssc = StreamingContext(sc, 10) # 10 时间间隔,每隔十秒启动一起流计算
  
  lines = ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
  words = lines.flatMap(lambda line: line.split(' '))
  wordCounts = words.map(lambda x : (x,1)).reduceByKey(lambda a,b:a+b)
  wordCounts.pprint()
  
  ssc.start()
  ssc.awaitTermination()
  ```

  运行代码

  ```sh
  $ cd /usr/local/spark/mycode/streaming/logfile/
  $ /usr/local/spark/bin/spark-submit FileStreaming.py
  ```

### 4.2 套接字流

- Spark Streaming 可以通过 Socket 端口监听并接收数据, 然后进行相应处理

- 客户端程序

  ```python
  #!/usr/bin/env python3
  from __future__ import print_function
  from pyspark import SparkContext
  from pyspark.streaming import StreamingContext
  
  import sys
  
  if __name__ == "__main__":
      if len(sys.argv) != 3:
          print("Usage: NetworkWordCount.py <hostname> <port>", file=sys.stderr)
          exit(-1)
  
      sc = SparkContext(appName="PythonStreamingNetworkWordCount")
      ssc = StreamingContext(sc, 1)
      lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
  
      counts = lines.flatMap(lambda line: line.split(" ")) \
          .map(lambda word: (word, 1)) \
          .reduceByKey(lambda a, b: a+b)
      counts.pprint()
  
      ssc.start()
      ssc.awaitTermination()
  ```

- 服务端程序

  - 服务端使用 net cat 模拟socket服务端

  ```sh
  nc -lk 9999
  ```

  - 执行步骤

    ![image-20210909235124438](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909235124438.png)

### 4.3 RDD 队列流

- 在调试 Spark Streaming 应用程序的时候, 我们可以使用 `streamingContext.queueStream(queueOfRDD)` 创建基于 RDD 队列的 DStream
- 新建一个 `RDDQueueStream.py` 代码文件, 功能是:每隔 1 秒创建一个 RDD, Streaming 每隔 2 秒就对数据进行处理

```python
#!/usr/bin/env python3
import time
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__ == "__main__":
    sc = SparkContext(appName="PythonStreamingQueueStream")
    ssc = StreamingContext(sc, 2)

    # 创建一个队列, 通过该队列可以把 RDD 推给一个 RDD 队列流
    rddQueue = []
    for i in range(5):
        rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 1001)], 10)]
        time.sleep(1)

    # 创建一个 RDD 队列流
    inputStream = ssc.queueStream(rddQueue)
    mappedStream = inputStream.map(lambda x: (x % 10, 1))
    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)
    reducedStream.pprint()

    ssc.start()
    ssc.stop(stopSparkContext=True, stopGraceFully=True) # stopGraceFully 队列里没有值了再停止
```

运行

```
$ cd /usr/local/spark/mycode/streaming/rddqueue
$ /usr/local/spark/bin/spark-submit RDDQueueStream.py
```

## 5.高级数据源

### 5.1 kafka 数据源

```python
#!/usr/bin/env python3
from __future__ import print_function

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

import sys


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: KafkaWordCount.py <zk> <topic>", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingKafkaWordCount")
    ssc = StreamingContext(sc, 1)

    zkQuorum, topic = sys.argv[1:]
    kvs = KafkaUtils.createStream(
        ssc,
        zkQuorum,
        "spark-streaming-consumer", # Consumer Group
        {topic: 1} # Topic and Partition
    )

    lines = kvs.map(lambda x: x[1])
    counts = lines.flatMap(lambda line: line.split(" ")) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()
```



## 6. DStream 转换操作

### 6.1 无状态转换

>无状态装换就是每次转换都是只关心这一个批次的数据, 不会关心历史批次的数据, 不会叠加状态

- `map(func)`: 对源 DStream 的每个元素, 采用 `func` 函数进行转换, 得到一个新的 DStream
- `flatMap(func)`: 与 map 相似, 但是每个输入项可用被映射为 **0 个或者多个**输出项
- `filter(func)`: 返回一个新的 DStream, 仅包含源 DStream 中满足函数 `func` 的项
- `repartition(numPartitions)`: 通过创建更多或者更少的分区改变 DStream 的并行程度
- `reduce(func)`: 利用函数 `func` 聚集源 DStream 中每个 RDD 的元素, 返回一个包含单元素 RDDs 的新 DStream
- `count()`: 统计源 DStream 中每个 RDD 的元素数量
- `union(otherStream)`: 返回一个新的 DStream, 包含源 DStream 和其他 DStream 的元素
- `countByValue()`: 应用于元素类型为 `K` 的 DStream 上, 返回一个 `(K, V)` 键值对类型的新 DStream, 每个键的值是在原 DStream 的每个 RDD 中的出现次数
- `reduceByKey(func, [numTasks])`: 当在一个由 `(K,V)` 键值对组成的 DStream 上执行该操作时, 返回一个新的由 `(K,V)` 键值对组成的 DStream, 每一个 key 的值均由给定的 `reduce(func)` 聚集起来
- `join(otherStream, [numTasks])`: 当应用于两个 DStream (一个包含 `(K,V)` 键值对,一个包含 `(K,W)` 键值对) , 返回一个包含 (K, (V, W)) 键值对的新 Dstream
- `cogroup(otherStream, [numTasks])`: 当应用于两个 DStream (一个包含 `(K,V)` 键值对,一个包含 `(K,W)` 键值对) , 返回一个包含 `(K, Seq[V], Seq[W])` 的元组, 很少用
- `transform(func)`: 通过对源 DStream 的每个 RDD 应用 RDD-to-RDD 函数, 创建一个新的 DStream. 支持在新的 DStream 中做任何 RDD 操作

### 6.2 有状态转换

**1. 滑动窗口转换操作**

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210910002600609.png" alt="image-20210910002600609" style="zoom:40%;" />

- 事先设定一个滑动窗口的长度 (也就是窗口的持续时间)
- 设定滑动窗口的时间间隔 (每隔多长时间执行一次计算) , 让窗口按照指定时间间隔在源 DStream 上滑动
- 每次窗口停放的位置上, 都会有一部分 Dstream (或者一部分 RDD) 被框入窗口内, 形成一个小段的 Dstream
- 可以启动对这个小段 DStream 的计算

- **滑动窗口转换操作 API**

  `.reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])`

  - 应用到一个 `(K,V)` 键值对组成的 DStream 上时, 会返回一个由 `(K,V)` 键值对组成的新的 DStream. 每一个 key 的值均由给定的 `reduce(func)` 进行聚合计算. 注意: 在默认情况下, 这个算子利用了 Spark 默认的并发任务数去分组. 可以通过 `numTasks` 参数的设置来指定不同的任务数

  `.reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`

  - **更加高效**的 `reduceByKeyAndWindow`, 每个窗口的 reduce 值, 是基于先前窗口的 reduce 值进行增量计算得到的; 它会对进入滑动窗口的新数据进行 reduce 操作, 并对离开窗口的老数据进行”逆向 reduce”操作. 但是, 只能用于”可逆 reduce 函数”, 即那些 reduce 函数都有一个对应的”逆向 reduce 函数” (以 `InvFunc` 参数传入)

    - 图解原理

    <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210910004245514.png" alt="image-20210910004245514" style="zoom:50%;" />

  ```python
  #!/usr/bin/env python3
  from __future__ import print_function
  import sys
  from pyspark import SparkContext
  from pyspark.streaming import StreamingContext
  
  if __name__ == "__main__":
      if len(sys.argv) != 3:
          print("Usage: WindowedNetworkWordCount.py <hostname> <port>", file=sys.stderr)
          exit(-1)
  
      sc = SparkContext(appName="PythonStreamingWindowedNetworkWordCount")
      ssc = StreamingContext(sc, 10)
      # 设置检查点 , 防止数据丢失
      ssc.checkpoint("file:///usr/local/spark/mycode/streaming/socket/checkpoint")
  
      lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
      counts = lines.flatMap(lambda line: line.split(" "))\
          .map(lambda word: (word, 1))\
          .reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)
  
      counts.pprint()
      ssc.start()
      ssc.awaitTermination()
  ```

  

**2. updateStateByKey 操作**

- 需要在跨批次之间维护状态时, 就必须使用 `updateStateByKey` 操作词频统计实例:

- 对于有状态转换操作而言, 本批次的词频统计, 会在之前批次的词频统计结果的基础上进行不断累加, 所以, 最终统计得到的词频, 是所有批次的单词的总的词频统计结果

```python
#!/usr/bin/env python3
from __future__ import print_function
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

import sys

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful.py <hostname> <port>", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful/")

    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])

    def updateFunc(new_values, last_sum):
        return sum(new_values) + (last_sum or 0)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" "))\
        .map(lambda word: (word, 1))\
        .updateStateByKey(updateFunc, initialRDD=initialStateRDD)

    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()
```

## 7. DStream 输出操作

### 7.1 保存到文本文件

```python
#!/usr/bin/env python3
from __future__ import print_function
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

import sys


def updateFunc(new_values, last_sum):
    return sum(new_values) + (last_sum or 0)

def main():
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful.py <hostname> <port>", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful/")

    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" "))\
        .map(lambda word: (word, 1))\
        .updateStateByKey(updateFunc, initialRDD=initialStateRDD)
running_counts.saveAsTextFiles("file:///usr/local/spark/mycode/streaming/stateful/output")
    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()


if __name__ == "__main__":
    main()
```

### 7.2 保存到数据库

```python
#!/usr/bin/env python3
from __future__ import print_function
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

import sys
import pymysql


def updateFunc(new_values, last_sum):
    return sum(new_values) + (last_sum or 0)

def dbfunc(records):
    db = pymysql.connect("localhost","root","123456","spark")
    cursor = db.cursor()
    def doinsert(p):
        sql = "insert into wordcount(word,count) values ('%s', '%s')" % (str(p[0]), str(p[1]))
        try:
            cursor.execute(sql)
            db.commit()
        except:
            db.rollback()
    for item in records:
        doinsert(item)

def func(rdd):
    repartitionedRDD = rdd.repartition(3)
    repartitionedRDD.foreachPartition(dbfunc)


def main():
    if len(sys.argv) != 3:
        print("Usage: NetworkWordCountStateful <hostname> <port>", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc = StreamingContext(sc, 1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful")

    # RDD with initial state (key, value) pairs
    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    running_counts = lines.flatMap(lambda line: line.split(" ")) \
        .map(lambda word: (word, 1)) \
        .updateStateByKey(updateFunc, initialRDD=initialStateRDD)
 
    running_counts.pprint()

    running_counts.foreachRDD(func)
    ssc.start()
    ssc.awaitTermination()

if __name__ == "__main__":
    main()

```

