## 1. SparkSQL简介

- Spark SQL是spark套件中一个模板，它将数据的计算任务通过SQL的形式转换成了RDD的计算，类似于Hive通过SQL的形式将数据的计算任务转换成了MapReduce。地位相当于hive.

- **Spark SQL的特点**：
   1、和Spark Core的无缝集成，可以在写整个RDD应用的时候，配置Spark SQL来完成逻辑实现。
   2、统一的数据访问方式，Spark SQL提供标准化的SQL查询。
   3、Hive的继承，Spark SQL通过内嵌的hive或者连接外部已经部署好的hive案例，实现了对hive语法的继承和操作。
   4、标准化的连接方式，Spark SQL可以通过启动thrift Server来支持JDBC、ODBC的访问，将自己作为一个BI Server使用

- **Spark SQL数据抽象：**
   1、RDD(Spark1.0)->DataFrame(Spark1.3)->DataSet(Spark1.6)
   2、Spark SQL提供了DataFrame和DataSet的数据抽象
   3、DataFrame就是RDD+Schema，可以认为是一张二维表格，劣势在于编译器不进行表格中的字段的类型检查，在运行期进行检查
   4、DataSet是Spark最新的数据抽象，Spark的发展会逐步将DataSet作为主要的数据抽象，弱化RDD和DataFrame.DataSet包含了DataFrame所有的优化机制。除此之外提供了以样例类为Schema模型的强类型
   5、DataFrame=DataSet[Row]
   6、DataFrame和DataSet都有可控的内存管理机制，所有数据都保存在非堆上，都使用了catalyst进行SQL的优化。

- **Spark SQL客户端查询：**
   1、可以通过Spark-shell来操作Spark SQL，spark作为SparkSession的变量名，sc作为SparkContext的变量名
   2、可以通过Spark提供的方法读取json文件，将json文件转换成DataFrame
   3、可以通过DataFrame提供的API来操作DataFrame里面的数据。
   4、可以通过将DataFrame注册成为一个临时表的方式，来通过Spark.sql方法运行标准的SQL语句来查询。

  

## 2. Dataframe

- RDD 和 DataFrame 区别
  - RDD 是分布式的 Java 对象的集合, 但是, 对象内部结构对于 RDD 而言却是不可知的

  - DataFrame 是一种以 RDD 为基础的分布式数据集, 提供了详细的结构信息

    ![image-20210909160426048](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909160426048.png)

### 2.1 创建

`SparkSession` 实现了 `SQLContext` 及 `HiveContext` 所有功能

> `SparkSession` 支持从不同的数据源加载数据, 并把数据转换成 `DataFrame`, 并且支持把 `DataFrame` 转换成 `SQLContext` 自身中的表, 然后使用 SQL 语句来操作数据

```python
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()
```

> 实际上, 在启动进入 pyspark 交互式环境以后, pyspark 就默认提供了一个 `SparkContext` 对象 (名称为 `sc` ) 和一个 `SparkSession` 对象 (名称为 `spark` )

在创建 DataFrame 时, 可以使用 `spark.read()` 或 `spark.read.format()` 操作, 从不同类型的文件中加载数据创建 DataFrame

```python
# 读取文本文件 people.txt 创建 DataFrame
spark.read.text("people.txt")
spark.read.format("text").load("people.txt")

# 读取 people.json 文件创建 DataFrame; 在读取本地文件或 HDFS 文件时, 要注意给出正确的文件路径
spark.read.json("people.json")
spark.read.format("json").load("people.json")

# 读取 people.parquet 文件创建 DataFrame
spark.read.parquet("people.parquet")
spark.read.format("parquet").load("people.parquet")
```

### 2.2 保存

```python
df.write.text("people.txt")
df.write.format("text").save("people.txt")

df.write.json("people.json")
df.write.format("json").save("people.json")

df.write.parquet("people.parquet")
df.write.format("parquet").save("people.parquet")
```

下面从示例文件 `people.json` 中创建一个 `DataFrame`, 名称为 `peopleDF`, 把 `peopleDF` 保存到另外一个 JSON 文件中, 然后, 再从 `peopleDF` 中选取一个列 (即 `name` 列), 把该列数据保存到一个文本文件中

```python
>>> peopleDF = spark.read.format("json").\
... load("file:///usr/local/spark/examples/src/main/resources/people.json")
>>> peopleDF.select("name", "age").write.format("json").\
... save("file:///usr/local/spark/mycode/sparksql/newpeople.json")
>>> peopleDF.select("name").write.format("text").\
... save("file:///usr/local/spark/mycode/sparksql/newpeople.txt")
```

会新生成一个名称为 `newpeople.json` 的**目录** (不是文件) 和一个名称为 `newpeople.txt` 的**目录** (不是文件)

- 示例

  ```python
  >>> peopleDF = spark.read.format("json").\
  ... load("file:///usr/local/spark/examples/src/main/resources/people.json")
  >>> peopleDF.select("name", "age").write.format("json").\
  ... save("file:///usr/local/spark/mycode/sparksql/newpeople.json")
  >>> peopleDF.select("name").write.format("text").\
  ... save("file:///usr/local/spark/mycode/sparksql/newpeople.txt")
  ```

  

### 2.3 常用操作

```python
>>> df.printSchema() # 查看模式信息
 
>>> df.select(df["name"], df["age"] + 1).show() # 查看name 列和age列, 并且age+1

>>> df.filter(df["age"] > 20).show() # 查看age>20的

>>> df.groupBy("age").count().show() # 计次

>>> df.sort(df["age"].desc(), df["name"].asc()).show() # 排序

# 其他
df.columns：显示列名
df.take(2)：取前2条，Row格式
df.toPandas()：将DataFrame格式的数据转成Pandas的DataFrame格式数据
df.collect()：收集所有数据
df.show()：显示数据；df.show(n)表示显示前n行
```

## 3. RDD 转换为Dataframe

### 3.1 利用反射机制推断 RDD 模式

- 数据文件people.txt

```
Michael,29
Andy,30
Justin,19
```

- 转换

```python
from pyspark.sql import Row

people = spark.sparkContext.\
    textFile("file:///usr/local/spark/examples/src/main/resources/people.txt").\
    map(lambda line: line.split(",")).\
    map(lambda p: Row(name=p[0], age=int(p[1])))

schemaPeople = spark.createDataFrame(people)

# 必须注册为临时表才能供下面的查询使用
schemaPeople.createOrReplaceTempView("people")

# DataFrame 中的每个元素都是一行记录, 包含 name 和 age 两个字段, 分别用 p.name 和 p.age 来获取值
personsDF = spark.sql("select name,age from people where age > 20") #DF
personsRDD = personsDF.rdd.map(lambda p:"Name: "+p.name+ ", "+"Age:"+str(p.age)) #RDD

personsRDD.foreach(print)

# Output
# Name: Michael, Age: 29
# Name: Andy, Age: 30
```

### 3.2 使用编程方式定义 RDD 模式

- 当无法提前获知数据结构时, 就需要采用编程方式定义 RDD 模式. 比如, 现在需要通过编程方式把 `people.txt` 加载进来生成 `DataFrame`, 并完成 SQL 查询

- 实现过程

  1. 制作表头
  2. 制作表中数据记录
  3. 拼接表头和表中的记录

  ![image-20210909172910786](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210909172910786.png)

  ```python
  from pyspark.sql.types import *
  from pyspark.sql import Row
  
  # 下面生成 "表头"
  schemaString = "name age"
  fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(" ")]
  schema = StructType(fields)
  
  # 下面生成 "表中的记录"
  lines = spark.sparkContext.\
      textFile("file:///usr/local/spark/examples/src/main/resources/people.txt")
  parts = lines.map(lambda x: x.split(","))
  people = parts.map(lambda p: Row(p[0], p[1].strip()))
  
  # 下面把 "表头" 和 "表中的记录" 拼装在一起
  schemaPeople = spark.createDataFrame(people, schema)
  
  # 注册一个临时表供下面查询使用
  schemaPeople.createOrReplaceTempView("people")
  results = spark.sql("SELECT name, age FROM people")
  results.show()
  
  # Output
  # +-------+---+
  # |   name|age|
  # +-------+---+
  # |Michael| 29|
  # |   Andy| 30|
  # | Justin| 19|
  # +-------+---+
  ```

  

## 4.  Spark SQL 读写数据库

- 环境准备
  1. 需要mysqljdbc驱动
  2. 放入spark/jars目录下

### 4.1 读取 MySQL 

MySQL 的读取利用了 JDBC Driver

```python
>>> jdbcDF = spark.read \
  .format("jdbc") \
  .option("driver","com.mysql.jdbc.Driver") \
  .option("url", "jdbc:mysql://localhost:3306/spark") \
  .option("dbtable", "student") \
  .option("user", "root") \
  .option("password", "123456") \
  .load()

>>> jdbcDF.show()
+---+--------+------+---+
| id|    name|gender|age|
+---+--------+------+---+
|  1| Xueqian|     F| 23|
|  2|Weiliang|     M| 24|
+---+--------+------+---+
```

### 4.2 写入 MySQL 

往 `spark.student` 表中插入两条记录

```python
#!/usr/bin/env python3

from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark import SparkContext,SparkConf
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()

# 下面设置模式信息
schema = StructType([StructField("id", IntegerType(), True), \
    StructField("name", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("age", IntegerType(), True)])

# 下面设置两条数据, 表示两个学生的信息
studentRDD = spark \
    .sparkContext \
    .parallelize(["3 Rongcheng M 26","4 Guanhua M 27"]) \
    .map(lambda x:x.split(" "))

# 下面创建 Row 对象, 每个 Row 对象都是 rowRDD 中的一行
rowRDD = studentRDD.map(lambda p:Row(int(p[0].strip()), p[1].strip(), p[2].strip(), int(p[3].strip())))

# 建立起 Row 对象和模式之间的对应关系, 也就是把数据和模式对应起来
studentDF = spark.createDataFrame(rowRDD, schema)

# 写入数据库
prop = {}
prop['user'] = 'root'
prop['password'] = '123456'
prop['driver'] = "com.mysql.jdbc.Driver"
studentDF.write.jdbc("jdbc:mysql://localhost:3306/spark",'student','append',prop)
```