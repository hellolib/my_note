## 1. 概述

- **Structured Streaming 的关键思想是将实时数据流视为一张正在不断添加数据的表. 可以把流计算等同于在一个静态表上的批处理查询, Spark 会在不断添加数据的无界输入表上运行计算, 并进行增量查询**
- **无界表**上对输入的查询将生成结果表, 系统每隔一定的周期会触发对无界表的计算并更新结果表

### 1.1 两种处理模型

- **微批处理**

  - Structured Streaming 默认使用微批处理执行模型, 这意味着 Spark流计算引擎会定期检查流数据源, 并对自上一批次结束后到达的新数据执行批量查询

  - 数据到达和得到处理并输出结果之间的延时超过 100 毫秒(在每次微批处理之前, 都会将日志偏移量写入到日志文件中, 防止处理过程中中断出错, 这样就造成了延迟)

    <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210910094505931.png" alt="image-20210910094505931" style="zoom:50%;" />

- **持续处理**

  - Spark 从 2.3.0 版本开始引入了持续处理的试验性功能, 可以实现流计算的毫秒级延迟

  - 在持续处理模式下, Spark 不再根据触发器来周期性启动任务, 而是**启动一系列的连续读取, 处理和写入结果的长时间运行的任务**

  - 只能保证数据至少被处理一次, 而不能保证数据只被处理一次

    ![image-20210910094747830](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210910094747830.png)

### 1.2 Spark Streaming 和 Structured Streaming的区别

- 相同点:
  - Spark Streaming 和 Structured Streaming 都是处理数据流
- 不同点:
  - Spark Streaming 的数据抽象是DStream ,本质是一系列的RDD, Structured Streaming的数据抽象是Dataframe
  - Structured Streaming和Spark SQL的数据抽象都是Dataframe, 但是Spark SQL处理静态数据, 这样Structrued Streaming 就将Spark SQL和Spark Streaming二者特性结合了起来
  - Structured Streaming 可以实现毫秒级响应(微批处理100毫秒级响应, 持续处理支持毫秒级响应), Spark Streaming 只能实现秒级响应
  - Structured Streaming 可以对Dataframe/Dataset 应用select, where, group, map, filter, flatMap等各种操作

## 2. Structured Streaming 程序编写

- 编写步骤
  1. 导入 spyspark模块
  2. 创建 Sparksession对象
  3. 创建输入数据源
  4. 定义流计过程
  5. 启动流计算井输出结果

- 实例任务: 

  - 一个包含很多行英文语句的数据流源源不断到达, Structured Streaming 程序对每行英文语句进行拆分, 并统计每个单词出现的频率

    ```python
    
    # 由于程序中需要用到拆分字符串和展开数组内的所有单词的功能
    # 所以引用了来自 `pyspark.sql.functions` 里面的 split 和 explode 函数
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import split, explode
    
    
    # 创建 SparkSession 对象
    if __name__ == "__main__":
        spark = SparkSession \
            .builder \
            .appName("StructuredNetworkWordCount") \
            .getOrCreate()
        spark.sparkContext.setLogLevel('WARN')
    
        # 创建输入数据源
        lines = spark \
            .readStream \
            .format("socket") \
            .option("host", "localhost") \
            .option("port", 9999) \
            .load()
    
        # 定义流计算过程
        words = lines.select(
          # explode 将容器中单词打散, 变成一行一个
          explode(
              split(lines.value, " ")
          ).alias("word")
          # alias 重新命名这一列为word
        )
        wordCounts = words.groupBy("word").count() # 进行分组统计
    
    
        # 启动流计算并输出结果 # 每隔8s启动一次流计算
        query = wordCounts \
            .writeStream \
            .outputMode("complete") \
            .format("console") \
            .trigger(processingTime="8 seconds") \ 
            .start()
        query.awaitTermination()
    
    
    ```

    

## 3. 输入源

### 3.1 file 源

- File 源 (或称为”文件源”) 以文件流的形式读取某个目录中的文件, 支持的文件格式为 csv, json, orc, parquet, text 等.

- **需要注意的是, 文件放置到给定目录的操作应当是原子性的, 即 不能长时间在给定目录内打开文件写入内容, 而是应当采取大部分操作系统都支持的, 通过写入到临时文件后移动文件到给定目录的方式来完成.**

  ```
  为了演示JSON格式文件的处理,这里随机生成一些JSON格式的文件来进行测试。代码文件内容如下:
  行的格式是类似如下:{" eventtime":1546939167," action":" logout"" district":" fujian"}\n
  ```

  

  ```python
  #!/usr/bin/env python3
  # -*- coding: utf-8 -*-
  # 导入需要用到的模块
  import os
  import shutil
  from pprint import pprint
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import window, asc
  from pyspark.sql.types import StructType, StructField
  from pyspark.sql.types import TimestampType, StringType
  
  # 定义 JSON 文件的路径常量
  TEST_DATA_DIR_SPARK = 'file:///tmp/testdata/'
  
  if __name__ == "__main__":
      # 定义模式, 为时间戳类型的 eventTime, 字符串类型的操作和省份组成
      schema = StructType([
          StructField("eventTime", TimestampType(), True),
          StructField("action", StringType(), True),
          StructField("district", StringType(), True)])
      spark = SparkSession \
          .builder \
          .appName("StructuredEMallPurchaseCount") \
          .getOrCreate()
   
      spark.sparkContext.setLogLevel('WARN')
  
      lines = spark \
          .readStream \
          .format("json") \
          .schema(schema) \
          .option("maxFilesPerTrigger", 100) \
          .load(TEST_DATA_DIR_SPARK)
  
      # 定义窗口
      windowDuration = '1 minutes'
      windowedCounts = lines \
          .filter("action = 'purchase'") \
          .groupBy('district', window('eventTime', windowDuration)) \
          .count() \
          .sort(asc('window'))
  
      # 启动流计算
      query = windowedCounts \
          .writeStream \
          .outputMode("complete") \
          .format("console") \
          .option('truncate', 'false') \
          .trigger(processingTime="10 seconds") \
          .start()
      query.awaitTermination()
  ```

### 3.2 kafka 源

- Kafka源是流处理最理想的输入源，因为它可以保证实时和容错。

- 实例测试:

  - 在这个实例中，使用生产者程序每0.1秒生成一个包含2个字母 的单词，并写入Kafka的名称为“wordcount-topic”的主题 （Topic）内。Spark的消费者程序通过订阅wordcount-topic， 会源源不断收到单词，并且每隔8秒钟对收到的单词进行一次词 频统计，把统计结果输出到Kafka的主题wordcount-result-topic 内，同时，通过2个监控程序检查Spark处理的输入和输出结果。 

  1. **启动Kafka**

     在Linux系统中新建一个终端（记作“Zookeeper终端”），输入下面命令启 动Zookeeper服务：

     ```sh
     $ cd /usr/local/kafka 
     $ bin/zookeeper-server-start.sh config/zookeeper.properties 
     ```

     不要关闭这个终端窗口，一旦关闭，Zookeeper服务就停止了。另外打开 第二个终端（记作“Kafka终端”），然后输入下面命令启动Kafka服务： 

     ```sh
     $ cd /usr/local/kafka 
     $ bin/kafka-server-start.sh config/server.properties
     ```

     不要关闭这个终端窗口，一旦关闭，Kafka服务就停止了。 再新开一个终端（记作“监控输入终端”），执行如下命令监控Kafka收到 的文本：

     ```sh
     $ cd /usr/local/kafka
     $ bin/kafka-console-consumer.sh \
     > --bootstrap-server localhost:9092 --topic wordcount-topic
     ```

     再新开一个终端（记作“监控输出终端”），执行如下命令监控输出的 结果文本：

     ```sh
     $ cd /usr/local/kafka
     $ bin/kafka-console-consumer.sh \
     > --bootstrap-server localhost:9092 --topic wordcount-result-topic
     
     ```

     

  2. **编写生产者（Producer）程序**

     代码文件spark_ss_kafka_producer.py内容如下：

     ```python
     #!/usr/bin/env python3
     import string
     import random
     import time
     from kafka import KafkaProduce
     if __name__ == "__main__":
         producer = KafkaProducer(bootstrap_servers=['localhost:9092'])
         while True:
             s2 = (random.choice(string.ascii_lowercase) for _ in range(2))
             word = ''.join(s2)
             value = bytearray(word, 'utf-8')
             producer.send('wordcount-topic', value=value) \
             .get(timeout=10)
             time.sleep(0.1)
     
     ```

     如果还没有安装Python3的Kafka支持，需要按照如下操作进行安装： 

     （1）首先确认有没有安装pip3，如果没有，使用如下命令安装：

     ```sh
     $ sudo apt-get install pip3
     ```

     （2）安装kafka-python模块，命令如下：

     ```sh
     $ sudo pip3 install kafka-python
     ```

     然后在终端中执行如下命令运行生产者程序：

     ```sh
     $ cd /usr/local/spark/mycode/structuredstreaming/kafka/
     $ python3 spark_ss_kafka_producer.py
     ```

     生产者程序执行以后，在“监控输入终端”的窗口内就可以看到持续 输出包含2个字母的单词

  3. **编写消费者（Consumer）程序**

     代码文件spark_ss_kafka_consumer.py内容如下：

     ```python
     #!/usr/bin/env python3
     from pyspark.sql import SparkSession
     if __name__ == "__main__":
        spark = SparkSession \
        .builder \
        .appName("StructuredKafkaWordCount") \
        .getOrCreate()
        spark.sparkContext.setLogLevel('WARN‘)
       lines = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", 'wordcount-topic') \
        .load() \
        .selectExpr("CAST(value AS STRING)")
        wordCounts = lines.groupBy("value").count()
       query = wordCounts \
        .selectExpr("CAST(value AS STRING) as key", 
       "CONCAT(CAST(value AS STRING), ':', CAST(count AS STRING)) as 
       value") \
        .writeStream \
        .outputMode("complete") \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("topic", "wordcount-result-topic") \
        .option("checkpointLocation", "file:///tmp/kafka-sink-cp") \
        .trigger(processingTime="8 seconds") \
        .start()
        query.awaitTermination()
     
     ```

     在终端中执行如下命令运行消费者程序：

     ```sh
     $ cd /usr/local/spark/mycode/structuredstreaming/kafka/
     $ /usr/local/spark/bin/spark-submit \
     > --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 \
     > spark_ss_kafka_consumer.py
     ```

     消费者程序运行起来以后，可以在“监控输出终端”看到类似如下的输 出结果：sq:3 bl:6 lo:8

### 3.3 socket 源

- Socket源一般仅用于测试或学习用途。

  ```
  Socket源从一个本地或远程主机的某个端口服务上读取数据，数据的编码为UTF8。因为Socket源使用内存保存读取到的所有数据，并且远端服务不能保证数据在出错后可以使用检查点或者指定当前已处理的偏移量来重放数据.
  ```

  

### 3.4 Rate源

- Rate源可每秒生成特定个数的数据行，每个数据行包括时间戳和值字 段。时间戳是消息发送的时间，值是从开始到当前消息发送的总个数， 从0开始。Rate源一般用来作为调试或性能基准测试。

- 案例:

  - 代码文件spark_ss_rate.py内容如下：

    ```python
    #!/usr/bin/env python3
    from pyspark.sql import SparkSession
    if __name__ == "__main__":
       spark = SparkSession \
       .builder \
       .appName("TestRateStreamSource") \
       .getOrCreate()
       spark.sparkContext.setLogLevel('WARN‘)
       lines = spark \
       .readStream \
       .format("rate") \
       .option('rowsPerSecond', 5) \
       .load()
       print(lines.schema)
       query = lines \
       .writeStream \
       .outputMode("update") \
       .format("console") \
       .option('truncate', 'false') \
       .start()
       query.awaitTermination()
    ```

  - 在Linux终端中执行如下命令执行spark_ss_rate.py：

    ```sh
    $ cd /usr/local/spark/mycode/structuredstreaming/rate/
    $ /usr/local/spark/bin/spark-submit spark_ss_rate.py
    ```

    

## 4. 输出操作

### 4.1 启动流计算

- DataFrame/Dataset 的 `.writeStream()` 方法将会返回 DataStreamWriter 接口, 接口通过 `.start()` 真正启动流计算, 并将 DataFrame/Dataset 写入到外部的输出接收器, DataStreamWriter 接口有以下几个主要函数

1. `format()`: 接收器类型.
2. **`outputMode`: 输出模式, 指定写入接收器的内容, 可以是 `Append` 模式, `Complete` 模式或 `Update` 模式.**
3. `queryName`: 查询的名称, 可选, 用于标识查询的唯一名称.
4. `trigger`: 触发间隔, 可选, 设定触发间隔, 如果未指定, 则系统将在上一次处理完成后立即检查新数据的可用性. 如果由于先前的处理尚未完成导致超过触发间隔, 则系统将在处理完成后立即触发新的查询.

### 4.2 输出模式

- 输出模式用于指定写入接收器的内容, 主要有以下几种:
  - **Append** 模式:只有结果表中自上次触发间隔后增加的新行, 才会被写入外部存储器. 这种模式一般适用于”不希望更改结果表中现有行的内容”的使用场景.
  - **Complete** 模式:已更新的完整的结果表可被写入外部存储器.
  - **Update** 模式:只有自上次触发间隔后结果表中发生更新的行, 才会被写入外部存储器. 这种模式与 Complete 模式相比, 输出较少, 如果结果表的部分行没有更新, 则不会输出任何内容. 当查询不包括聚合时, 这个模式等同于 Append 模式.

### 4.3 输出接收器

- 系统内置的输出接收器包括 File 接收器, **Kafka** 接收器, **Foreach** 接 收器, **Console** 接收器, **Memory** 接收器等, 其中, Console 接收器 和 Memory 接收器仅用于调试用途. 有些接收器由于无法保证输出 的持久性, 导致其不是容错的. 以 File 接收器为例, 这里把 7.2 节的实例修改为使用 File 接收器, 修改后的代码文件为 StructuredNetworkWordCountFileSink.py

```python
#!/usr/bin/env python3
from pyspark.sql import SparkSession
from pyspark.sql.functions import split
from pyspark.sql.functions import explode
from pyspark.sql.functions import length


if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("StructuredNetworkWordCountFileSink") \
        .getOrCreate()

    spark.sparkContext.setLogLevel('WARN')

    lines = spark \
        .readStream \
        .format("socket") \
        .option("host", "localhost") \
        .option("port", 9999) \
        .load()


    words = lines.select(
        explode(
            split(lines.value, " ")
            ).alias("word")
        )
    all_length_5_words = words.filter(length("word") == 5)
    query = all_length_5_words \
        .writeStream \
        .outputMode("append") \
        .format("parquet") \
        .option("path", "file:///tmp/filesink") \
        .option("checkpointLocation", "file:///tmp/file-sink-cp") \
        .trigger(processingTime="8 seconds") \
        .start()
    query.awaitTermination()
```

由于程序执行后不会在终端输出信息, 这时可新建一个终端, 执行如下命令查看 File 接收器保存的位置:

```shs
$ cd /tmp/filesink
$ ls
```

可以看到以 parquet 格式保存的类似如下的文件列表:

```
part-00000-2bd184d2-e9b0-4110-9018-a7f2d14602a9-c000.snappy.parquet
part-00000-36eed4ab-b8c4-4421-adc6-76560699f6f5-c000.snappy.parquet
part-00000-dde601ad-1b49-4b78-a658-865e54d28fb7-c000.snappy.parquet
part-00001-eedddae2-fb96-4ce9-9000-566456cd5e8e-c000.snappy.parquet
_spark_metadata
```

可以使用 `strings` 命令查看文件内的字符串, 具体如下:

```
$ strings part-00003-89584d0a-db83-467b-84d8-53d43baa4755-c000.snappy.parquet
```