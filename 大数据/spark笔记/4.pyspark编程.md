## 1.pyspark 简介

<img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210907232555486.png" alt="image-20210907232555486" style="zoom:33%;" />

### 环境依赖

- Python3
- spark3

---

- 配置环境变量或者配置saprk-env.sh 变量

  ```shell
  vi ~/.bashrc
  
  export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
  export SPARK_HOME=/usr/local/spark
  export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH
  export PATH=$SPARK_HOME/python:$PATH
  export PYSPARK_PYTHON=python3
  
  # 注意：py4j-0.10.7-src.zip要到$SPARK_HOME/python/lib目录查看是否是这个名称。不同版本的py4j的名称会有差别
  
  source ~/.bashrc
  ```

- 启动
  - `./bin/pyspark ` 相当于`./bin/pyspark --master local[*]`

### pyspark启动

- `pyspark --master local` 使用一个 Workers线程本地化运行 SPARK(完全不并行)
- **`pyspark --master local[*]`使用逻辑CPU个数数量的线程来本地化运行 Spark**
- `pyspark --master local[k]`使用K个 Worker线程本地化运行 Spark(理想情況下,K应该根据运行机器的CPU核数设定)
- `pyspark --master spark://HOST:PORT`连接到指定的 Spark standalone master。默认端口是7077
- **`pyspark --master yarn-client`以客户端模式连接YARN集群。集群的位置可以在 HADOOP_CONF_DIR环境变量中找到, 相当于`pyspark --master yarn`**
- **`pyspark --master yarn-cluster`以集群模式连接YARN集群。集群的位置可以在 HADOOP_CONF_DIR环境变量中找到, 线上使用**
- `pyspark --master mesos://HOST:PORT`连接到指定的 Mesos集群。默认接口是5050

![image-20210908003604315](https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210908003604315.png)

## 2.SparkContext

- **sc** 

  - **在pyspark交互式的命令行中,sc是一个常量, 已经创建好了**, 如果是在自己写的脚本, 需要手动创建
    - **sc**:在pyspark交互式的命令行中SparkContext常量
    - **spark**:在pyspark交互式的命令行中SparkContext常量
  - SparkContext是任何spark功能的入口点。当我们运行任何Spark应用程序时，启动一个驱动程序，它具有main函数，并在此处启动SparkContext。然后，驱动程序在工作节点上的执行程序内运行操作
  - SparkContext使用Py4J启动 **JVM** 并创建 **JavaSparkContext**。默认情况下，PySpark将SparkContext作为 **'sc'提供** ，因此创建新的SparkContext将不起作用。

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210908091135233.png" alt="image-20210908091135233" style="zoom:33%;" />

- 以下代码块包含PySpark类的详细信息以及SparkContext可以采用的参数。

```python
class pyspark.SparkContext (
   master = None,
   appName = None,
   sparkHome = None,
   pyFiles = None,
   environment = None,
   batchSize = 0,
   serializer = PickleSerializer(),
   conf = None,
   gateway = None,
   jsc = None,
   profiler_cls = <class 'pyspark.profiler.BasicProfiler'>
)
```

### 参数

以下是SparkContext的参数。

- **Master** - 它是连接到的集群的URL。
- **appName** - 您的工作名称。
- **sparkHome** - Spark安装目录。
- **pyFiles** - 要发送到集群并添加到PYTHONPATH的.zip或.py文件。
- **environment** - 工作节点环境变量。
- **batchSize** - 表示为单个Java对象的Python对象的数量。 设置1以禁用批处理，设置0以根据对象大小自动选择批处理大小，或设置为-1以使用无限批处理大小。
- **serializer** - RDD序列化器。
- **Conf** - L {SparkConf}的一个对象，用于设置所有Spark属性。
- **gateway** - 使用现有网关和JVM，否则初始化新JVM。
- **JSC** - JavaSparkContext实例。
- **profiler_cls** - 用于进行性能分析的一类自定义Profiler（默认为pyspark.profiler.BasicProfiler）。

在上述参数中，主要使用 **master** 和 **appname** 。***任何PySpark程序的前两行如下所示***

```python
from pyspark import SparkContext
sc = SparkContext("local", "First App")
```

### 示例

- PySpark Shell

  - 计算 **README.md** 文件中带有字符“a”或“b”的行 **数** 。那么，让我们说一个文件中有5行，3行有'a'字符，那么输出将是→ **Line with a：3** 。字符'b'也是如此。

    **注** -: 我们不会在以下示例中创建任何SparkContext对象，因为默认情况下，当PySpark shell启动时，Spark会自动创建名为sc的SparkContext对象。 如果您尝试创建另一个SparkContext对象，您将收到以下错误 **“ValueError：无法一次运行多个SparkContexts”。**

  ```scala
  <<< logFile = "file:///home/hadoop/spark-2.1.0-bin-hadoop2.7/README.md"
  <<< logData = sc.textFile(logFile).cache()
  <<< numAs = logData.filter(lambda s: 'a' in s).count()
  <<< numBs = logData.filter(lambda s: 'b' in s).count()
  <<< print "Lines with a: %i, lines with b: %i" % (numAs, numBs)
  Lines with a: 62, lines with b: 30
  ```

- Python程序

  ```python
  from pyspark import SparkContext
  logFile = "file:///home/hadoop/spark-2.1.0-bin-hadoop2.7/README.md"  
  sc = SparkContext("local", "first app")
  logData = sc.textFile(logFile).cache()
  numAs = logData.filter(lambda s: 'a' in s).count()
  numBs = logData.filter(lambda s: 'b' in s).count()
  print "Lines with a: %i, lines with b: %i" % (numAs, numBs)
  
  # Output: Lines with a: 62, lines with b: 30
  ```

## 3.pycharm 远程spark

- 教程: https://www.jianshu.com/p/06b40a77b6ee?utm_campaign=hugo&utm_medium=reader_share&utm_content=note&utm_source=weixin-friends

- 每一个py文件的环境配置:

  <img src="https://raw.githubusercontent.com/hellolib/pictures/main/Typora/pic-00-gitee/image-20210908121721724.png" alt="image-20210908121721724" style="zoom:33%;" />

  ```
  PYTHONPATH=/usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9-src.zip:/usr/local/spark
  SPARK_HOME=/usr/local/spark
  PYSPARK_PYTHON=/usr/local/python3/bin/python3
  ```

### 控制日志输出级别

- 终端修改

  在`pySpark`终端可使用下面命令来改变日志级别
  `sc.setLogLevel("WARN") # 或者INFO等`

- 修改日志设置文件

  **通过调整日志的级别来控制输出的信息量.减少`Spark Shell`使用过程中在终端显示的日志. **

  - 切换当前路径到Spark安装路径
  - 拷贝一份日志设置文件的模板文件
    `cp log4j.properties.template log4j.properties.template`
  - 找到下面一行内容
    `log4j.rootCategory=INFO, console`
    改为如下
    `log4j.rootCategory=WARN, console`