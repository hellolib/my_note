# 8-22.101 分词jieba|拼音pypiyin| 机器学习gensim

## 1.分词jieba模块

```python
import jieba

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式,建议使用,但是速度较慢
print(", ".join(seg_list))
```

## 2.转拼音pypiyin模块

- lazy_pinyin 
- TONE,TONE2,TONE3  转换模式

```python
from pypinyin import lazy_pinyin,TONE,TONE2,TONE3

a = "我要给圆圆发消息"
#换成字母
az = "woyaogeiyuanyuanfaxiaoxi"

b = "我要给元元发消息"
#换成字母
bz = "woyaogeiyuanyuanfaxiaoxi"

c = "我要给苑苑发消息"
cz = "woyaogeiyuanyuanfaxiaoxi"

resa = lazy_pinyin(a,style=TONE)
print(resa)  # ['wǒ', 'yào', 'gěi', 'yuán', 'yuán', 'fā', 'xiāo', 'xī']
resb = lazy_pinyin(b,style=TONE2)
print(resb)  #['wo3', 'ya4o', 'ge3i', 'yua2n', 'yua2n', 'fa1', 'xia1o', 'xi1']
resc = lazy_pinyin(c,style=TONE3)
print(resc)  #['wo3', 'yao4', 'gei3', 'yuan4', 'yuan4', 'fa1', 'xiao1', 'xi1']

print("".join(resc))  # 列表转化字符串
```

## 3.机器学习综合库gensim模块

- 使用步骤:

```python
# 1.模块导入
import jieba
import gensim
from gensim import corpora
from gensim import models
from gensim import similarities


# 2.制作问题库 

l1 = ["你的名字是什么", "你今年几岁了", "你有多高你胸多大", "你胸多大"]  # 问题库

# 3.对问题样本和问题库分词处理

a = "你今年多大了"  # 问题样本
all_doc_list = []
for doc in l1:
    doc_list = [word for word in jieba.cut(doc)]
    all_doc_list.append(doc_list)
doc_test_list = [word for word in jieba.cut(a)]
print(all_doc_list)
print(doc_test_list)


# 4.制作语料库

dictionary = corpora.Dictionary(all_doc_list)  # 制作词袋
# 词袋的理解
# 词袋就是将很多很多的词,进行排列形成一个 词(key) 与一个 标志位(value) 的字典
# 例如: {'什么': 0, '你': 1, '名字': 2, '是': 3, '的': 4, '了': 5, '今年': 6, '几岁': 7, '多': 8, '有': 9, '胸多大': 10, '高': 11}
# 至于它是做什么用的,带着问题往下看
print("token2id", dictionary.token2id)
print("dictionary", dictionary, type(dictionary))

# -->问题库的语料库
corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]
# 语料库:
# 这里是将all_doc_list 中的每一个列表中的词语 与 dictionary 中的Key进行匹配
# 得到一个匹配后的结果,例如['你', '今年', '几岁', '了']
# 就可以得到 [(1, 1), (5, 1), (6, 1), (7, 1)]
# 1代表的的是 你 1代表出现一次, 5代表的是 了  1代表出现了一次, 以此类推 6 = 今年 , 7 = 几岁
print("corpus", corpus, type(corpus))

# -->问题的语料库
# 将需要寻找相似度的分词列表 做成 语料库 doc_test_vec
doc_test_vec = dictionary.doc2bow(doc_test_list)
print("doc_test_vec", doc_test_vec, type(doc_test_vec))

# 5. 将corpus语料库(初识语料库) 使用Lsi模型进行训练

lsi = models.LsiModel(corpus)
# 模型有很多,这里的只是需要学习Lsi模型来了解的,这里不做阐述
print("lsi", lsi, type(lsi))
# 语料库corpus的训练结果
print("lsi[corpus]", lsi[corpus])
# 获得语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示
print("lsi[doc_test_vec]", lsi[doc_test_vec])

# 6. 获取文本相似度

# 稀疏矩阵相似度 将 主 语料库corpus的训练结果 作为初始值
index = similarities.SparseMatrixSimilarity(lsi[corpus], num_features=len(dictionary.keys()))
print("index", index, type(index))

# 将 语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 与 语料库corpus的 向量表示 做矩阵相似度计算
sim = index[lsi[doc_test_vec]]
print("sim", sim, type(sim))

# 7. 获取相似度最高的结果
# 对下标和相似度结果进行一个排序,拿出相似度最高的结果
# cc = sorted(enumerate(sim), key=lambda item: item[1],reverse=True)
cc = sorted(enumerate(sim), key=lambda item: -item[1])
print(cc)

text = l1[cc[0][0]]
print(a,text)

```

