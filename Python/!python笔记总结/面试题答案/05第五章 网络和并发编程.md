[TOC]

#### 1、python的底层网络交互模块有哪些？

```python
socket, socketserver()
urllib,urllib2 , requests
socket：我们经常把socket翻译为套接字，网络协议的大接口,帮助我们完成网络传输过程中的osi4层以及以下信息的封装
承包了下面四层复杂的数据包的封装
```

```python
Python中，系统自带的urllib和urllib2都提供了功能强大的HTTP支持，但是API接口确实太难用了。requests作为更高一层的封装，确实在大部分情况下对得起它的slogan——HTTP for Humans。 （为人类写的HTTP）

简要了解API
	API，英文全称Application Programming Interface，翻译为“应用程序编程接口”。是一些预先定义的函数，目的是提供应用程序与开发人员基于某软件或硬件得以访问一组例程的能力，而又无需访问源码，或理解内部工作机制的细节。——百度百科
    我们不妨把API的诞生过程用一个小故事展示出来：研发人员A开发了软件A，研发人员B正在研发软件B。有一天，研发人员B想要调用软件A的部分功能来用，但是他又不想从头看一遍软件A的源码和功能实现过程，怎么办呢？研发人员A想了一个好主意：我把软件A里你需要的功能打包好，写成一个函数；你按照我说的流程，把这个函数放在软件B里，就能直接用我的功能了！其中，API就是研发人员A说的那个函数。  	
```

```python
 
    urllib,urllib2 是python标准库，都是通过URL打开资源，
	其中urllib短板太大，例如它只能接受URL，却无法对请求进行headers的伪装，这样写的爬虫发送的请求会被很多网站直接阻挡掉。
    因此python基金会不得不推出urllib2对这个功能进行增强，urllib2可以接受一个Request对象，并以此设置一个URL的headers。
    但是问题在于urllib2并没有完全取代urllib的所有功能，比如urllib中的urlencode()【urlencode是一个函数，可将字符串以URL编码，用于编码处理。】方法，在urllib2中就是没有的。所以urllib和urllib2不得不一起使用。

    requests是第三方库，Requests 使用的是 urllib3，继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。
    
    补充：Python软件基金会是一个致力于Python编程语言的非营利组织，成立于2001年3月6日。基金会的任务在于促进Python使用社群的发展，并负责Python社群中的各项工作，包括开发Python核心版本、管理智慧财产权、开发者研讨会（包含PhyCon），以及募款。
    
    详细了解可以到爬虫阶段进行深层次的了解。。。。。。。
```

#### 2、简述OSI网络七层协议?

```python
OSI是一个开放性的通信系统互联参考模型，它是一个定义的非常好的协议规范。互联网的本质就是一系列的网络协议，这个协议就视为OSI协议。人们按照功能不同，分工不同分为了七层。
会话层
表示层
应用层      http/https/ftp协议
传输层      tcp/udp协议，端口    四层交换机，四层路由器
网络层      IP地址，             设备：三层路由器，三层交换器（带路由功能的交换机）
数据链路层   Mac与ARP协议 ，     设备：二层交换机，网卡
物理层                          设备：双绞线，电缆，光缆等
```

#### 3、什么是C/S和B/S架构？

```python
B/S  浏览器/服务器模式
C/S  客户/服务器模式

一、B/S模式的优点和缺点
B/S结构的优点
（1）、具有分布性特点，可以随时随地进行查询、浏览等业务处理。 
（2）、业务扩展简单方便，通过增加网页即可增加服务器功能。 
（3）、维护简单方便，只需要改变网页，即可实现所有用户的同步更新。 
（4）、开发简单，共享性强
B/S 模式的缺点
（1）、个性化特点明显降低，无法实现具有个性化的功能要求。 
（2）、操作是以鼠标为最基本的操作方式，无法满足快速操作的要求。 
（3）、页面动态刷新，响应速度明显降低。 
（4）、无法实现分页显示，给数据库访问造成较大的压力。 
（5）、功能弱化，难以实现传统模式下的特殊功能要求。

二、C/S 模式的优点和缺点 
C/S 模式的优点 
1.由于客户端实现与服务器的直接相连，没有中间环节，因此响应速度快。 
2.操作界面漂亮、形式多样，可以充分满足客户自身的个性化要求。 
3.C/S结构的管理信息系统具有较强的事务处理能力，能实现复杂的业务流程。 
C/S 模式的缺点 
1.需要专门的客户端安装程序，分布功能弱，针对点多面广且不具备网络条件的用户群体，不能够实现快速部署安装和配置。 
2.兼容性差，对于不同的开发工具，具有较大的局限性。若采用不同工具，需要重新改写程序。 
3.开发成本较高，需要具有一定专业水准的技术人员才能完成。
```

#### 4、简述TCP三次握手、四次挥手的流程？

```pyhton
三次握手：
在TCP/IP协议中，TCP协议提供可靠的连接服务，采用三次握手建立一个连接。
第1次握手：建立连接时，客户端A发送 一个TCP的SYN（同步序列编号（Synchronize Sequence Numbers）包（标志位置 SYN=j），并进入SYN_SEND状态，等待服务器B确认。
第2次握手：服务器B收到SYN包，必须确认客户A的SYN（ACK=j+1），同时自己也发送一个SYN包（SYN=k），即SYN+ACK，此时服务器B进入SYN_RECV状态。
第3次握手：客户端A收到服务器B的SYN+ACK包，向服务器B发送确认包ACK（ACK=k+1），此包发送完毕，客户端A和服务器B进入ESTABLISHED状态，完成三次握手。
完成三次握手，客户端与服务器开始传送数据。

四次挥手：
TCP的连接的拆除需要发送四个包，因此称为四次挥手（four-way handshake）。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。
    由于TCP连接是全双工（又称为双向同时通信，即通信的双方可以同时发送和接收信息的信息交互方式。）的，因此每个方向都必须单独进行关闭。这个原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的链接。收到一个FIN只意味着这方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。
 （1）客户端A发送一个FIN，用来关闭客户端A到服务器B的数据传送
（2）服务器B收到这个FIN，它发回一个ACK，确认收到的序号加1，和SYN一样，一个FIN将占用一个序号。
（3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A。
（4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1

为什么建立连接协议是三次握手，而关闭连接却是四次挥手呢？
	这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的连接请求后，它可以把ACK和SYN(ACK起应答作用，而SYN起同步作用)放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可能未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。
	
报文：是网络中交换与传输的数据单元，即站点一次性要发送的数据块。包含了将要发送的完整的数据信息，其长短很不一致，长度不限且可变。
```

#### 5、什么是arp协议？

```pyhton
1.这是一个通过ip找mac地址的协议 
2.由于有了socket,用户在使用网络的时候,只需要关心对方用户的ip地址就可以了
3.如果用户即将和这个ip进行通信,那么还需要知道它的mac地址
4.这个时候就需要由你的机器发起一个arp请求
5.由交换机进行广播
6.对应的机器会回应这个arp请求
7.通过交换机单播发给你的机器
```

#### 6、TCP和UDP的区别？为何基于tcp协议的通信比基于udp协议的通信更可靠？

```python
TCP：面向连接的可靠的流式传输 适合传输比较大的文件,对稳定性要求比较高的
UDP：无连接的 快速 但不可靠 适合传输对效率要求比较高的短消息

可靠地原因：TCP的可靠保证，是它的三次握手双向机制，这一机制保证校验了数据，保证了他的可靠性。
UDP就没有了，udp信息发出后,不验证是否到达对方,所以不可靠。
```

#### 7、什么是局域网和广域网？

```python
通俗的说，广域网就是外网，局域网是内网。
一、局域网
局域网（Local Area Network），简称LAN，是指在某一区域内由多台计算机互联成的计算机组。“某一区域”指的是同一办公室、同一建筑物、同一公司和同一学校等，一般是方圆几千米以内。局域网可以实现文件管理、应用软件共享、打印机共享、扫描仪共享、工作组内的日程安排、电子邮件和传真通信服务等功能。局域网是封闭型的，可以由办公室内的两台计算机组成，也可以由一个公司内的上千台计算机组成。
二、广域网
广域网（Wide Area Network），简称WAN，是一种跨越大的、地域性的计算机网络的集合。通常跨越省、市，甚至一个国家。广域网包括大大小小不同的子网，子网可以是局域网，也可以是小型的广域网。
```

#### 8、什么是socket？简述基于tcp协议的套接字通信流程。

```python
	Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部。
     （socket服务端）
        1. 启动socket    sk = socket.socket()
        2. 绑定ip和端口  sk.bind(('127.0.0.1',9000))
        3. 监听   sk.listen()
        6. 接收数据  sk.accept()
        7. 发送数据 conn.send(1024)
        9. 断开连接 conn.close()
					sk.close()
        
   （socket客户端）
        4. 连接  sk = socket.socket(),sk.connect(('127.0.0.1',9000))
        5. 发送数据 sk.send(1024)
        8. 接收数据 sk.recv(1024)
        9. 断开连接  sk.close()
           
```

#### 9、什么是黏包？socked中造成黏包的原因是什么？哪些情况会发生黏包现象？

```python
黏包：指数据混乱问题(发送端发送数据,接收端不知如何去接收)，只有tcp协议才会发送粘包,udp(以一条一条进行传输的）不会发生，
有一个合包机制(nagle算法),将多次连续发送且间隔较小的数据,进行打包成一块数据传送. 还有一个机制是拆包机制,在发送端,因为受到网卡的MTU限制（网络上传送的最大数据包。大部分网络设备的MTU都是1500字节）,会将大的超过MTU限制的数据,进行拆分,拆分成多个小的数据,进行传输.  当传输到目标主机的操作系统层时,会重新将多个小的数据合并成原本的数据

原因：
	合包机制nagle
    拆包机制
    TCP的流式传输无边界
    
黏包现象怎么产生的：
	发送端：
    首先,我们发出的信息不是立即通过网络传送到另一端
    而是我们发到操作系统的缓存中,
    tcp协议首先流式传输无边界,第二是可靠所以每一条数据都有回执
    那么为了节省网络上延迟的时间
    连续发送出的多个短信息就会黏在一起
    由发送端的缓存发送除去,所以接收到的就是黏在一起的数据了
	接收端：
    发送端发送的消息也不是直接发送到对面的应用中
    而是发到了对方操作系统的缓存中
    如果连续发送的数据在对方的缓存中没有被及时取走
   	那么也会发生黏包现象
   
```

#### 10.、IO多路复用的作用？

```pyhton
IO多路复用：操作系统提供的，对于程序来说实际上是一个代理，帮助监听所有的通信对象，是否有数据来到操作系统中，一旦有就通知你，你在根据通知来接收相应的数据，你不需要一直循环着问每一个对象是否有信息来，而是阻塞等待，任意一个对象有信息来，我就接收，本质就是对网络操作的监听机制

实现并发 不让cpu太忙


作用：代理功能
	通过一种机制，可以监视多个文件描述符，一旦描述符就绪（读就绪和写就绪），能通知程序进行相应的读写操作，I/O多路复用避免阻塞在io上，原本为多进程或多线程来接收多个连接的消息变为单进程或单线程保存多个socket的状态后轮询处理
```

#### 11、什么是防火墙以及作用？

```python
防火墙是位于内部网和外部网之间的屏障，它按照系统管理员预先定义好的规则来控制数据包的进出。防火墙是系统的第一道防线，其作用是防止非法用户的进入。
```

#### 12、select、poll、epoll模型的区别？

```python
select：把一对SK.CONN对象交给操作系统处理,是在所有操作系统都有的,操作系统采用轮询的方式（标识符问来的数据是谁的）,随着监听的对象越来越多，效率会越来越差，轮询的时间会延迟,能够处理的监听个数是有限的，超过一定的个数（1024个连接），就不能进行监听了

poll:也是轮询的方式，但是底层的数据结构优化了
    
epoll:Windows上没有，Linux上有,不用轮询,采用回调函数的机制，一旦数据来了，立即去调用一个函数让这个函数去通知是哪个对象的数据来了
```

#### 13、简述进程、线程、协程的区别以及应用场景？

```pyhton
进程：
1.每一个程序运行起来都至少是一个进程
2.进程是被操作系统调度的，有很多相关的算法
3.进程是计算机中最小的资源分配单位
4.进程之间是数据隔离的,通过管道调用队列进行数据传输S

线程：
1.轻量级的进程
2.线程创建出来为了解决并发问题的
3.线程是计算机中被CPU调度的最小单位
4.线程之间是数据/资源共享的
5.线程的开启和关闭时间/切换快
6.线程不能独立存在，必须依赖于进程存在


协程：
1.必须在只有一个单线程里实现并发
2.修改共享数据不需要加锁
3.一个协程遇到IO操作自动切换到其他协程
4.协程（纤程，轻型线程），对于操作系统来说协程是不可见的，不需要操作系统调度
5.协程是程序级别的操作单位

```

#### 14、什么是GIL锁？

```python
全局解释器锁（GIL)，这个锁是锁线程的
线程锁这件事情是由cpython解释器完成的，对于python来说，同一时刻只能有一个线程被cpu访问，彻底的解决了多核环境下的安全问题

Cpython解释器的下层没有用到并发

出现线程不安全的条件：
1.出现+=/-=这样的操作（程序中所有的+=/-=都是不安全的，因为这样的加减在python中可以拆分成两部）
2.全局变量
```

#### 15、python中如何使用线程池和进程池？

```python
from threading import currentThread
from concurrent.futures import ThreadPoolExecutor as Pool
# from concurrent.futures import ProcessPoolExecutor as Pool
concurrent.futures模块提供了高度封装的异步调用接口
ThreadPoolExecutor：线程池，提供异步调用
ProcessPoolExecutor: 进程池，提供异步调用

    #开启线程
def func(num):
     print('in %s func'%num,currentThread())
     time.sleep(random.random())
     return num**2

 tp = ThreadPoolExecutor(5)       #开启5个线程池
 ret_l = []
 for i in range(30):
     ret = tp.submit(func,i)      #submit 异步提交任务
     ret_l.append(ret)
 for ret in ret_l:
     print(ret.result())           #result() 取得结果
  
 #开启进程、线程
 import os
 def func(num):
     # print('in %s func'%num,currentThread())   #currentThread() 主线程
     print('in %s func'%num,os.getpid())    #进程ID
     time.sleep(random.random())
     return num**2
 if __name__ == '__main__':
     # tp = ThreadPoolExecutor(5)    
     tp = Pool(5)                     
     ret_l = []
     for i in range(30):
         ret = tp.submit(func,i)
         ret_l.append(ret)
     tp.shutdown()     # close + join
     for ret in ret_l:
         print(ret.result())


#进程池：
import time
from multiprocessing import Pool  # 池
def func(i):
    i -= 1
    time.sleep(1)
    return i**2
if __name__ == '__main__':
    p = Pool(5)
    l = []
    for i in range(10):
        ret = p.apply_async(func,args=(i,))  # 自动带join  异步的  apply_async异步提交任务
        l.append(ret)
    for ret in l:
        print(ret.get())
        
               
from multiprocessing import Pool  # 池
def func(i):
    i -= 1
    return i**2

#　你的池中打算放多少个进程,个数cpu的个数 * 1|2
if __name__ == '__main__':
    p = Pool(5)
    ret = p.map(func,range(100))  # 自动带join
    print(ret)
```

#### 16、threading.local的作用？

```pyhton
为每个线程创建一个独立的空间，使得线程对自己的空间中的数据进行操作（数据隔离）。local 这是一个类
线程之间数据隔离使用


import time
from threading import Thread,local
class Foo(local):
    num = 0
foo = Foo()

def add_num(i):
    foo.num = i
    time.sleep(1)
    print(i,foo.num)

for i in range(10):
    task = Thread(target=add_num,args=(i,))
    task.start()
    答案：0 0
1 1
4 4
3 3
5 5
6 6
2 2
8 8
7 7
9 9
```

#### 17、进程之间如何进行通信？？？？

```python
进程间通信  IPC通信 
基本上是基于网络通信的
用redis实现

两种机制：
管道：
pipe，基于socket实现的
管道是数据不安全的

队列：
queue，基于管道实现的
数据安全的模型，管道+锁
put防值，get取值


生产者消费者模型：
import time
from multiprocessing import Queue,Process

def producer(name,food,num,q):
    '''生产者'''
    for i in range(num):
        time.sleep(0.3)
        foodi = food + str(i)
        print('%s生产了%s'%(name,foodi))
        q.put(foodi)

def consumer(name,q):
    while True:
        food = q.get()   # 等待接收数据
        if food == None:break
        print('%s吃了%s'%(name,food))
        time.sleep(1)

if __name__ == '__main__':
    q = Queue(maxsize=10)  
    p1 = Process(target=producer,args = ('宝元','泔水',20,q))
    p2 = Process(target=producer,args = ('战山','鱼刺',10,q))
    c1 = Process(target=consumer, args=('alex', q))
    c2 = Process(target=consumer, args=('wusir', q))
    p1.start()   # 开始生产
    p2.start()   # 开始生产
    c1.start()
    c2.start()
    p1.join()    # 生产者结束生产了
    p2.join()    # 生产者结束生产了
    q.put(None)  # put None 操作永远放在所有的生产者结束生产之后
    q.put(None)  # 有几个消费者 就put多少个None
```

#### 18、什么是并发和并行？

```pyhton
并行 : 并行是指两者同时执行，比如赛跑，两个人都在不停的往前跑；（资源够用，比如三个线程，四核的CPU ）
资源有限的情况下，两者交替轮流使用资源
从宏观上，在一个时间段上可以看出是同时执行的，比如一个服务器同时处理多个任务

并发 : 并发是指资源有限的情况下，两者交替轮流使用资源，比如一段路(单核CPU资源)同时只能过一个人，A走一段后，让给B，B用完继续给A ，交替使用，目的是提高效率。


并行是指两者同时执行
从微观上，也就是一个精确的时间片刻，有不同的程序在执行，这就要求必须有多个处理器
区别:
并行是从微观上，也就是在一个精确的时间片刻，有不同的程序在执行，这就要求必须有多个处理器。
并发是从宏观上，在一个时间段上可以看出是同时执行的，比如一个服务器同时处理多个session。
```

#### 19、解释什么是异步非阻塞？

```pyhton
有几件事情，同时完成，并且所有程序不阻塞，
网络通信有阻塞和非阻塞之分，例如对于接收数据的函数recv：在阻塞方式下，没有数据到达时，即接收不到数据时，程序会停在recv函数这里等待数据的到来；而在非阻塞方式下就不会等，如果没有数据可接收就立即返回-1表示接收失败。

同步与异步是对应的，它们是线程之间的关系，两个线程之间要么是同步的，要么是异步的。
阻塞与非阻塞是对同一个线程来说的，在某个时刻，线程要么处于阻塞，要么处于非阻塞。
```

#### 20、路由器和交换机的区别？

```python
一、
	路由器可以给你的局域网自动分配IP，虚拟拨号，就像一个交通警察，指挥着你的电脑该往哪走，你自己不用操心那么多了。交换机只是用来分配网络数据的。  
二、
　　路由器在网络层，路由器根据IP地址寻址，路由器可以处理TCP/IP协议，交换机不可以。
　　交换机在中继层（数据链路层），交换机根据MAC地址寻址。
三、
　　路由器可以把一个IP分配给很多个主机使用，这些主机对外只表现出一个IP。交换机可以把很多主机连起来，这些主机对外各有各的IP。    
四、
	路由器可以提供防火墙，交换机不能提供该功能。集线器、交换机都是做端口扩展的，就是扩大局域网(通常都是以太网)的接入点，也就是能让局域网可以连进来更多的电脑。路由器是用来做网间连接，也就是用来连接不同的网络。    
五、
	举个例子：路由器相当于邮局，把信投递到收件人地址，它的任务就完成了。但是信邮到了你们宿舍楼，而这个地址不是你一个人专享的，所以楼管王大爷还要负责把信给到你手里，他不会关心收件人地址，只看收件人姓名，然后打个内线电话叫你来取信。
　　如果没有邮局，你没法向世界各地的漂亮妹子们发信，也没法从楼外的漂亮妹子那里收信。但是因为楼管王大爷的存在，你仍然可以通过他与同宿舍楼的好基友书信往来。
　　所有邮局构成的系统，就是“广域网”，而你的宿舍楼，就是“局域网”，构建局域网是不需要路由器的。
```

#### 20.路由器和交换机的区别?

```python
路由器:负责巡经的网络设备,在互联网中从众多条路径中寻找通讯最少的一条网络路径提供给用户。
交换机：基于mac地址识别，能够完成封装转发数据包功能的网络设备。以太网端口的交换机完全可以实现全双工方式工作。
不同点：
1）工作层次最初是不同的。开始时候，交换机就在数据链路层OSI的第二层,而路由器,在OSI的第三层网络层.
2）数据转发的依据对象不同。交换机是利用物理mac地址确定转发数据的目的地址，而路由器则是利用不同的网络IP地址来确定转发数据的目的地址。
3）传统的交换机只能分割冲突域，不能分割广播域。而路由器可以分割广播域。由于交换机连接所有的广播域，在某些情况下会导致通信的拥堵和安全的漏洞。路由器上的网段被分配成不同的广播域，广播域数据不会穿过路由器。三层交换机已经有这样的功能.
4）路由器提供防火墙服务。仅仅转发特定地址的数据包。
```

#### 21.什么是域名解析？

```python
域名解析是吧域名指向网站空间IP。也叫域名指向。用户通过ISP接入网络,就会被分配到一个DNS的服务器，通常是一个代理服务器，或者是自己电脑网络设置的。然后被DNS进行解析出一个IP。
```

#### 22.如何修改本地的hosts？

```python
hosts文件是一个没有扩展名的系统文件。它的主要作用就是加快域名解析，还可以屏蔽网站等。实际上就是定义了IP地址和主机名之间的映射关系。当用户在浏览器中输入一个需要登录的网址时，系统会首先自动从Hosts文件中寻找对应的IP地址，一旦找到，浏览器会立即打开对应网页，如果没有找到，则浏览器会将网址提交DNS服务器进行IP地址解析。这也是提高快速打开网页的方法！
文件位置：c:\windows\system32\drivers\etc，注意这个文件一定是在系统盘
```

#### 23.生产者消费者模型应用场景？

```python
应用:转码,日志记录
矛盾：生产数据的线程速度>消费数据的线程速度 生产者就必须等消费者
  生产数据的线程速度<消费数据的线程速度 消费者必须等生产者
作用：使用生产者消费者模型可以解决大多数的并发问题。
方法：平衡生产线程和消费线程的工作能力来提高整体的处理效率
方式一:用队列,生产消费之间都不直接通讯.通过阻塞队列缓存
    
    
    爬取数据（生产者） 分析数据（消费者）
```

```python
from multiprocessing import Process,Queueimport time,random,osdef consumer(q):
    while True:
        res=q.get()
        time.sleep(random.randint(1,3))
        print('\033[45m%s 吃 %s\033[0m' %(os.getpid(),res))
def producer(q):
    for i in range(10):
        time.sleep(random.randint(1,3))
        res='包子%s' %i
        q.put(res)
        print('\033[44m%s 生产了 %s\033[0m' %(os.getpid(),res))
if __name__ == '__main__':
    q=Queue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=(q,))

    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))

    #开始    p1.start()
    c1.start()
print('主')
问题:这里的消费者一直在跑一个死循环
```

```python
方式二:
升级:生产完成后,发出一个结束信号,终止掉消费者
from multiprocessing import Process,Queueimport time,random,osdef consumer(q):
    while True:
        res=q.get()
        if res is None:break   #收到结束信号则结束
        time.sleep(random.randint(1,3))
        print('\033[45m%s 吃 %s\033[0m' %(os.getpid(),res))
def producer(q):
    for i in range(10):
        time.sleep(random.randint(1,3))
        res='包子%s' %i
        q.put(res)
        print('\033[44m%s 生产了 %s\033[0m' %(os.getpid(),res))
    q.put(None) #发送结束信号if __name__ == '__main__':
    q=Queue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=(q,))
    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))

    #开始    p1.start()
    c1.start()
	print('主')
```

#### 24.什么是cdn?

```python
cdn的全称是Content Delivery Network，即内容分发网络。其基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输得更快、更稳定。
谈到CDN的作用，可以用8年买火车票的经历来形象比喻：
8年前，还没有火车票代售点一说，12306.cn更是无从说起。那时候火车票还只能在火车站的售票大厅购买，而我所住的小县城并不通火车，火车票都要去市里的火车站购买，而从县城到市里，来回就是4个小时车程，简直就是浪费生命。后来就好了，小县城里出现了火车票代售点，可以直接在代售点购买火车，方便了不少，全市人民再也不用在一个点苦逼的排队买票了。
CDN就可以理解为分布在每个县城的火车票代售点，用户在浏览网站的时候，CDN会选择一个离用户最近的CDN边缘节点来响应用户的请求，这样海南移动用户的请求就不会千里迢迢跑到北京电信机房的服务器（假设源站部署在北京电信机房）上了。
	CDN的优势很明显：
（1）CDN节点解决了跨运营商和跨地域访问的问题，访问延时大大降低；
（2）大部分请求在CDN边缘节点完成，CDN起到了分流作用，减轻了源站的负载。
```

#### 25.从flag A 执行到flag B 的时间大致是多少秒？

```python
import threading
import time

def _wait():
  time.sleep(60)
#flag a
start = time.time()
t = threading.Tread(target=_wait(),daemon = False)
t.start()
#flag b
endtime = time.time()
print(endtime - start)
```

#### 26.有A.txt 和B.txt 两个文件,使用多进程和进程池的方式分别读取者两个文件.

```python
进程池:
from multiprocessing import Pool

def func(file):
    with open(file,’r’,encoding=’utf8’)as f:
    text = f.read()
    return text
if __name__ ==’__mian__’:
    P = Pool(5)
    for i in [‘a’,’b’]:
    ret = p.apply(func,args = (i,))
    print(ret) 
```

```python
多进程:
from multiprocessing import Process

def func(file):
    with open(file,’r’,encoding =’utf-8’)as f:
    text = f.read()
    print(text)
if __name__ ==’__mian__’:
    P1 = process(target =func,args = (‘A’,))
    P2 = process(target =func,args = (‘B’,))
    P1.start()
    P2.start()
```

#### 27.以下那些是常见的tcp flags?

```python
.以下那些是常见的tcp flags?
    A. Syn
    B. Rst
    C. Ack
    D. Urg
答案:不常见urg  ABCD
SYN表示建立连接，
FIN表示关闭连接，
ACK表示响应，
PSH表示有 DATA数据传输，
RST表示连接重置。
https://blog.csdn.net/mary19920410/article/details/58030147
1）URG：紧急指针标志，为1时表示紧急指针有效，为0则忽略紧急指针。
2）ACK：确认序号标志，为1时表示确认号有效，为0表示报文中不含确认信息，忽略确认号字段。
3）PSH：push标志，为1表示是带有push标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队。
4）RST：重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者用于拒绝非法的报文段和拒绝连接请求。
5）SYN：同步序号，用于建立连接过程，在连接请求中，SYN=1和ACK=0表示该数据段没有使用捎带的确认域，而连接应答捎带一个确认，即SYN=1和ACK=1。
6）FIN：finish标志，用于释放连接，为1时表示发送方已经没有数据发送了，即关闭本方数据流。
```

#### 28.下面关于网络七层和四层的描述,哪条是错误的?

```python
下面关于网络七层和四层的描述,哪条是错误的?
A.SNMP工作在四层
B.四层是网络的传输层,主要包括IP和端口信息
C.七层是网络应用层(协议层),比如http协议就在第七层
D.四层主要应用于TCP和UDP的代理，七层主要应用于http协议的代理
答案：A，B 都错了....
SNMP是英文"Simple Network Management Protocol"的缩写，中文意思是"简单网络管理协议"。SNMP是一种简单网络管理协议，它属于TCP/IP五层协议中的应用层协议
http://www.cnblogs.com/xdp-gacl/p/3978825.html
```

#### 29.tracerroute一般使用的是那种网络层协议?

```python
tracerroute一般使用的是那种网络层协议?
 	A.Vrrp
    B.Udp
    C.Arp
    D.Icmp
选择的是D,这里是linux的命令,在windows中是tracert,和ping命令是一样的.但是可以查看出中间越过几个节点.

```

#### 30.iptable知识点考察,根据要求写出防火墙的规则?

```python
A.屏蔽192.168.1.5访问本机dns服务端口
    iptable方法参考:   https://www.cnblogs.com/ilinuxer/p/6364064.html
    iptable -A INPUT -p udp -m udp -s 192.168.1.1 --dport 53 -j REJECT
    B.允许10.1.1.0/24访问本机的udp 8888 9999 端口
    Iptable  -A  INPUT  -p udp -m udp  -s 192.168.1.0/24 -m multiport --destination -ports 8888, 9999 -j ACCEPT

```

#### 31.业务服务器192.168.1.2访问192.168.1.3数据接口，无法正常返回数据，请根据以上信息写出排查思路

```
1.ping 保证网络联通
2.用telnet -ip  - p  查看端口是否连接通
3.查看防火墙规则屏蔽.
4.服务是否正常启动
5.重启

```

#### 32.实现一个简单的socket编程

```python
Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部。
     （socket服务端）
        1. 启动socket    sk = socket.socket()
        2. 绑定ip和端口  sk.bind(('127.0.0.1',9000))
        3. 监听   sk.listen()
        6. 接收数据  sk.accept()
        7. 发送数据 conn.send(1024)
        9. 断开连接 conn.close()
		sk.close()
        
   （socket客户端）
        4. 连接  sk = socket.socket(),sk.connect(('127.0.0.1',9000))
        5. 发送数据 sk.send(1024)
        8. 接收数据 sk.recv(1024)
        9. 断开连接  sk.close()
```

#### 33.谈一下对于多线程编程的理解，对于CPU密集型怎样使用多线程，说说线程池，线程锁的用法，有没有用过multiprocessing,或者concurrent.future?

```python
多线程编程 避开io 但是CPYTHON解释器不允许多线程

使用多线程的优势:
1.开启进程的开销变短
2.操作系统调度的时间变短
3.操作系统减负
线程是否越多越好？ 分析如下：
一个计算为主的程序（专业一点称为CPU密集型程序）。多线程跑的时候，可以充分利用起所有的cpu核心，比如说4个核心的cpu,开4个线程的时候，可以同时跑4个线程的运算任务，此时是最大效率。

但是如果线程远远超出cpu核心数量 反而会使得任务效率下降，因为频繁的切换线程也是要消耗时间的。
因此对于cpu密集型的任务来说，线程数等于cpu数是最好的了。
https://blog.csdn.net/qq_20009015/article/details/84141790
concurrent.futures模块提供了高度封装的异步调用接口
ThreadPoolExecutor：线程池，提供异步调用
ProcessPoolExecutor: 进程池，提供异步调用
https://www.cnblogs.com/Bottle-cap/articles/10357527.html

concurrent.futures模块的基础是Exectuor，Executor是一个抽象类，它不能被直接使用。但是它提供的两个子类ThreadPoolExecutor和ProcessPoolExecutor却是非常有用，顾名思义两者分别被用来创建线程池和进程池的代码。我们可以将相应的tasks直接放入线程池/进程池，不需要维护Queue来操心死锁的问题，线程池/进程池会自动帮我们调度。
```

```python
进程池
from concurrent.futures import ProcessPoolExecutor
import os,time,random
def task(n):
    print('%s is running' %os.getpid())
    time.sleep(2)
    return n**2


if __name__ == '__main__':
    p=ProcessPoolExecutor()  #不填则默认为cpu的个数
    l=[]
    start=time.time()
    for i in range(10):
        obj=p.submit(task,i)   #submit()方法返回的是一个future实例，要得到结果需要用obj.result()
        l.append(obj)

    p.shutdown()  #类似用from multiprocessing import Pool实现进程池中的close及join一起的作用
    print('='*30)
    # print([obj for obj in l])
    print([obj.result() for obj in l])
    print(time.time()-start)

    #上面方法也可写成下面的方法
    # start = time.time()
    # with ProcessPoolExecutor() as p:   #类似打开文件,可省去.shutdown()
    #     future_tasks = [p.submit(task, i) for i in range(10)]
    # print('=' * 30)
    # print([obj.result() for obj in future_tasks])
    # print(time.time() - start)
```

```
线程池
from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
import threading
import os,time,random
def task(n):
    print('%s:%s is running' %(threading.currentThread().getName(),os.getpid()))
    time.sleep(2)
    return n**2

if __name__ == '__main__':
    p=ThreadPoolExecutor()   #不填则默认为cpu的个数*5
    l=[]
    start=time.time()
    for i in range(10):
        obj=p.submit(task,i)
        l.append(obj)
    p.shutdown()
    print('='*30)
    print([obj.result() for obj in l])
    print(time.time()-start)

#上面方法也可写成下面的方法
    # start = time.time()
    # with ThreadPoolExecutor() as p:   #类似打开文件,可省去.shutdown()
    #     future_tasks = [p.submit(task, i) for i in range(10)]
    # print('=' * 30)
    # print([obj.result() for obj in future_tasks])
    # print(time.time() - start)
```

```
回调函数
from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
import requests
import os
import time
from threading import currentThread
def get_page(url):
    print('%s:<%s> is getting [%s]' %(currentThread().getName(),os.getpid(),url))
    response=requests.get(url)
    time.sleep(2)
    return {'url':url,'text':response.text}
def parse_page(res):  #此处的res是一个p.submit获得的一个future对象，不是结果
    res=res.result()  #res.result()拿到的才是对应的结果
    print('%s:<%s> parse [%s]' %(currentThread().getName(),os.getpid(),res['url']))
    with open('db.txt','a') as f:
        parse_res='url:%s size:%s\n' %(res['url'],len(res['text']))
        f.write(parse_res)
if __name__ == '__main__':
    # p=ProcessPoolExecutor()
    p=ThreadPoolExecutor()
    urls = [
        'https://www.baidu.com',
        'https://www.baidu.com',
        'https://www.baidu.com',
        'https://www.baidu.com',
        'https://www.baidu.com',
        'https://www.baidu.com',
    ]

    for url in urls:
        # multiprocessing.pool_obj.apply_async(get_page,args=(url,),callback=parse_page)
        p.submit(get_page, url).add_done_callback(parse_page) #与之前的回调函数拿到的结果不同，这里拿到的是前面submit方法执行完后返回的对象，要.result才能拿到对应的结果
    p.shutdown()
    print('主',os.getpid())
```

```
map方法
from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
import os,time
def task(n):
    print('%s is running' %os.getpid())
    time.sleep(2)
    return n**2


if __name__ == '__main__':
    # p=ProcessPoolExecutor()
    p=ThreadPoolExecutor()
    start = time.time()
    obj=p.map(task,range(10))
    p.shutdown()
    print('='*30)
    print(list(obj))
    print(time.time() - start)
```

#### 34.关于守护线程说法,正确的是: 

```
关于守护线程说法,正确的是: 
A.所有的非守护线程终止，即使存在守护线程，进程运行终止。
B.所有守护线程终止，即使存在非守护线程，进程运行终止。
C.只要守护线程或者是非守护线程其中之一存在，进程就不会终止。
D.只要所有的守护线程和非守护线程终止运行之后，进程才会终止。
答案是 A

当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要任何非守护线程还在运行，程序就不会终止 

守护线程，守护进程是什么?
1.主进程创建守护进程
2.守护进程会在主进程代码运行结束的情况下，立即挂掉。
3.守护进程本身就是一个子进程。
4.主进程在其代码结束后就已经算运行完毕了（守护进程在此时就被回收）,然后主进程会一直等非守护的子进程都运行完毕后回收子进程的资源(否则会产生僵尸进程)，才会结束。
守护线程
1.守护线程会在"该进程内所有非守护线程全部都运行完毕后,守护线程才会挂掉"。并不是主线程运行完毕后守护线程挂掉。这一点是和守护进程的区别之处！
2.守护线程守护的是：当前进程内所有的子线程！
3.主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收）。因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。守护线程最后死。
```

#### 35.TCP协议在每次建立或者拆除连接时，都要在收发双方之间交换几次报文？

```python
TCP协议在每次建立或者拆除连接时，都要在收发双方之间交换几次报文？
答案 三次
```

#### 36.描述多进程开发中join和deamon的区别?

```python
join：当子线程调用join时，主线程会被阻塞，当子线程结束后，主线程才能继续执行。
deamon：当子进程被设置为守护进程时，主进程结束，不管子进程是否执行完毕，都会随着主进程的结束而结束。
```

#### 37. 请简述 GIL 对 Python 性能的影响 ?

```python
GIL是CPython中特有的全局解释器锁,作用就是保证同一时刻只有一个线程可以执行代码, 也因此造成了我们使用多线程的时候无法实现并行。

通常python解释器中当一个线程需要占用CPU执行之前，它需要GIL锁, 否则即使已经被操作系统调度出来，但仍然无法执行计算。所以Python解释器中，线程的想要执行CPU指令需要2个条件：

1) 被操作系统调度出来 (操作系统允许它占用CPU)
2) 获取到GIL锁 (CPython解释器允许它执行指令)

但并不总是能满足这2个条件。经常出现的情况是：已经抢到了cpu的资源，却受到GIL锁限制。这就是GIL影响Python性能的主要原因。

如果Python在单核CPU的机器上执行，它的多线程与单线程、以及其它语言的多线程在本质上并没有什么不一样。(所有线程都是轮流占用CPU执行指令).

而如果Python在多核CPU机器上执行的时候，性能则会非常槽糕。主要原因是在单核的时候，同时只有一个线程在执行CPU，所以这个线程总是能获取到GIL。而换到多核的时候，同时会有多个线程在不同的CPU核心上执行，此时不同线程之间就需要竞争GIL，而GIL只能同时被一个线程申请到，所以会导致其它线程处于闲置状态 (即使它已经拥有了CPU资源)。所以Python在多核CPU上的多线程始终只有单线程在跑程序
```

#### 38. 曾经在哪里使用过: 线程、 进程 、协程 ?

```python
1) 使用多进程请求多个url来减少网络等待浪费的时间.爬虫实例
2) 多进程实现套接字并发
3) 生产者-消费者模型: 在一直生产并且一直消费的过程中,避免供求关系不平衡

    flask有限使用协成
    DJANGO是多线程
    
    
    
import time
import random
from multiprocessing import Process,Queue
 
def produce(name,food,q):
    for i in range(1,4):
        res = '%s%s'%(food,i)
        time.sleep(random.uniform(1,3))
        q.put(res)
        print('\033[45m厨师%s生产了%s%s\033[0m'%(name,food,i))
 
def customer(name,q):
    while True:
        res = q.get()
        if res is None:
            break
        time.sleep(random.uniform(1,3))
        print('\033[46m%s 吃了 %s\033[0m'%(name,res))
 
if __name__ == '__main__':
    q = Queue()
    p1 = Process(target=produce,args=('bob','包子',q))
    p2 = Process(target=produce,args=('tom','馒头',q))
    p3 = Process(target=produce,args=('tony','花卷',q))
 
    c1 = Process(target=customer,args=('顾客1',q))
    c2 = Process(target=customer,args=('顾客1',q))
 
    p1.start()
    p2.start()
    p3.start()
    c1.start()
    c2.start()
 
    p1.join()
    p2.join()
    p3.join()
    q.put(None)
    q.put(None)
    print('主进程 is done')

1) 多线程实现socket:
import multiprocessing
import threading

import socket
s=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
s.bind(('127.0.0.1',8080))
s.listen(5)

def action(conn):
    while True:
        data=conn.recv(1024)
        print(data)
        conn.send(data.upper())

if __name__ == '__main__':
    while True:
        conn,addr=s.accept()

        p=threading.Thread(target=action,args=(conn,))
        p.start()
        
 2) 生产者-消费者模型: 在一直生产并且一直消费的过程中,避免供求关系不平衡
        
1. 计算密集型的时候使用进程。

2. I/O密集型的时候使用线程。

	io密集型：有阻塞的状态，就是一直会执行CPU（中间就一个等待状态，这个就叫做IO密集型）。例如：sleep状态
计算密集型任务：没有等待的状态就是计算密集型，从上到下执行没有等待。
	在Python中没法同时使用多个CPU，在同一时刻，多个线程是互相抢占资源的，在cpython运行中加了一把锁GIL。
	如果任务是IO密集型的可以使用多线程（阻塞等待时，就是放GIL，给另一个线程执行的机会）
	如果是计算密集型任务时，无法使用多线程（如果遇到CPU密集型的线程，一只占用CPU，不会被I/O阻塞）

3. 协程在 I/O 密集型场景中的应用:
	在 I/O 密集型的应用中，CPU 可能总是苦苦等待着 I/O 操作的完成。如果是一个提供 Web 的服务的话，也就意味着一个线程会因为 I/O 阻塞而无法快速的对其他请求进行响应。势必也造成一种资源浪费和效率低下。在这种时候，协程的价值就体现了出来，例如基于 Python 语言实现的 tornado Web 框架就是基于此原理。
```

#### 39. 请使用 yield 实现一个协程 ?

```python
def constomer():    # 主-> 线程
    r = 'yield '
    while True:
        n = yield r
        if not n:
            return
        print('[CONSVMER] Constomering %s'%n)
        r = '200 ok'
def produce(c):      # 副-> 协程
    c.send(None)
    n = 0
    while n<5:
        n += 1
        print('[PRODUCE] Producing %s...'%n)
        r = c.send(n)
        print('[PRODUCE] Constomer return=%s'%r)
    c.close()
c = consumer()  # <generator object consumer at 0x0000024CEE8B8830>
produce(c)

运行结果:
[PRODUCE] Producing 1...
[CONSVMER] Constomering 1
[PRODUCE] Constomer return=200 ok
[PRODUCE] Producing 2...
[CONSVMER] Constomering 2
[PRODUCE] Constomer return=200 ok
......
```



#### 40.请使用 python 内置 async 语法实现一个协程 ?

```python
import time
import asyncio

now = lambda : time.time()

async def do_some_work(x):
    print("waiting:", x)
    # await 后面就是调用耗时的操作
	await asyncio.sleep(x)   # 模拟了阻塞或者耗时操作
    return "Done after {}s".format(x)
    
    
start = now()
# 这里是一个协程对象，这个时候do_some_work函数并没有执行
coroutine = do_some_work(2)
#  创建一个事件loop(循环)
loop = asyncio.get_event_loop()
# task对象是Future类的子类，保存了协程运行后的状态，用于未来获取协程的结果
task = asyncio.ensure_future(coroutine)
# 将协程注册到事件循环loop中，并启动事件循环
loop.run_until_complete(coroutine)

print("Time:",now()-start)

结果:
waiting: 2
Task ret: Done after 2s
Time: 2.0025360584259033

	使用async可以定义协程对象，使用await可以针对耗时的操作进行挂起，就像生成器里的yield一样，函数让出控制权。协程遇到await，事件循环将会挂起该协程，执行别的协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行

	耗时的操作一般是一些IO操作，例如网络请求，文件读取等。我们使用asyncio.sleep函数来模拟IO操作。协程的目的也是让这些IO操作异步化。
```



#### 41. 简述线程死锁是如何造成的 ?  如何避免 ?

```python
死锁现象 本质是 代码逻辑的问题
是指两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，
若无外力作用，它们都将无法推进下去。

避免:
1) 添加互斥锁:来保证共享数据操作的完整性。来保证在任一时刻，只能有一个线程访问该对象。
   无论在相同的线程还是不同的线程,都只能连续acquire一次
   要想再acquire,必须先release
2)添加递归锁 :  RLcok
   在同一个线程中,可以无限次的acquire
   但是要想在其他线程中也acquire,
   必须现在自己的线程中添加和acquire次数相同的release

   # 这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。
   # 一个线程拿到锁，counter加1,该线程内又碰到加锁的情况，则counter继续加1，这期间所有其他线程都只能等待，等待该线程释放所有锁，即counter递减到0为止


死锁现象:
import time
from threading import Thread,Lock
noodle_lock = Lock()
frok_lock = Lock()
def eat1(name):
    noodle_lock.acquire()
    print("%s 拿到面条了"% name)
    frok_lock.acquire()
    print("%s 拿到叉子了" % name)
    print("%s 开始吃面了" % name)
    time.sleep(0.2)
    frok_lock.release()
    print("%s 放下叉子了"% name)
    noodle_lock.release()
    print("%s 放下面了"% name)
def eat2(name):
    frok_lock.acquire()
    print("%s 拿到叉子了" % name)
    noodle_lock.acquire()
    print("%s 拿到面条了" % name)
    print("%s 开始吃面了" % name)
    time.sleep(0.2)
    noodle_lock.release()
    print("%s 放下面了" % name)
    frok_lock.release()
    print("%s 放下叉子了" % name)
Thread(target = eat1,args = ("alex",)).start()
Thread(target = eat1,args = ("wusir",)).start()
Thread(target = eat2,args = ("宝元",)).start()
Thread(target = eat2,args = ("太白",)).start()
```

#### 42. asyncio 是什么 ?

```python
asyncio 第三方模块, 可以实现异步网络操作, 并发 和 协程
官网描述: 当代码需要执行一个耗时的 I/O 操作的时候, 它只发出 I/O 的指令, 并不等待 I/O 的结果, 然后去执行其它的代码, 以提高效率。

关于 asyncio 的关键字:
event_loop 事件循环：程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数

coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。

task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含了任务的各种状态

future: 代表将来执行或没有执行的任务的结果。它和task上没有本质上的区别

async/await 关键字：python3.5用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。
```

#### 43. gevent 模块是什么 ?

```python
event 是一个第三方库，可以轻松通过gevent实现并发同步或异步编程，在gevent中用到的主要模式是Greenlet, 它是以C扩展模块形式接入Python的轻量级协程。 Greenlet全部运行在主程序操作系统进程的内部，但它们被协作式地调度。
简单来说: gevent 基于greenlet实现的 多个任务交给gevent管理 遇到IO就是用greenlet进行切换 实现协程

# 简单实例:
import gevent
def play():
    print("start play")
    gevent.sleep(1)
    print("end play")
def sleep():
    print("start sleep")
    gevent.sleep(1)
    print("end sleep")
gevent.joinall([g1,g2])
g1.join()
g2.join()

执行顺序:
    start play
	start sleep
	end play
	end sleep
    
gevent.sleep 模拟的是gevent可以识别的io阻塞,而gevent是不能直接识别time.sleep或其他的阻塞,使用猴子补丁来识别阻塞.
from gevent import monkey;monkey.patch_all()必须放到被打补丁者的前面，如time，socket模块之前
```

#### 44. 什么是 twisted 框架 ?

```

```

#### 45. 什么是 LVS?

```python
LVS是Linux Virtual Server的简写, Linux虚拟服务器，是一个虚拟的服务器集群系统.
作用：LVS主要用于多服务器的负载均衡。
它工作在网络层，可以实现高性能，高可用的服务器集群技术。
它廉价，可把许多低性能的服务器组合在一起形成一个超级服务器。
它易用，配置非常简单，且有多种负载均衡的方法。
它稳定可靠，即使在集群的服务器中某台服务器无法正常工作，也不影响整体效果。另外可扩展性也非常好。
```

#### 46. 什么是 Nginx ?

```python
前言:
我们平时访问的网站服务 就是 Web 网络服务，一般是指允许用户通过浏览器访问到互联网中各种资源的服务。
Web 网络服务是一种被动访问的服务程序，即只有接收到互联网中其他主机发出的 请求后才会响应，最终用于提供服务程序的 Web 服务器会通过 HTTP(超文本传输协议)或 HTTPS(安全超文本传输协议)把请求的内容传送给用户。
nginx就是一种能够提供 Web 网络服务的程序.

1. nginx是一个开源的，支持高性能，高并发(能支持几万并发连接)的www服务和代理服务软件。
2. nginx性能比较高，占用的系统资源更少，支持更高的并发连接，有更高的访问效率。(在3万并发连接下开启10个nginx线程消耗的内存不到200M)
3. nginx不但是一个优秀的web服务软件，还可以作为http的反向代理，负载均衡，以及缓存服务使用。
4. 支持异步网络i/o事件模型epoll
```

#### 47. 什么是 keeppalived ?

```
	Keepalived的作用是检测服务器的状态，如果有一台web服务器宕机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。
	
Keepalived是一款高可用软件，它的功能主要包括两方面：
1）通过IP漂移，实现服务的高可用：服务器集群共享一个虚拟IP，同一时间只有一个服务器占有虚拟IP并对外提供服务，若该服务器不可用，则虚拟IP漂移至另一台服务器并对外提供服务；
2）对LVS应用服务层的应用服务器集群进行状态监控：若应用服务器不可用，则keepalived将其从集群中摘除，若应用服务器恢复，则keepalived将其重新加入集群中。

工作原理:
在keepalived工作时，主master节点会不断的向备节点发送心跳消息，告诉备节点自己还活着，
当master节点故障时，就无法发送心跳消息，备节点就无法检测到来自master的心跳了，于是调用自身的接管程序，接管master节点的ip资源以及服务，
当master主节点恢复时，备backup节点又会释放接管的ip资源和服务，回复到原本的备节点角色。

Keepalived可以单独使用，即通过IP漂移实现服务的高可用，也可以结合LVS使用，即一方面通过IP漂移实现LVS负载均衡层的高可用，另一方面实现LVS应用服务层的状态监控，如图所示：
```

#### 48. 什么是 haproxy ?

```
	HAProxy是一个使用C语言编写的自由及开放源代码软件[1]，其提供高可用性、负载均衡，以及基于TCP和HTTP的应用程序代理。
	HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。
	HAProxy实现了一种事件驱动, 单一进程模型，此模型支持非常大的并发连接数。多进程或多线程模型受内存限制 、系统调度器限制以及无处不在的锁限制，很少能处理数千并发连接。事件驱动模型因为在有更好的资源和时间管理的用户空间(User-Space) 实现所有这些任务，所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。这就是为什么他们必须进行优化以 使每个CPU时间片(Cycle)做更多的工作。
```

#### 49. 什么是负载均衡 ?

```python
Web服务器，直接面向用户，往往要承载大量并发请求，单台服务器难以负荷，这个时候使用多台web服务器组成集群，前端使用Nginx负载均衡，将请求分散的打到我们的后端服务器集群中，合理的分摊服务器压力，达到服务器性能的最大优化。那么会大大提升系统的吞吐率、请求性能、高容灾.

负载均衡是由多台服务器以对称的方式组成一个服务器集合，每台服务器都具有等价的地位，都可以单独对外供应效力而无须其他服务器的辅助。经过某种负载分管技术，将外部发送来的央求均匀分配到对称结构中的某一台服务器上，而接收到央求的服务器独登时回应客户的央求。均衡负载可以平均分配客户央求到服务器列阵，借此供应快速获取重要数据，解决很多并发访问效力问题。这种群集技术可以用最少的出资取得接近于大型主机的性能。

Nginx要实现负载均衡需要用到proxy_pass代理模块配置, Nginx负载均衡与Nginx代理不同地方在于, Nginx代理仅代理一台服务器，而Nginx负载均衡则是将客户端请求代理转发至一组upstream虚拟服务池, Nginx可以配置代理多台服务器，当一台服务器宕机之后，仍能保持系统可用。
```

#### 50. 什么是 rpc 以及应用场景 ?

```
RPC是指远程过程调用，也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。

RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。
RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。
```

#### 51. 什么是正向代理和反向代理 ?

```python
正向代理: 正向代理的对象是 客户端
正向代理，它的工作原理就像一个跳板（VPN），简单的说：
我是一个用户，我访问不了某网站，但是我能访问一个代理服务器，这个代理服务器，他能访问那个我不能访问的网站，于是我先连上代理服务器，告诉他我需要那个无法访问网站的内容，代理服务器去取回来，然后返回给我。

正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。

正向代理的用途：
（1）访问原来无法访问的资源，如google
（2） 可以做缓存，加速访问资源
（3）对客户端访问授权，上网进行认证
（4）代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息

反向代理:
对于客户端而言，代理服务器就像是原始服务器。保护和隐藏原始资源服务器.
	客户端是无感知代理的存在的，反向代理对外都是透明的，访问者者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。
	反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。
    
反向代理的作用：
（1）保证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击
（2）负载均衡，通过反向代理服务器来优化网站的负载
```

#### 52.csv文件 [erotic.csv] 中共存在271 万多条数据, 请获取其中的 subscription_id, 并使用线程池为每100 条数据创建一个线程去处理 (打印或通过爬虫去提交到某处), erotic.scv 文件格式为:

```
"subscription_id", "erotic", "num"
"UCURGHWsDe7S-v1ufCAq9Rfw", "5", "1"
```





 