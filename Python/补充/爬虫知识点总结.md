### 知识点总结
- requests模块
    - 爬取数据流程：
        - 指定url
        - 发起请求(get/post==>response)
        - 获取响应数据
        - 数据解析
        - 持久化存储
    - get，post作用：
        - 发起请求获取响应对象
    - get，post参数：
        - url
        - data/params
        - headers
        - proxies
    - 处理ajax动态加载的数据：
        - 动态加载数据捕获
        - 如何获取动态加载的数据?
            - 基于抓包工具进行全局搜索,定位到数据所在的数据包,数据包中可以提取到url和请求参数.
    - 模拟登陆：
        - 目的:有些数据时必须经过登录后才可以爬取.
    - 打码平台使用：
        - 超级鹰
        - 云打码
        - 打码兔
    - cookie的处理：
        - 手动处理:从抓包工具中捕获cookie将其作用到headers字典中.
        - 自动处理:requests.Session()==>session
        - cookie是一组键值对,相关的数据可能是动态生成(动态加密)
            - cookie:split:(10组)
            - cookie:split:(10组)
            - 变化的键值数据做基于抓包全局搜索
    - 代理ip：
        - 构建一个免费的代理池
    - 线程池：import mutiprosing.dummy import Pool
            - map(func,alist)
    - 单线程+多任务异步协程
        - 协程对象:特殊函数返回的对象
        - 任务对象:协程对象的进一步封装
            - 绑定回调:task.add_done_callback(func)
        - 事件循环对象:装载的是多个任务对象.只有当EventLoop对象启动后,该对象就可以对其内部的多个任务对象执行异步操作.
        - await,async
        - aiohttp
    - 图片懒加载：
        - 伪属性
- 数据解析:1.标签定位 2.数据的提取
    - 正则表达式：
    - bs4：
        - soup.tagName
        - text
        - string
        - find/find_all()
        - select()
        - tag['attrName']
    - xpath：
        - 属性定位
        - 索引定位
        - /text()
        - tag/@attrname
    - pyquery
- selenium:可见即可得
    - 关联:
        - 便捷获取动态加载的数据
        - 实现模拟登录
    - 使用流程：
        - 实例化一个浏览器对象(驱动)
        - 相关自动化的操作
        - 关闭浏览器对象
    - find系列函数：
        - 标签的定位
        - click()
        - send_keys()
        - page_sourse:返回的就是页面中所有的页面源码数据
    - switch_to.frame函数
        - 使用场景:当定位的标签是存在于iframe标签之中的
    - 动作链ActionChains:
        - ActionChains(bro)
        - perform():立即执行动作链
    - phantomJs：是一款无可视化界面的浏览器
    - 谷歌无头浏览器：
    - 规避检测:
- scrapy
    - 项目创建流程：
        - scrapy startproject ProName
        - cd ProName
        - scrapy genspider xxx www.xxx.com
    - 数据解析(xpath):
        - response.xpath('xpath表达式')
        - 此xpath和etree的xpath的区别:
            - 此xpath最终返回的列表元素为Selector对象.
            - 调用extract/extract_first()将Selector对象中的字符串数据进行提取
    - 持久化存储：
        - 基于终端指令
            - 只可以将parse方法的返回值存储在指定后缀的文本文件中
            - scrapy crawl xxx -o filePath
        - 基于管道
            - 1.数据解析
            - 2.在item类中构建相关的属性
            - 3.实例化item类型的对象,且将解析到的数据存储封装到该对象中
            - 4.向管道提交item对象:yield item
            - 5.在管道的process_item方法中接收item且对其进行任意形式的持久化存储
            - 6.在配置文件中开启管道
            
            - 细节:
                - 1.一个管道类对应一种方式的持久化存储
                - 2.爬虫文件提交的item只会提交给优先级最高的那一个管道类
                - 3.process_item中的return item表示的含义就是将item传递给下一个即将被执行的管道类
                - 4.open_spider(self,spider),close_spider只会被各自执行一次
    - 处理分页数据爬取：
        - 手动请求发送:
            - yield scrapy.Request(url,callback)
            - yirdl scrapy.FormRequest(url,formdata,callback)
    - cookie处理：自动
    - 日志等级：LOG_LEVEL = 'ERROR'
    - 请求传参：
        -作用:被用作于基于scrapy的深度爬取
        - yield scrapy.Request(url,callback,meta):将meta字典传递给callback
            - callback接收meta:response.meta
    - 五大核心组件原理：
        - 引擎:处理所有的数据流,触发事务
        - Spider
        - 管道
        - 下载器
        - 调度器:过滤器和队列
    - 下载中间件：
        - 爬虫中间件
        - 下载中间件:process_request,process_response,process_exception
            - 作用:批量拦截所有的请求和响应
            - 拦截请求?
                - 进行UA伪装
                    - process_request:request.headers['User-Agent'] = 'xxxx'
                - 进行代理操作
                    - request.meta['proxy'] = 'http://ip:port'
            - 拦截响应?
                - 篡改响应数据或者响应对象
    - UA池和代理池：
    - selenium在scrapy中的应用：
        - 浏览器的创建:爬虫类属性
        - 关闭浏览器对象:爬虫类,重写closed,在该方法中关闭浏览器
        - 在中间件中获取浏览器对象,完成相关自动化的操作
    - crawlSpider：
        - 是Spider的一个子类,功能要比Spider多(连接提取器,规则解析器)
        - LinkExtractor:
            - 根据正则指定的规则进行指定url的提取
        - Rule:
            - 接收LinkExtractor提取出来的链接,然后对其进行请求发送,将请求到的数据进行指定规则的数据解析
- 分布式
    - scrapy为何不能实现分布式：
        - 调度器无法被共享
        - 管道无法被共享
    - scrapy-redis的作用：
    - 实现分布式的方式：
    - 流程：
- 增量式爬虫:
    - 核心:去重.redis的set集合实现的去重
    - 概念:检测网站数据更新的情况.

### 案例总结
    - 爬取肯德基餐厅位置信息：http://www.kfc.com.cn/kfccda/index.aspx
    - 爬取药监总局：http://125.35.6.84:81/xk/
    - 爬取糗事百科图片：https://www.qiushibaike.com/pic/
    - 下载免费简历模板：http://sc.chinaz.com/jianli/free.html
    - 煎蛋网图片爬取：http://jandan.net/ooxx
    - 解析城市名称：https://www.aqistudy.cn/historydata/
    - 古诗文网：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx
    - 网易新闻：https://news.163.com/


### 反爬机制总结
- robots.txt
- UA检测
- 验证码
- js加密
- cookie
- 禁IP
- 动态token
- 数据动态加载
- 图片懒加载
- js混淆

### 数据清洗
- 空值检测删除空值所在的行数据：
- 空值检测填充空值：
- 异常值检测和过滤：
- 重复行检测和删除：

### 面试题
- 如何提升爬虫的效率
    - 线程池
    - scrapy配置文件相关配置（禁用cookie，禁止重试，减小下载超时，增加并发，日志等级）
    - 单线程+多任务异步协程
    - 分布式
- scrapy核心组件工作流程
- scrapy中如何设置代理（两种方法）
    - 中间件
    - 环境变量（os.environ['HTTPS_PROXY'] = 'https:ip:port'）
- scrapy如何实现限速
    
    - DOWNLOAD_DELAY = 1
- scrapy如何实现暂停爬虫
    - JOBDIR='sharejs.com'
    - control-C
- pipeline如何丢弃一个item对象
- scrapy-redis组件作用
- 实现深度和广度优先:默认为深度优先。
    - DEPTH_PRIORITY = 1
    - SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'
    - SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'
    - 深度优先：不全部保留结点，占用空间少；运行速度慢
    - 光度优先：保留全部结点，占用空间大；运行速度快
    - https://www.cnblogs.com/zhaof/p/7092400.html

		a)	通过在settings.py中设置DEPTH_LIMIT的值可以限制爬取深度，这个深度是与start_urls中定义url的相对值。也就是相对url的深度。例如定义url为：http://www.domz.com/game/,DEPTH_LIMIT=1那么限制爬取的只能是此url下一级的网页。深度大于设置值的将被ignore。

- 掌握哪些基于爬虫的模块
    - urllib
    - requests
    - aiohttp
- 比较难的反爬:
    - 动态数据加载
    - js加密:Pyexcl
    - js混淆
    - 动态参数捕获
- 移动端数据的抓取:
    
    - 配置流程:基于fiddler
- 你爬取过哪些类型的数据,量级是多少?
    - 新闻资讯
    - 电商:30-200w
    - 金融数据
    - 医疗
- 爬虫框架:
    - scrapy
    - pyspider
- 如何解析数携带标签的数据?
    
    - 使用bs4
- 如何实现全栈数据的爬取:
    - 手动请求发送(Spider)
    - CrawlSpider

- 数据清洗:
    - 清洗缺失值:
        - dropna
        - fillna
    - 清洗重复值:
        - drop_duplicated(keep)
    - 清洗异常值:
        - 判定异常值的条件
        - isnull notnull any all
- 数据分析
- numpy
    - 创建数据的方式
        - np.array()
        - np.random
        - np.linspace
        - np.arange
    - numpy数据的相关运算的方式
        - 索引
        - 切片
            - arr[1:3]
            - arr[行,列]
        - 级联:concatenate(objs,axis)就是多多个numpy数据进行横向或者纵向的拼接
        - 变形:reshape,将指定维度的属组变形成任意维度属组
        - 聚合操作:max,min,sum......
        - 排序:sort
- pandas
    - DataFrame
        - 1.创建
        - 2.可以将外部文件中的数据读取加载到df中.read系列的函数
        - 3.将df中的数据写入到外部.to系列的函数
        - 4.索引和切片
        - 5.级联:concat(obj,axis)
        - 6.合并:merge,对数据的合并,必须要有合并的条件
            - 如果想保证合并后数据的完整性:how='outer'
        - 7.数据随机抽样:
            - take():行列索引进行任意的排列
            - permutation(x):返回的是0-(x-1)之间的一个随机序列
        - 8.替换:replace
        - 9.map,是Series的方法
            - 映射:基于映射关系表(字典)map(dic)
            - 运算工具:map(func)
        - 分组:groupby,必须要有分组条件
            - 分组之后必聚合
            - transform/apply:如果聚合操作是我们自定义的方法
        - 数据清洗
        - 去重
        - 空值的检测
- matplotlib
    - 线形图
    - 直方图:密度图
    - 散点图:
    - 柱状图
    - 饼图
    - 3D图形:玫瑰图,线形图

- dba:纯数据分析,实现数据分析策略制定
- 数据分析+建模:pandas,matplotlib+sklearn
- 机器学习:
    - sklearn
        - 线性模型
        - 树模型
    - 算法模型
         - 作用
    - 样本数据:
        - 和模型之间的关联
    - 如何提取合适的样本数据

