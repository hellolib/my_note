{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点总结\n",
    "- requests模块\n",
    "    - 爬取数据流程：\n",
    "        - 指定url\n",
    "        - 发起请求(get/post==>response)\n",
    "        - 获取响应数据\n",
    "        - 数据解析\n",
    "        - 持久化存储\n",
    "    - get，post作用：\n",
    "        - 发起请求获取响应对象\n",
    "    - get，post参数：\n",
    "        - url\n",
    "        - data/params\n",
    "        - headers\n",
    "        - proxies\n",
    "    - 处理ajax动态加载的数据：\n",
    "        - 动态加载数据捕获\n",
    "        - 如何获取动态加载的数据?\n",
    "            - 基于抓包工具进行全局搜索,定位到数据所在的数据包,数据包中可以提取到url和请求参数.\n",
    "    - 模拟登陆：\n",
    "        - 目的:有些数据时必须经过登录后才可以爬取.\n",
    "    - 打码平台使用：\n",
    "        - 超级鹰\n",
    "        - 云打码\n",
    "        - 打码兔\n",
    "    - cookie的处理：\n",
    "        - 手动处理:从抓包工具中捕获cookie将其作用到headers字典中.\n",
    "        - 自动处理:requests.Session()==>session\n",
    "        - cookie是一组键值对,相关的数据可能是动态生成(动态加密)\n",
    "            - cookie:split:(10组)\n",
    "            - cookie:split:(10组)\n",
    "            - 变化的键值数据做基于抓包全局搜索\n",
    "    - 代理ip：\n",
    "        - 构建一个免费的代理池\n",
    "    - 线程池：import mutiprosing.dummy import Pool\n",
    "            - map(func,alist)\n",
    "    - 单线程+多任务异步协程\n",
    "        - 协程对象:特殊函数返回的对象\n",
    "        - 任务对象:协程对象的进一步封装\n",
    "            - 绑定回调:task.add_done_callback(func)\n",
    "        - 事件循环对象:装载的是多个任务对象.只有当EventLoop对象启动后,该对象就可以对其内部的多个任务对象执行异步操作.\n",
    "        - await,async\n",
    "        - aiohttp\n",
    "    - 图片懒加载：\n",
    "        - 伪属性\n",
    "- 数据解析:1.标签定位 2.数据的提取\n",
    "    - 正则表达式：\n",
    "    - bs4：\n",
    "        - soup.tagName\n",
    "        - text\n",
    "        - string\n",
    "        - find/find_all()\n",
    "        - select()\n",
    "        - tag['attrName']\n",
    "    - xpath：\n",
    "        - 属性定位\n",
    "        - 索引定位\n",
    "        - /text()\n",
    "        - tag/@attrname\n",
    "    - pyquery\n",
    "- selenium:可见即可得\n",
    "    - 关联:\n",
    "        - 便捷获取动态加载的数据\n",
    "        - 实现模拟登录\n",
    "    - 使用流程：\n",
    "        - 实例化一个浏览器对象(驱动)\n",
    "        - 相关自动化的操作\n",
    "        - 关闭浏览器对象\n",
    "    - find系列函数：\n",
    "        - 标签的定位\n",
    "        - click()\n",
    "        - send_keys()\n",
    "        - page_sourse:返回的就是页面中所有的页面源码数据\n",
    "    - switch_to.frame函数\n",
    "        - 使用场景:当定位的标签是存在于iframe标签之中的\n",
    "    - 动作链ActionChains:\n",
    "        - ActionChains(bro)\n",
    "        - perform():立即执行动作链\n",
    "    - phantomJs：是一款无可视化界面的浏览器\n",
    "    - 谷歌无头浏览器：\n",
    "    - 规避检测:\n",
    "- scrapy\n",
    "    - 项目创建流程：\n",
    "        - scrapy startproject ProName\n",
    "        - cd ProName\n",
    "        - scrapy genspider xxx www.xxx.com\n",
    "    - 数据解析(xpath):\n",
    "        - response.xpath('xpath表达式')\n",
    "        - 此xpath和etree的xpath的区别:\n",
    "            - 此xpath最终返回的列表元素为Selector对象.\n",
    "            - 调用extract/extract_first()将Selector对象中的字符串数据进行提取\n",
    "    - 持久化存储：\n",
    "        - 基于终端指令\n",
    "            - 只可以将parse方法的返回值存储在指定后缀的文本文件中\n",
    "            - scrapy crawl xxx -o filePath\n",
    "        - 基于管道\n",
    "            - 1.数据解析\n",
    "            - 2.在item类中构建相关的属性\n",
    "            - 3.实例化item类型的对象,且将解析到的数据存储封装到该对象中\n",
    "            - 4.向管道提交item对象:yield item\n",
    "            - 5.在管道的process_item方法中接收item且对其进行任意形式的持久化存储\n",
    "            - 6.在配置文件中开启管道\n",
    "            \n",
    "            - 细节:\n",
    "                - 1.一个管道类对应一种方式的持久化存储\n",
    "                - 2.爬虫文件提交的item只会提交给优先级最高的那一个管道类\n",
    "                - 3.process_item中的return item表示的含义就是将item传递给下一个即将被执行的管道类\n",
    "                - 4.open_spider(self,spider),close_spider只会被各自执行一次\n",
    "    - 处理分页数据爬取：\n",
    "        - 手动请求发送:\n",
    "            - yield scrapy.Request(url,callback)\n",
    "            - yirdl scrapy.FormRequest(url,formdata,callback)\n",
    "    - cookie处理：自动.\n",
    "    - 日志等级：LOG_LEVEL = 'ERROR'\n",
    "    - 请求传参：\n",
    "        -作用:被用作于基于scrapy的深度爬取\n",
    "        - yield scrapy.Request(url,callback,meta):将meta字典传递给callback\n",
    "            - callback接收meta:response.meta\n",
    "    - 五大核心组件原理：\n",
    "        - 引擎:处理所有的数据流,触发事务\n",
    "        - Spider\n",
    "        - 管道\n",
    "        - 下载器\n",
    "        - 调度器:过滤器和队列\n",
    "    - 下载中间件：\n",
    "        - 爬虫中间件\n",
    "        - 下载中间件:process_request,process_response,process_exception\n",
    "            - 作用:批量拦截所有的请求和响应\n",
    "            - 拦截请求?\n",
    "                - 进行UA伪装\n",
    "                    - process_request:request.headers['User-Agent'] = 'xxxx'\n",
    "                - 进行代理操作\n",
    "                    - request.meta['proxy'] = 'http://ip:port'\n",
    "            - 拦截响应?\n",
    "                - 篡改响应数据或者响应对象\n",
    "    - UA池和代理池：\n",
    "    - selenium在scrapy中的应用：\n",
    "        - 浏览器的创建:爬虫类属性\n",
    "        - 关闭浏览器对象:爬虫类,重写closed,在该方法中关闭浏览器\n",
    "        - 在中间件中获取浏览器对象,完成相关自动化的操作\n",
    "    - crawlSpider：\n",
    "        - 是Spider的一个子类,功能要比Spider多(连接提取器,规则解析器)\n",
    "        - LinkExtractor:\n",
    "            - 根据正则指定的规则进行指定url的提取\n",
    "        - Rule:\n",
    "            - 接收LinkExtractor提取出来的链接,然后对其进行请求发送,将请求到的数据进行指定规则的数据解析\n",
    "- 分布式\n",
    "    - scrapy为何不能实现分布式：\n",
    "        - 调度器无法被共享\n",
    "        - 管道无法被共享\n",
    "    - scrapy-redis的作用：\n",
    "    - 实现分布式的方式：\n",
    "    - 流程：\n",
    "- 增量式爬虫:\n",
    "    - 核心:去重.redis的set集合实现的去重\n",
    "    - 概念:检测网站数据更新的情况."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例总结\n",
    "    - 爬取肯德基餐厅位置信息：http://www.kfc.com.cn/kfccda/index.aspx\n",
    "    - 爬取药监总局：http://125.35.6.84:81/xk/\n",
    "    - 爬取糗事百科图片：https://www.qiushibaike.com/pic/\n",
    "    - 下载免费简历模板：http://sc.chinaz.com/jianli/free.html\n",
    "    - 煎蛋网图片爬取：http://jandan.net/ooxx\n",
    "    - 解析城市名称：https://www.aqistudy.cn/historydata/\n",
    "    - 古诗文网：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx\n",
    "    - 网易新闻：https://news.163.com/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反爬机制总结\n",
    "- robots.txt\n",
    "- UA检测\n",
    "- 验证码\n",
    "- js加密\n",
    "- cookie\n",
    "- 禁IP\n",
    "- 动态token\n",
    "- 数据动态加载\n",
    "- 图片懒加载\n",
    "- js混淆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗\n",
    "- 空值检测删除空值所在的行数据：\n",
    "- 空值检测填充空值：\n",
    "- 异常值检测和过滤：\n",
    "- 重复行检测和删除："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面试题\n",
    "- 如何提升爬虫的效率\n",
    "    - 线程池\n",
    "    - scrapy配置文件相关配置（禁用cookie，禁止重试，减小下载超时，增加并发，日志等级）\n",
    "    - 单线程+多任务异步协程\n",
    "    - 分布式\n",
    "- scrapy核心组件工作流程\n",
    "- scrapy中如何设置代理（两种方法）\n",
    "    - 中间件\n",
    "    - 环境变量（os.environ['HTTPS_PROXY'] = 'https:ip:port'）\n",
    "- scrapy如何实现限速\n",
    "    - DOWNLOAD_DELAY = 1\n",
    "- scrapy如何实现暂停爬虫\n",
    "    - JOBDIR='sharejs.com'\n",
    "    - control-C\n",
    "- pipeline如何丢弃一个item对象\n",
    "- scrapy-redis组件作用\n",
    "- 实现深度和广度优先:默认为深度优先。\n",
    "    - DEPTH_PRIORITY = 1\n",
    "    - SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\n",
    "    - SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\n",
    "    - 深度优先：不全部保留结点，占用空间少；运行速度慢\n",
    "    - 光度优先：保留全部结点，占用空间大；运行速度快\n",
    "    - https://www.cnblogs.com/zhaof/p/7092400.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\t通过在settings.py中设置DEPTH_LIMIT的值可以限制爬取深度，这个深度是与start_urls中定义url的相对值。也就是相对url的深度。例如定义url为：http://www.domz.com/game/,DEPTH_LIMIT=1那么限制爬取的只能是此url下一级的网页。深度大于设置值的将被ignore。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 掌握哪些基于爬虫的模块\n",
    "    - urllib\n",
    "    - requests\n",
    "    - aiohttp\n",
    "- 比较难的反爬:\n",
    "    - 动态数据加载\n",
    "    - js加密:Pyexcl\n",
    "    - js混淆\n",
    "    - 动态参数捕获\n",
    "- 移动端数据的抓取:\n",
    "    - 配置流程:基于fiddler\n",
    "- 你爬取过哪些类型的数据,量级是多少?\n",
    "    - 新闻资讯\n",
    "    - 电商:30-200w\n",
    "    - 金融数据\n",
    "    - 医疗\n",
    "- 爬虫框架:\n",
    "    - scrapy\n",
    "    - pyspider\n",
    "- 如何解析数携带标签的数据?\n",
    "    - 使用bs4\n",
    "- 如何实现全栈数据的爬取:\n",
    "    - 手动请求发送(Spider)\n",
    "    - CrawlSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据清洗:\n",
    "    - 清洗缺失值:\n",
    "        - dropna\n",
    "        - fillna\n",
    "    - 清洗重复值:\n",
    "        - drop_duplicated(keep)\n",
    "    - 清洗异常值:\n",
    "        - 判定异常值的条件\n",
    "        - isnull notnull any all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据分析\n",
    "- numpy\n",
    "    - 创建数据的方式\n",
    "        - np.array()\n",
    "        - np.random\n",
    "        - np.linspace\n",
    "        - np.arange\n",
    "    - numpy数据的相关运算的方式\n",
    "        - 索引\n",
    "        - 切片\n",
    "            - arr[1:3]\n",
    "            - arr[行,列]\n",
    "        - 级联:concatenate(objs,axis)就是多多个numpy数据进行横向或者纵向的拼接\n",
    "        - 变形:reshape,将指定维度的属组变形成任意维度属组\n",
    "        - 聚合操作:max,min,sum......\n",
    "        - 排序:sort\n",
    "- pandas\n",
    "    - DataFrame\n",
    "        - 1.创建\n",
    "        - 2.可以将外部文件中的数据读取加载到df中.read系列的函数\n",
    "        - 3.将df中的数据写入到外部.to系列的函数\n",
    "        - 4.索引和切片\n",
    "        - 5.级联:concat(obj,axis)\n",
    "        - 6.合并:merge,对数据的合并,必须要有合并的条件\n",
    "            - 如果想保证合并后数据的完整性:how='outer'\n",
    "        - 7.数据随机抽样:\n",
    "            - take():行列索引进行任意的排列\n",
    "            - permutation(x):返回的是0-(x-1)之间的一个随机序列\n",
    "        - 8.替换:replace\n",
    "        - 9.map,是Series的方法\n",
    "            - 映射:基于映射关系表(字典)map(dic)\n",
    "            - 运算工具:map(func)\n",
    "        - 分组:groupby,必须要有分组条件\n",
    "            - 分组之后必聚合\n",
    "            - transform/apply:如果聚合操作是我们自定义的方法\n",
    "        - 数据清洗\n",
    "        - 去重\n",
    "        - 空值的检测\n",
    "- matplotlib\n",
    "    - 线形图\n",
    "    - 直方图:密度图\n",
    "    - 散点图:\n",
    "    - 柱状图\n",
    "    - 饼图\n",
    "    - 3D图形:玫瑰图,线形图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dba:纯数据分析,实现数据分析策略制定\n",
    "- 数据分析+建模:pandas,matplotlib+sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 机器学习:\n",
    "    - sklearn\n",
    "        - 线性模型\n",
    "        - 树模型\n",
    "    - 算法模型\n",
    "         - 作用\n",
    "    - 样本数据:\n",
    "        - 和模型之间的关联\n",
    "    - 如何提取合适的样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
