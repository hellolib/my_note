- 错误码:ConectionPool:
  
  - 爬虫报错原因；
  
    ```python
    1.在短时间内向网站发起了一个高频的请求
    # 使用代理
    2.连接池(http)中的资源被耗尽
    # 立即将请求断开:Connection:close
    ```

- xpth和bs4两种解析方式最大的区别；

  ```python
  - 数据解析的作用:为了实现聚焦爬虫
      - bs4:
          - soup.tagName
          - find/find_all('tagName',attrName='value')
          - select('Selector')
              - > 空格
          - string/text
          - tag['href']
      - xpath:
          - //tagName
          - //tagName[@attrName="value"]
          - //div[1]
          - //text  or /text
          - //a/@href
      - bs4和xpath最明显的一个区别什么?
      - 解析出携带标签的局部内容?
          - bs4相关标签定位的方法或者属性返回值就是携带标签的内容
  ```

## 1.代理(proxy)

- 代理服务器:实现请求转发,从而可以实现更换请求的ip地址
  
- 代理的匿名度:
  - 透明:服务器知道你使用了代理并且知道你的真实ip
  - 匿名:服务器知道你使用了代理,但是不知道你的真实ip
  - 高度匿名(高匿):服务器不知道你使用了代理,更不知道你的真实ip
  
- 代理的类型:
  - http:该类型的代理只可以转发http协议的请求
  - https:只可以转发https协议的请求

- **requests请求中更换ip:**

  在requests添加一个参数 proxies={'http/https':'ip:port'}

  ```python
  import requests
  headers = {
      'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
  }
  url = 'https://www.baidu.com/s?wd=ip'
  #proxies={'http/https':'ip:port'}
  page_text = requests.get(url=url,headers=headers,proxies={'https':'1.197.203.187:9999'}).text
  with open('ip.html','w',encoding='utf-8') as fp:
      fp.write(page_text)
  ```

  

- 免费代理ip的网站

  - 快代理
  - 西祠代理
  - goubanjia
    - 代理精灵(推荐):<http://http.zhiliandaili.cn/>

- 在爬虫中遇到ip被禁掉如何处理?

  - 使用代理
  - 构建一个代理池
  - 拨号服务器

- 代理池构建

  ```python
  #基于代理精灵构建一个ip池
  from lxml import etree
  all_ips = [] # 列表形式的代理池
  proxy_url = 'http://t.11jsq.com/index.php/api/entry?method=proxyServer.generate_api_url&packid=1&fa=0&fetch_key=&groupid=0&qty=52&time=1&pro=&city=&port=1&format=html&ss=5&css=&dt=1&specialTxt=3&specialJson=&usertype=2'  # 代理ip地址
  proxy_page_text = requests.get(url=proxy_url,headers=headers).text
  tree = etree.HTML(proxy_page_text)
  proxy_list = tree.xpath('//body//text()')
  for ip in proxy_list:
      dic = {'https':ip}
      all_ips.append(dic)
  all_ips
  
  import random
  
  #爬取西祠代理中的免费代理ip
  url = 'https://www.xicidaili.com/nn/%d'
  free_proxies = []
  for page in range(1,30):
      new_url = format(url%page)
      page_text = requests.get(new_url,headers=headers,proxies=random.choice(all_ips)).text
      tree = etree.HTML(page_text)
      tr_list = tree.xpath('//*[@id="ip_list"]//tr')[1:]#xpath表达式中不可以出现tbody
      for tr in tr_list:
          ip = tr.xpath('./td[2]/text()')[0]
          port = tr.xpath('./td[3]/text()')[0]
          t_type = tr.xpath('./td[7]/text()')[0]
          
          dic = {
              'ip':ip,
              'port':port,
              'type':t_type
          }
          free_proxies.append(dic)
      print('第{}页爬取完毕!!!'.format(page))
  print(len(free_proxies))
  ```

  

## 2.cookie

- 作用:保存客户端的相关状态

- 在请求中携带cookie,在爬虫中如果遇到了cookie的反爬如何处理?

  - **手动处理**
    - 在抓包工具中捕获cookie,将其封装在headers中
    - 应用场景:cookie没有有效时长且不是动态变化
  - **自动处理**
    - 使用session机制
    - 使用场景:动态变化的cookie
    - session对象:该对象和requests模块用法几乎一致.如果在请求的过程中产生了cookie,如果该请求使用session发起的,则cookie会被自动存储到session中.

  ```python
  import requests
  session = requests.Session()
  main_url = 'https://xueqiu.com' #推测对该url发起请求会产生cookie
  session.get(main_url,headers=headers)
  
  url = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json'
  params = {
      'since_id': '-1',
      'max_id': '20346152',
      'count': '15',
      'category': '-1',
  }
  page_text = session.get(url,headers=headers,params=params).json()
  page_text
  ```

## 3.验证码的识别

- 相关的线上打码平台识别

  - 打码兔

  - 云打码

  - 超级鹰:

    http://www.chaojiying.com/about.html

    - 1.注册,登录(用户中心的身份认证)
    - 2.登录后:
      - 创建一个软件:软件ID->生成一个软件id
      - 下载示例代码:开发文档->python->下载

## 4.模拟登录

- 为什么在爬虫中需要实现模拟登录?
  
  - 有的数据是必须经过登录后才可以显示出来的!
  
- 涉及到的反爬:
  - robots.txt
  - UA 伪装
  - 图片懒加载
  - 验证码
  - 动态请求参数:每次请求对应的请求参数都是动态变化
    - 动态捕获:通常情况下,动态的请求参数都会被隐藏在前台页面的源码中
  - cookie
  
  ```python
  # 古诗文网 验证码模拟的登录
  def getCodeImgText(imgPath,img_type):
      chaojiying = Chaojiying_Client('bobo328410948', 'bobo328410948', '899370')	#用户中心>>软件ID 生成一个替换 96001
      im = open(imgPath, 'rb').read()													#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//
      return chaojiying.PostPic(im, img_type)['pic_str']
  
  #使用session捕获cookie
  s = requests.Session()
  first_url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'
  s.get(first_url,headers=headers)
  
  url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'
  page_text = requests.get(url,headers=headers).text
  tree = etree.HTML(page_text)
  img_src = 'https://so.gushiwen.org'+tree.xpath('//*[@id="imgCode"]/@src')[0]
  img_code_data = s.get(img_src,headers=headers).content
  with open('./gushiwen.jpg','wb') as fp:
      fp.write(img_code_data)
  img_text = getCodeImgText('./gushiwen.jpg',1004)
  print(img_text)
  
  #动态捕获动态的请求参数
  __VIEWSTATE = tree.xpath('//*[@id="__VIEWSTATE"]/@value')[0]
  __VIEWSTATEGENERATOR = tree.xpath('//*[@id="__VIEWSTATEGENERATOR"]/@value')[0]
  
  #点击登录按钮后发起请求的url:通过抓包工具捕获
  login_url = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx'
  data = {
      '__VIEWSTATE': __VIEWSTATE,
      '__VIEWSTATEGENERATOR': __VIEWSTATEGENERATOR,
      'from': 'http://so.gushiwen.org/user/collect.aspx',
      'email': 'www.zhangbowudi@qq.com',
      'pwd': 'bobo328410948',
      'code': img_text,
      'denglu': '登录',
  }
  main_page_text = s.post(login_url,headers=headers,data=data).text
  with open('main.html','w',encoding='utf-8') as fp:
      fp.write(main_page_text)
  ```
  
  